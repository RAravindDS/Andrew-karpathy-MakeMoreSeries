{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Andrej Karpathy Notes","text":"<p>This is not official one, this is made by fan boy. I watched all his vidoes and made a notes for all the videos. Hope you will guys enjoy this notes \ud83d\ude00</p>"},{"location":"01-BigramLLM/","title":"01. Build a Bigram Model","text":"<p>View Source Code | Watch Video Walkthrough</p> In\u00a0[\u00a0]: Copied! <pre># let's build a bigram language model here :) \n\n## In bigram language modelling we only work with 2 characters at a time \n## Only looking at one character which is given, then model try to predict next character. \n## It only look at previous character to find the next character!!\n</pre> # let's build a bigram language model here :)   ## In bigram language modelling we only work with 2 characters at a time  ## Only looking at one character which is given, then model try to predict next character.  ## It only look at previous character to find the next character!!  In\u00a0[1]: Copied! <pre># let's explore the dataset first \n# !wget -O \"names.txt\" https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n\n## loading the data \ndata = open(\"names.txt\", 'r').read().splitlines()\n</pre> # let's explore the dataset first  # !wget -O \"names.txt\" https://raw.githubusercontent.com/karpathy/makemore/master/names.txt  ## loading the data  data = open(\"names.txt\", 'r').read().splitlines()  In\u00a0[44]: Copied! <pre>## loading the bad words dataset \nimport pandas as pd \n\ndata = pd.read_csv(\"bad-words.csv\") \ndata.columns =  [\"bad_words\"]\ndata = data['bad_words'].tolist()\n\n# removing \"&amp;\" and \"'\" \n\n# data = [i.replace(\"&amp;\", \"\").replace(\"'\", \"\").replace(\"-\", \"\") for i in data]\n</pre> ## loading the bad words dataset  import pandas as pd   data = pd.read_csv(\"bad-words.csv\")  data.columns =  [\"bad_words\"] data = data['bad_words'].tolist()  # removing \"&amp;\" and \"'\"   # data = [i.replace(\"&amp;\", \"\").replace(\"'\", \"\").replace(\"-\", \"\") for i in data] In\u00a0[45]: Copied! <pre># data exploring \n\n## len of words \nprint(f\"Len of all words: {len(data)}\")\n\n## largest and smallest words: \nprint(f\"{min(len(w) for w in data)} -&gt; smallest word count  \\n{max(len(w) for w in data)} -&gt; largest word count\")\n</pre> # data exploring   ## len of words  print(f\"Len of all words: {len(data)}\")  ## largest and smallest words:  print(f\"{min(len(w) for w in data)} -&gt; smallest word count  \\n{max(len(w) for w in data)} -&gt; largest word count\") <pre>Len of all words: 1616\n2 -&gt; smallest word count  \n27 -&gt; largest word count\n</pre> In\u00a0[46]: Copied! <pre>## let's see the bigrams in our dataset \n\nb = dict()\nfor w in data: \n    chs = [\"&lt;S&gt;\"] + list(w) + [\"&lt;E&gt;\"]  # adding start and end token for each word\n    for ch1, ch2 in zip(chs, chs[1:]): \n        bigram = (ch1, ch2) \n        b[bigram] = b.get(bigram, 0)+1 # If vale not exists in dict we are adding 0 \n                           ## -If value exists we are adding 1, so exist value + 1\n</pre> ## let's see the bigrams in our dataset   b = dict() for w in data:      chs = [\"\"] + list(w) + [\"\"]  # adding start and end token for each word     for ch1, ch2 in zip(chs, chs[1:]):          bigram = (ch1, ch2)          b[bigram] = b.get(bigram, 0)+1 # If vale not exists in dict we are adding 0                             ## -If value exists we are adding 1, so exist value + 1 In\u00a0[47]: Copied! <pre># storing in dict is not good, let's store in the 2 dim array \n## where row is first character, column is second character and value is counts of both \n\n### let's store as a tensor array \nimport torch \n\nN = torch.zeros((35, 35), dtype = torch.int32) # 35 unique characters in dataset + . token\n</pre> # storing in dict is not good, let's store in the 2 dim array  ## where row is first character, column is second character and value is counts of both   ### let's store as a tensor array  import torch   N = torch.zeros((35, 35), dtype = torch.int32) # 35 unique characters in dataset + . token In\u00a0[48]: Copied! <pre># unique charcters in dataset \nchars = sorted(list(set(\"\".join(data)))) \n\n## let's create a lookup table like label2id and id2label \nstoi = {k:v+1 for v, k in enumerate(chars)} \nstoi['.'] = 0\nitos = {k:v for v, k in stoi.items()} \n\n\n## let's store everything in tensors\nfor w in data: \n    chs = [\".\"] + list(w) + [\".\"]  \n    for ch1, ch2 in zip(chs, chs[1:]): \n        ix1, ix2 = stoi[ch1], stoi[ch2] \n        N[ix1, ix2] += 1\n</pre> # unique charcters in dataset  chars = sorted(list(set(\"\".join(data))))   ## let's create a lookup table like label2id and id2label  stoi = {k:v+1 for v, k in enumerate(chars)}  stoi['.'] = 0 itos = {k:v for v, k in stoi.items()}    ## let's store everything in tensors for w in data:      chs = [\".\"] + list(w) + [\".\"]       for ch1, ch2 in zip(chs, chs[1:]):          ix1, ix2 = stoi[ch1], stoi[ch2]          N[ix1, ix2] += 1  In\u00a0[49]: Copied! <pre>print(f\"Nr of unique characters in our dataset: {len(chars)}\")\n</pre> print(f\"Nr of unique characters in our dataset: {len(chars)}\") <pre>Nr of unique characters in our dataset: 34\n</pre> In\u00a0[50]: Copied! <pre>import matplotlib.pyplot as plt \n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  %matplotlib inline  In\u00a0[51]: Copied! <pre># let's visualize the tensors \n\nplt.figure(figsize = (16, 16)) \nplt.imshow(N, cmap=\"Blues\")\n\nfor i in range(35): # 34 is for number of unique characters in the dataset + \".\" character so = 35 chars \n    for j in range(35): \n        chstr = itos[i] + itos[j] \n        plt.text(j, i, chstr, ha = \"center\", va = \"bottom\", color = \"red\")\n        plt.text(j, i, N[i,j].item(), ha = \"center\", va = \"top\", color = \"gray\")\n\nplt.axis(\"off\");\n</pre> # let's visualize the tensors   plt.figure(figsize = (16, 16))  plt.imshow(N, cmap=\"Blues\")  for i in range(35): # 34 is for number of unique characters in the dataset + \".\" character so = 35 chars      for j in range(35):          chstr = itos[i] + itos[j]          plt.text(j, i, chstr, ha = \"center\", va = \"bottom\", color = \"red\")         plt.text(j, i, N[i,j].item(), ha = \"center\", va = \"top\", color = \"gray\")  plt.axis(\"off\");  In\u00a0[52]: Copied! <pre># let's take a first row probabilities and normalizes those \np = N[0].float() ## taking the first row and converting to float because we need to normalize \np = p /p.sum() ## normalizing \nprint(f\"After normalizing: {p} \\nsum is {p.sum()}\")\n</pre> # let's take a first row probabilities and normalizes those  p = N[0].float() ## taking the first row and converting to float because we need to normalize  p = p /p.sum() ## normalizing  print(f\"After normalizing: {p} \\nsum is {p.sum()}\")   <pre>After normalizing: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0012, 0.0000, 0.0000,\n        0.0421, 0.1015, 0.0811, 0.0526, 0.0192, 0.0804, 0.0427, 0.0452, 0.0093,\n        0.0266, 0.0254, 0.0303, 0.0470, 0.0458, 0.0074, 0.0854, 0.0037, 0.0266,\n        0.1269, 0.0458, 0.0087, 0.0105, 0.0278, 0.0019, 0.0031, 0.0019]) \nsum is 1.0\n</pre> In\u00a0[53]: Copied! <pre># let's sample this gaussian distribution(normalized probabilites) using torch multinomial functions \n\ng = torch.Generator().manual_seed(2147483647) ## it always generates the same result \nix = torch.multinomial(p, num_samples=10, replacement=True, generator=g) ## multinomial  helps to sample from gaussian distribution \n## takes gaussian distribution as input and output the index. Sampling based on the probabilites \n\nix ## this ix is always same\n</pre> # let's sample this gaussian distribution(normalized probabilites) using torch multinomial functions   g = torch.Generator().manual_seed(2147483647) ## it always generates the same result  ix = torch.multinomial(p, num_samples=10, replacement=True, generator=g) ## multinomial  helps to sample from gaussian distribution  ## takes gaussian distribution as input and output the index. Sampling based on the probabilites   ix ## this ix is always same  Out[53]: <pre>tensor([24, 27, 24, 10, 10, 28, 28, 10, 22, 24])</pre> In\u00a0[54]: Copied! <pre>## let's see how our bigram model is working :) \n\ng = torch.Generator().manual_seed(2147483647) ## setting manual seed, it outputs same very time !! \n\nfor _ in range(10): \n    ix = 0 \n    out = []\n    while True: \n        p = N[ix].float()  # Randomly sampling the rows \n        p = p / p.sum()    # normalizing the sampled rows \n        # p = torch.ones(35) / 35.0  ## try with commeting previous 2 lines (this is untrained) (previous 2 lines are like trained model)\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() \n        out.append(itos[ix])\n        if ix == 0: break; # 0 means \".\" that's why we are stopping it : ) \n\n    print(\"\".join(out))\n</pre> ## let's see how our bigram model is working :)   g = torch.Generator().manual_seed(2147483647) ## setting manual seed, it outputs same very time !!   for _ in range(10):      ix = 0      out = []     while True:          p = N[ix].float()  # Randomly sampling the rows          p = p / p.sum()    # normalizing the sampled rows          # p = torch.ones(35) / 35.0  ## try with commeting previous 2 lines (this is untrained) (previous 2 lines are like trained model)         ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()          out.append(itos[ix])         if ix == 0: break; # 0 means \".\" that's why we are stopping it : )       print(\"\".join(out)) <pre>butack.\nbiching.\nh.\nmingse.\nves.\nbaledleraswe.\nlore.\nketfucotatadenataconggaterass.\nuk.\nbiafur.\n</pre> In\u00a0[55]: Copied! <pre>## our previous code is working fine but this is in-efficient way. So let's code better. \n\n### Sometime model outputs 0 probabilities this is because there is no occurence in the tensor. If you take log of 0 is (-infinity)\n### This is not good because in later we will take log of each prob. So we can add some duplicate count. \n### This duplicate cound is called \"Model Smoothing !!\"\n\n# adding fake counts (model smoothing)\nP = (N+1).float()\nP /= P.sum(dim = 1, keepdim=True)  # here we are doing broad-casting (look in pytorch)\ng = torch.Generator().manual_seed(2147483647) \n\nfor _ in range(10): \n    ix = 0 \n    out = []\n    while True: \n        p = P[ix]\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() \n        out.append(itos[ix])\n        if ix == 0: break; # 0 means \".\" that's why we are stopping it : ) \n\n    print(\"\".join(out))\n\n\n# the output is exact same results but we used broadcasting here :) it's faster ::\n</pre> ## our previous code is working fine but this is in-efficient way. So let's code better.   ### Sometime model outputs 0 probabilities this is because there is no occurence in the tensor. If you take log of 0 is (-infinity) ### This is not good because in later we will take log of each prob. So we can add some duplicate count.  ### This duplicate cound is called \"Model Smoothing !!\"  # adding fake counts (model smoothing) P = (N+1).float() P /= P.sum(dim = 1, keepdim=True)  # here we are doing broad-casting (look in pytorch) g = torch.Generator().manual_seed(2147483647)   for _ in range(10):      ix = 0      out = []     while True:          p = P[ix]         ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()          out.append(itos[ix])         if ix == 0: break; # 0 means \".\" that's why we are stopping it : )       print(\"\".join(out))   # the output is exact same results but we used broadcasting here :) it's faster ::  <pre>butack.\nbiching.\nh.\nmingse.\nves.\nbaledleraswe.\nlore.\nketfucotatadenataco&amp;-uaterass.\nukfaiafur.\ngolerennemelos2glleeber.\n</pre> In\u00a0[\u00a0]: Copied! <pre># we just trained a bigram model by just conunting the inupt characters. \n## Let's calculate the quality of the model :) \n### To calculate this will use the \"Max likelihood prob\"\n\n\"\"\"\nMaximum Likelihood probability: \n\n-&gt; Likelihood means product of all the probabilities. \n-&gt; The product of all probabilities (likelihood) should be as high as possible, that is maximum probability.\n-&gt; Mostly the probabilities are in the range between 0 to 1, if you product all your probabilites(likelihood) it will be very small number.\n-&gt; So, because of this issue, people always work with log-likelihood. \n-&gt; Simply we take the log of the probability is log-likelihood. \n-&gt; Log is just a monotonic transformation of the probability. \n-&gt; If you pass 1(probability) to log you get you 0 (good) probaility, as you go lower and lower probability like -2, -4 the log will grow more and more negative until all the way to negative infinity probability.  \n\n## intuitive way :) \nprobs = [0003432.23, 002.077, 004.07898]\nprobs1 = [0003432.23, 002.077, 004.07898]\nprobs2 = [0003432.23, 002.077, 004.07898]\n\nlikelihood -&gt; probs * probs1 * probs2 =&gt; likelihood \nlog(likelihood) -&gt; log(probs) + log(probs1) + log(probs2) =&gt; log(likelihood) \n\n----- \nLikelihood is product of the probabilites \nLog-likelihood is sum of the log of the individual probabilities\n-----\n\nHere the catch is if all the prob is nearly 1 (high), the likelihood is zero.  \nIf all the prob is negative (bad), the likelihood will grow more and more negative.  \n\nThe problem is in loss function, it assumes low is good but our Max log prob is inverse of this. \nSo, to solve this they added inverse sign to the existing form. \n\nThis is called \"negative log likelihood\" just adding negative sign to previous form. \nSo now we need to minimize this, because we added the negative sign \n\n\n## intuitive way :) \nprobs = [0003432.23, 002.077, 004.07898]\nprobs1 = [0003432.23, 002.077, 004.07898]\nprobs2 = [0003432.23, 002.077, 004.07898]\n\nlikelihood -&gt; probs * probs1 * probs2 =&gt; likelihood \nlog(likelihood) -&gt; log(probs) + log(probs1) + log(probs2) =&gt; log(likelihood) \nnegative log(likelihood) -&gt; -(log(probs) + log(probs2) + log(probs2)) =&gt; negative log(likelihood)\n\nSomeimtes people calculate the average of the negative log likelihood for more simplicity. \nHere also we need to minimize the avg negative log likehlihood because it's inverse of log likelihood.\n\"\"\"\n\n\n## let's see the probability assigned to bigrams by model :) \n\nlog_likelihood = 0.0  # just initializing the var \ncount = 0\nfor w in data:#[:2]:  \n    chs = [\".\"] + list(w) + [\".\"]  \n    for ch1, ch2 in zip(chs, chs[1:]): \n        ix1, ix2 = stoi[ch1], stoi[ch2] \n        prob = P[ix1, ix2] # gettting the probabilty \n        logprob = torch.log(prob) # log (probability) \n        log_likelihood += logprob\n        print(f\"Bigram: {ch1}{ch2} | prob: {prob:.4f} | nll: {log_likelihood:.4f}\")\n        count += 1\n\n\nnll = -log_likelihood /count  ## calclating the average of negative log likelihood \nprint(f\"\\n{nll=}\")\n\n## our ultimte aim to build a model that reduce this negative log likelihood loss \n\n## You can also check probability for your own data. Instead of data in for loop give your own words \n## Example: for i in [\"aravind\"]\n</pre> # we just trained a bigram model by just conunting the inupt characters.  ## Let's calculate the quality of the model :)  ### To calculate this will use the \"Max likelihood prob\"  \"\"\" Maximum Likelihood probability:   -&gt; Likelihood means product of all the probabilities.  -&gt; The product of all probabilities (likelihood) should be as high as possible, that is maximum probability. -&gt; Mostly the probabilities are in the range between 0 to 1, if you product all your probabilites(likelihood) it will be very small number. -&gt; So, because of this issue, people always work with log-likelihood.  -&gt; Simply we take the log of the probability is log-likelihood.  -&gt; Log is just a monotonic transformation of the probability.  -&gt; If you pass 1(probability) to log you get you 0 (good) probaility, as you go lower and lower probability like -2, -4 the log will grow more and more negative until all the way to negative infinity probability.    ## intuitive way :)  probs = [0003432.23, 002.077, 004.07898] probs1 = [0003432.23, 002.077, 004.07898] probs2 = [0003432.23, 002.077, 004.07898]  likelihood -&gt; probs * probs1 * probs2 =&gt; likelihood  log(likelihood) -&gt; log(probs) + log(probs1) + log(probs2) =&gt; log(likelihood)   -----  Likelihood is product of the probabilites  Log-likelihood is sum of the log of the individual probabilities -----  Here the catch is if all the prob is nearly 1 (high), the likelihood is zero.   If all the prob is negative (bad), the likelihood will grow more and more negative.    The problem is in loss function, it assumes low is good but our Max log prob is inverse of this.  So, to solve this they added inverse sign to the existing form.   This is called \"negative log likelihood\" just adding negative sign to previous form.  So now we need to minimize this, because we added the negative sign    ## intuitive way :)  probs = [0003432.23, 002.077, 004.07898] probs1 = [0003432.23, 002.077, 004.07898] probs2 = [0003432.23, 002.077, 004.07898]  likelihood -&gt; probs * probs1 * probs2 =&gt; likelihood  log(likelihood) -&gt; log(probs) + log(probs1) + log(probs2) =&gt; log(likelihood)  negative log(likelihood) -&gt; -(log(probs) + log(probs2) + log(probs2)) =&gt; negative log(likelihood)  Someimtes people calculate the average of the negative log likelihood for more simplicity.  Here also we need to minimize the avg negative log likehlihood because it's inverse of log likelihood. \"\"\"   ## let's see the probability assigned to bigrams by model :)   log_likelihood = 0.0  # just initializing the var  count = 0 for w in data:#[:2]:       chs = [\".\"] + list(w) + [\".\"]       for ch1, ch2 in zip(chs, chs[1:]):          ix1, ix2 = stoi[ch1], stoi[ch2]          prob = P[ix1, ix2] # gettting the probabilty          logprob = torch.log(prob) # log (probability)          log_likelihood += logprob         print(f\"Bigram: {ch1}{ch2} | prob: {prob:.4f} | nll: {log_likelihood:.4f}\")         count += 1   nll = -log_likelihood /count  ## calclating the average of negative log likelihood  print(f\"\\n{nll=}\")  ## our ultimte aim to build a model that reduce this negative log likelihood loss   ## You can also check probability for your own data. Instead of data in for loop give your own words  ## Example: for i in [\"aravind\"] In\u00a0[15]: Copied! <pre># We simply trained a bigram model using counts and we end with some-what results. \n# Now we change the gear to neural network, still it's a bigram model but we use \"gradient descent\" to optimize the loss function; \n# Let's start building the neural networks for our bigram model. \n## Note: Our neural network going to contain one hidden layer :) \n\n## The first step is to create a dataset for our model. Let's do it : )\n</pre> # We simply trained a bigram model using counts and we end with some-what results.  # Now we change the gear to neural network, still it's a bigram model but we use \"gradient descent\" to optimize the loss function;  # Let's start building the neural networks for our bigram model.  ## Note: Our neural network going to contain one hidden layer :)   ## The first step is to create a dataset for our model. Let's do it : )  In\u00a0[58]: Copied! <pre># create the training set of all the bigrams \n\nxs, ys = [], []\nfor w in data[:1]: # currently we are doing it for single word (just for understanding !! )\n    chs = [\".\"] + list(w) + [\".\"] \n\n    for ch1, ch2 in zip(chs, chs[1:]): \n        ix1, ix2 = stoi[ch1], stoi[ch2]\n        xs.append(ix1); ys.append(ix2) # xs is like input and ys is like output (next character)\n\n## let's convert to tensors \nxs = torch.tensor(xs) \nys = torch.tensor(ys)  \n\n## Here print xs[0] and ys[0]. \n## If the input is xs[0], the desired label would be ys[0]\nprint(f\"If input is xs[0]: {itos[xs[0].item()]} the desired label would be ys[0]: {itos[ys[0].item()]}\")\nprint(f\"If input is xs[1]: {itos[xs[1].item()]} the desired label would be ys[1]: {itos[ys[1].item()]}\")\n</pre> # create the training set of all the bigrams   xs, ys = [], [] for w in data[:1]: # currently we are doing it for single word (just for understanding !! )     chs = [\".\"] + list(w) + [\".\"]       for ch1, ch2 in zip(chs, chs[1:]):          ix1, ix2 = stoi[ch1], stoi[ch2]         xs.append(ix1); ys.append(ix2) # xs is like input and ys is like output (next character)  ## let's convert to tensors  xs = torch.tensor(xs)  ys = torch.tensor(ys)    ## Here print xs[0] and ys[0].  ## If the input is xs[0], the desired label would be ys[0] print(f\"If input is xs[0]: {itos[xs[0].item()]} the desired label would be ys[0]: {itos[ys[0].item()]}\") print(f\"If input is xs[1]: {itos[xs[1].item()]} the desired label would be ys[1]: {itos[ys[1].item()]}\") <pre>If input is xs[0]: . the desired label would be ys[0]: m\nIf input is xs[1]: m the desired label would be ys[1]: o\n</pre> In\u00a0[59]: Copied! <pre>## Now we created a training data, now let's see how to feed this to the model :) \n## The problem is we can't directly send this integers to the model, so let's convert this integers to one hot vectors. Pytorch has the function \n\nimport torch.nn.functional as F\n\nxenc = F.one_hot(xs, num_classes= 35).float()  # we are casting the one hot to float because neurons except like this :) \nplt.imshow(xenc);\n</pre> ## Now we created a training data, now let's see how to feed this to the model :)  ## The problem is we can't directly send this integers to the model, so let's convert this integers to one hot vectors. Pytorch has the function   import torch.nn.functional as F  xenc = F.one_hot(xs, num_classes= 35).float()  # we are casting the one hot to float because neurons except like this :)  plt.imshow(xenc); In\u00a0[60]: Copied! <pre>## let's initialize the weights for our model and multipley with \n\nW = torch.randn((35, 35))  # 35 -&gt; number of unique characters \nxenc @ W  ## @ --&gt; matrix multiplication operator \n## [15, 35] x [35, 35]  ==&gt; (15, 35) output shape\n## xenc.shape x W.shape\n</pre> ## let's initialize the weights for our model and multipley with   W = torch.randn((35, 35))  # 35 -&gt; number of unique characters  xenc @ W  ## @ --&gt; matrix multiplication operator  ## [15, 35] x [35, 35]  ==&gt; (15, 35) output shape ## xenc.shape x W.shape Out[60]: <pre>tensor([[-1.0400,  0.0441, -1.3624, -1.2079, -0.0035,  0.9026, -0.1026,  0.9106,\n          0.4065,  0.1689,  1.4212,  0.4559, -0.4924, -1.3990,  0.3903,  0.7698,\n         -0.1067,  0.3271,  0.8358, -1.3211, -0.1307,  0.3192, -0.8392,  0.4163,\n          1.4121,  0.6199,  1.0891,  0.7709,  1.4927, -1.7631, -0.6014,  0.8289,\n          0.4709, -0.0966, -0.5037],\n        [ 1.4732,  1.7493, -0.0963,  0.9757,  0.3469,  0.1042,  0.8517,  0.9005,\n          0.8200,  0.1866,  1.8885, -1.6515, -0.1875,  0.3480,  0.4734,  1.5282,\n         -0.1713, -0.8307,  0.8135,  1.7071,  1.2609, -0.9841,  0.3387,  2.8431,\n         -0.7258,  0.3162,  1.5496, -1.5097,  0.9435,  1.8485,  0.5430, -0.6068,\n          0.8453, -0.4501,  1.3500],\n        [-0.7842, -2.1068, -1.3965, -1.8047, -0.5424,  0.2606,  0.4057,  0.1716,\n          2.0276, -0.8124, -1.0029,  1.6854,  1.2616,  2.3840,  1.1024,  1.8563,\n         -0.7985,  1.0913, -0.4454,  0.7334, -2.4487, -0.5937,  0.5230,  0.1769,\n          0.8616,  0.3186,  0.3841,  0.7664,  1.0452,  1.5384, -0.2956, -0.8659,\n          1.6338,  0.4258,  1.5575],\n        [ 0.4885, -0.2571,  1.1700, -2.1040,  0.1978,  0.8133, -0.5569, -1.4341,\n          0.4079,  0.7926,  0.9849, -0.7353,  1.2832,  0.3999, -0.1259, -1.1396,\n         -0.7163, -0.6247,  1.4820, -1.2032,  0.2303,  1.9246, -1.1158, -0.5467,\n         -0.3668, -0.4117, -0.5891, -1.4090, -0.3549, -0.2629, -1.0963,  0.6761,\n         -0.3151,  0.2204, -0.4051],\n        [ 0.4371, -1.2492,  0.3426,  0.8290, -0.0609,  0.5323,  0.7788,  1.8393,\n         -1.7689, -0.9797, -0.3625, -0.4256,  0.5607, -0.1737, -0.4917, -0.8559,\n          0.2027, -0.5800, -0.1253, -1.2614, -0.6498, -0.7597,  0.5634,  1.9512,\n          0.4891, -2.1488,  1.4609,  0.4402, -0.5524, -0.3583,  0.5509, -1.1756,\n         -0.3105,  0.0065, -3.1129],\n        [ 1.1934, -0.2834,  0.9185,  1.5921,  0.6613, -1.0518,  0.8232,  1.3163,\n         -0.1148,  1.2191, -1.7939, -0.6121,  0.7298, -1.0429, -0.5589,  1.2857,\n         -1.4730,  0.0331, -0.8375,  0.3050,  0.6102, -0.6425, -0.2156, -1.5270,\n         -0.4171, -0.7027, -0.4717,  1.5112, -2.0720, -1.4213, -0.6784, -1.1954,\n          1.3819,  0.4625, -0.1353],\n        [-1.5491, -0.0260, -0.7204,  0.7721,  0.4707, -0.4295, -0.8121,  0.2519,\n         -2.7533, -0.1068,  0.1688, -0.2627, -1.5926, -0.0374,  0.5025,  1.6908,\n         -0.8870,  0.0923,  0.3669,  0.0511,  0.5994,  0.2056, -0.8170, -1.6274,\n         -0.2228, -0.6105, -1.1916,  0.7671,  1.5444, -0.5181, -1.4226,  0.4130,\n         -0.5480,  0.6552,  0.0737],\n        [-0.7842, -2.1068, -1.3965, -1.8047, -0.5424,  0.2606,  0.4057,  0.1716,\n          2.0276, -0.8124, -1.0029,  1.6854,  1.2616,  2.3840,  1.1024,  1.8563,\n         -0.7985,  1.0913, -0.4454,  0.7334, -2.4487, -0.5937,  0.5230,  0.1769,\n          0.8616,  0.3186,  0.3841,  0.7664,  1.0452,  1.5384, -0.2956, -0.8659,\n          1.6338,  0.4258,  1.5575],\n        [ 0.1739, -1.6899,  0.0905,  0.0571,  0.6518, -0.5610,  0.8339,  0.2041,\n          0.3915, -0.0135, -1.3106,  0.5391,  0.0569, -0.4404,  1.8407, -0.3120,\n         -0.4155,  0.9831, -3.1290, -1.4104, -1.0995, -0.4042,  0.9808, -1.6695,\n         -0.0902,  0.0577, -0.6569, -0.7578,  1.1272,  0.4102, -1.4501,  1.9938,\n          0.3000,  1.2261, -1.3055],\n        [-1.5491, -0.0260, -0.7204,  0.7721,  0.4707, -0.4295, -0.8121,  0.2519,\n         -2.7533, -0.1068,  0.1688, -0.2627, -1.5926, -0.0374,  0.5025,  1.6908,\n         -0.8870,  0.0923,  0.3669,  0.0511,  0.5994,  0.2056, -0.8170, -1.6274,\n         -0.2228, -0.6105, -1.1916,  0.7671,  1.5444, -0.5181, -1.4226,  0.4130,\n         -0.5480,  0.6552,  0.0737],\n        [ 0.0554,  1.6713, -0.3597,  0.1076,  1.4408,  0.6389, -1.3698, -1.0624,\n         -0.9722, -1.2138,  0.4542, -1.6486,  0.2808, -1.3393, -0.5839, -1.0269,\n         -1.0479,  0.0710,  0.3798,  0.1285, -0.2231, -0.5424,  0.8038,  0.7549,\n         -0.0464, -1.1714,  0.5109,  0.9198,  1.1065, -0.6464,  0.1603, -0.1637,\n         -0.5381, -0.5232,  1.8182],\n        [-1.5169,  0.8552,  1.0822,  0.9588,  0.4401,  0.2547,  0.1738,  0.0606,\n         -0.1391,  0.1639,  1.0509,  0.8412, -0.4578, -2.0499, -0.2578, -0.0771,\n         -0.5054,  0.5043,  1.1663,  0.3612,  0.9659,  0.8183, -1.5098, -0.6018,\n          0.6518,  0.9463,  2.1855,  0.4801, -0.0685, -0.0942,  1.2762, -0.0424,\n          1.2045, -0.0338,  0.3560],\n        [ 0.4371, -1.2492,  0.3426,  0.8290, -0.0609,  0.5323,  0.7788,  1.8393,\n         -1.7689, -0.9797, -0.3625, -0.4256,  0.5607, -0.1737, -0.4917, -0.8559,\n          0.2027, -0.5800, -0.1253, -1.2614, -0.6498, -0.7597,  0.5634,  1.9512,\n          0.4891, -2.1488,  1.4609,  0.4402, -0.5524, -0.3583,  0.5509, -1.1756,\n         -0.3105,  0.0065, -3.1129],\n        [ 0.4885, -0.2571,  1.1700, -2.1040,  0.1978,  0.8133, -0.5569, -1.4341,\n          0.4079,  0.7926,  0.9849, -0.7353,  1.2832,  0.3999, -0.1259, -1.1396,\n         -0.7163, -0.6247,  1.4820, -1.2032,  0.2303,  1.9246, -1.1158, -0.5467,\n         -0.3668, -0.4117, -0.5891, -1.4090, -0.3549, -0.2629, -1.0963,  0.6761,\n         -0.3151,  0.2204, -0.4051],\n        [-1.7373,  0.0380, -1.9484, -1.9892, -0.4980, -0.4857,  0.5160,  0.3740,\n          0.4365, -0.1398,  1.9853, -0.0495, -0.2213, -2.2622, -0.4466, -0.0307,\n          1.3129,  0.0062, -0.3954,  0.9220, -1.3599, -0.0524, -1.7691,  0.2125,\n         -1.0778, -0.2043, -1.3771,  0.1399,  1.4173, -1.2186,  0.0676, -0.8424,\n          1.1601, -0.0369,  1.1607]])</pre> In\u00a0[61]: Copied! <pre>## now we build a one hidden layer for our model, now let's try to find the output. \n## Modle outputs some positive and negative integers which don't make any sense for us. We need a output which represents the next character probability. \n## Basically our model outputs the \"log counts\", now we need to get only counts from this to remove the log we use exponential. \n\n\n\"\"\"\nExponential: \n-&gt; It takes both positive and negative numbers. \n-&gt; If you pluging negative numbers you get output bellow 1 to infinity. \n-&gt; If you plugig greater than 1 you always get output greater than 1 always to infinity. \n\"\"\"\n\n## let's do that exponential \nlogits = xenc @ W # log-of-counts is called logits \ncounts = logits.exp()\nprobs = counts / counts.sum(dim = 1, keepdims = True)  # just normalizing the rows \nprobs \n\n## The last 2 line of code is just a softmax algorithm :)\n</pre> ## now we build a one hidden layer for our model, now let's try to find the output.  ## Modle outputs some positive and negative integers which don't make any sense for us. We need a output which represents the next character probability.  ## Basically our model outputs the \"log counts\", now we need to get only counts from this to remove the log we use exponential.    \"\"\" Exponential:  -&gt; It takes both positive and negative numbers.  -&gt; If you pluging negative numbers you get output bellow 1 to infinity.  -&gt; If you plugig greater than 1 you always get output greater than 1 always to infinity.  \"\"\"  ## let's do that exponential  logits = xenc @ W # log-of-counts is called logits  counts = logits.exp() probs = counts / counts.sum(dim = 1, keepdims = True)  # just normalizing the rows  probs   ## The last 2 line of code is just a softmax algorithm :)  Out[61]: <pre>tensor([[0.0067, 0.0199, 0.0049, 0.0057, 0.0190, 0.0470, 0.0172, 0.0474, 0.0286,\n         0.0226, 0.0790, 0.0301, 0.0117, 0.0047, 0.0282, 0.0412, 0.0171, 0.0265,\n         0.0440, 0.0051, 0.0167, 0.0262, 0.0082, 0.0289, 0.0783, 0.0354, 0.0567,\n         0.0412, 0.0849, 0.0033, 0.0105, 0.0437, 0.0305, 0.0173, 0.0115],\n        [0.0454, 0.0599, 0.0095, 0.0276, 0.0147, 0.0116, 0.0244, 0.0256, 0.0236,\n         0.0125, 0.0688, 0.0020, 0.0086, 0.0147, 0.0167, 0.0480, 0.0088, 0.0045,\n         0.0235, 0.0574, 0.0367, 0.0039, 0.0146, 0.1787, 0.0050, 0.0143, 0.0490,\n         0.0023, 0.0267, 0.0661, 0.0179, 0.0057, 0.0242, 0.0066, 0.0402],\n        [0.0057, 0.0015, 0.0031, 0.0021, 0.0072, 0.0162, 0.0187, 0.0148, 0.0947,\n         0.0055, 0.0046, 0.0672, 0.0440, 0.1352, 0.0375, 0.0798, 0.0056, 0.0371,\n         0.0080, 0.0259, 0.0011, 0.0069, 0.0210, 0.0149, 0.0295, 0.0171, 0.0183,\n         0.0268, 0.0354, 0.0580, 0.0093, 0.0052, 0.0638, 0.0191, 0.0592],\n        [0.0350, 0.0166, 0.0693, 0.0026, 0.0262, 0.0485, 0.0123, 0.0051, 0.0323,\n         0.0475, 0.0576, 0.0103, 0.0776, 0.0321, 0.0190, 0.0069, 0.0105, 0.0115,\n         0.0946, 0.0065, 0.0271, 0.1473, 0.0070, 0.0124, 0.0149, 0.0142, 0.0119,\n         0.0053, 0.0151, 0.0165, 0.0072, 0.0423, 0.0157, 0.0268, 0.0143],\n        [0.0325, 0.0060, 0.0296, 0.0481, 0.0198, 0.0358, 0.0458, 0.1321, 0.0036,\n         0.0079, 0.0146, 0.0137, 0.0368, 0.0177, 0.0128, 0.0089, 0.0257, 0.0118,\n         0.0185, 0.0059, 0.0110, 0.0098, 0.0369, 0.1478, 0.0342, 0.0024, 0.0905,\n         0.0326, 0.0121, 0.0147, 0.0364, 0.0065, 0.0154, 0.0211, 0.0009],\n        [0.0637, 0.0146, 0.0484, 0.0950, 0.0374, 0.0068, 0.0440, 0.0721, 0.0172,\n         0.0654, 0.0032, 0.0105, 0.0401, 0.0068, 0.0111, 0.0699, 0.0044, 0.0200,\n         0.0084, 0.0262, 0.0356, 0.0102, 0.0156, 0.0042, 0.0127, 0.0096, 0.0121,\n         0.0876, 0.0024, 0.0047, 0.0098, 0.0058, 0.0770, 0.0307, 0.0169],\n        [0.0052, 0.0237, 0.0118, 0.0527, 0.0390, 0.0158, 0.0108, 0.0313, 0.0016,\n         0.0219, 0.0288, 0.0187, 0.0050, 0.0234, 0.0402, 0.1320, 0.0100, 0.0267,\n         0.0351, 0.0256, 0.0443, 0.0299, 0.0108, 0.0048, 0.0195, 0.0132, 0.0074,\n         0.0524, 0.1140, 0.0145, 0.0059, 0.0368, 0.0141, 0.0469, 0.0262],\n        [0.0057, 0.0015, 0.0031, 0.0021, 0.0072, 0.0162, 0.0187, 0.0148, 0.0947,\n         0.0055, 0.0046, 0.0672, 0.0440, 0.1352, 0.0375, 0.0798, 0.0056, 0.0371,\n         0.0080, 0.0259, 0.0011, 0.0069, 0.0210, 0.0149, 0.0295, 0.0171, 0.0183,\n         0.0268, 0.0354, 0.0580, 0.0093, 0.0052, 0.0638, 0.0191, 0.0592],\n        [0.0236, 0.0037, 0.0217, 0.0210, 0.0381, 0.0113, 0.0457, 0.0243, 0.0294,\n         0.0196, 0.0054, 0.0340, 0.0210, 0.0128, 0.1251, 0.0145, 0.0131, 0.0531,\n         0.0009, 0.0048, 0.0066, 0.0133, 0.0529, 0.0037, 0.0181, 0.0210, 0.0103,\n         0.0093, 0.0613, 0.0299, 0.0047, 0.1458, 0.0268, 0.0677, 0.0054],\n        [0.0052, 0.0237, 0.0118, 0.0527, 0.0390, 0.0158, 0.0108, 0.0313, 0.0016,\n         0.0219, 0.0288, 0.0187, 0.0050, 0.0234, 0.0402, 0.1320, 0.0100, 0.0267,\n         0.0351, 0.0256, 0.0443, 0.0299, 0.0108, 0.0048, 0.0195, 0.0132, 0.0074,\n         0.0524, 0.1140, 0.0145, 0.0059, 0.0368, 0.0141, 0.0469, 0.0262],\n        [0.0220, 0.1109, 0.0146, 0.0232, 0.0881, 0.0395, 0.0053, 0.0072, 0.0079,\n         0.0062, 0.0328, 0.0040, 0.0276, 0.0055, 0.0116, 0.0075, 0.0073, 0.0224,\n         0.0305, 0.0237, 0.0167, 0.0121, 0.0466, 0.0444, 0.0199, 0.0065, 0.0348,\n         0.0523, 0.0630, 0.0109, 0.0245, 0.0177, 0.0122, 0.0124, 0.1284],\n        [0.0035, 0.0376, 0.0471, 0.0417, 0.0248, 0.0206, 0.0190, 0.0170, 0.0139,\n         0.0188, 0.0457, 0.0370, 0.0101, 0.0021, 0.0123, 0.0148, 0.0096, 0.0265,\n         0.0513, 0.0229, 0.0420, 0.0362, 0.0035, 0.0088, 0.0307, 0.0412, 0.1421,\n         0.0258, 0.0149, 0.0145, 0.0572, 0.0153, 0.0533, 0.0154, 0.0228],\n        [0.0325, 0.0060, 0.0296, 0.0481, 0.0198, 0.0358, 0.0458, 0.1321, 0.0036,\n         0.0079, 0.0146, 0.0137, 0.0368, 0.0177, 0.0128, 0.0089, 0.0257, 0.0118,\n         0.0185, 0.0059, 0.0110, 0.0098, 0.0369, 0.1478, 0.0342, 0.0024, 0.0905,\n         0.0326, 0.0121, 0.0147, 0.0364, 0.0065, 0.0154, 0.0211, 0.0009],\n        [0.0350, 0.0166, 0.0693, 0.0026, 0.0262, 0.0485, 0.0123, 0.0051, 0.0323,\n         0.0475, 0.0576, 0.0103, 0.0776, 0.0321, 0.0190, 0.0069, 0.0105, 0.0115,\n         0.0946, 0.0065, 0.0271, 0.1473, 0.0070, 0.0124, 0.0149, 0.0142, 0.0119,\n         0.0053, 0.0151, 0.0165, 0.0072, 0.0423, 0.0157, 0.0268, 0.0143],\n        [0.0039, 0.0229, 0.0031, 0.0030, 0.0134, 0.0136, 0.0369, 0.0320, 0.0341,\n         0.0192, 0.1605, 0.0210, 0.0177, 0.0023, 0.0141, 0.0214, 0.0819, 0.0222,\n         0.0148, 0.0554, 0.0057, 0.0209, 0.0038, 0.0273, 0.0075, 0.0180, 0.0056,\n         0.0254, 0.0910, 0.0065, 0.0236, 0.0095, 0.0703, 0.0212, 0.0704]])</pre> In\u00a0[62]: Copied! <pre>## summary of all the neural network stuff \n \nn = 15\nnlls = torch.zeros(n)\nfor i in range(n): \n    x = xs[i].item()  ## grabbing the input \n    y = ys[i].item()  ## grabbing the output \n    print(f\"Input: {itos[x]}, Desired output: {itos[y]}\")\n\n    logits = probs[i, y]   ## grabbing the probabilites for (input  and  output)\n    # from the model (it tells that probability of each character after the input, it tells the probability of each character occuring next \n    print(f\"Outupt logit: {probs[i]}\")\n    print(f\"The probability of our desired next character {itos[y]} is : {logits}\")\n    loglogits = torch.log(logits) ## log of counts \n    nll = -loglogits ## negative log likelihood \n    print(f\"Negative log likelihood: {nll}\")\n    nlls[i] = nll \n    print(f\"{'-'*19}\\n\")\n\nprint(f\"average log likelihood is {nlls.mean().item()}\")\n</pre> ## summary of all the neural network stuff    n = 15 nlls = torch.zeros(n) for i in range(n):      x = xs[i].item()  ## grabbing the input      y = ys[i].item()  ## grabbing the output      print(f\"Input: {itos[x]}, Desired output: {itos[y]}\")      logits = probs[i, y]   ## grabbing the probabilites for (input  and  output)     # from the model (it tells that probability of each character after the input, it tells the probability of each character occuring next      print(f\"Outupt logit: {probs[i]}\")     print(f\"The probability of our desired next character {itos[y]} is : {logits}\")     loglogits = torch.log(logits) ## log of counts      nll = -loglogits ## negative log likelihood      print(f\"Negative log likelihood: {nll}\")     nlls[i] = nll      print(f\"{'-'*19}\\n\")  print(f\"average log likelihood is {nlls.mean().item()}\")  <pre>Input: ., Desired output: m\nOutupt logit: tensor([0.0067, 0.0199, 0.0049, 0.0057, 0.0190, 0.0470, 0.0172, 0.0474, 0.0286,\n        0.0226, 0.0790, 0.0301, 0.0117, 0.0047, 0.0282, 0.0412, 0.0171, 0.0265,\n        0.0440, 0.0051, 0.0167, 0.0262, 0.0082, 0.0289, 0.0783, 0.0354, 0.0567,\n        0.0412, 0.0849, 0.0033, 0.0105, 0.0437, 0.0305, 0.0173, 0.0115])\nThe probability of our desired next character m is : 0.026241740211844444\nNegative log likelihood: 3.640403985977173\n-------------------\n\nInput: m, Desired output: o\nOutupt logit: tensor([0.0454, 0.0599, 0.0095, 0.0276, 0.0147, 0.0116, 0.0244, 0.0256, 0.0236,\n        0.0125, 0.0688, 0.0020, 0.0086, 0.0147, 0.0167, 0.0480, 0.0088, 0.0045,\n        0.0235, 0.0574, 0.0367, 0.0039, 0.0146, 0.1787, 0.0050, 0.0143, 0.0490,\n        0.0023, 0.0267, 0.0661, 0.0179, 0.0057, 0.0242, 0.0066, 0.0402])\nThe probability of our desired next character o is : 0.17873473465442657\nNegative log likelihood: 1.7218525409698486\n-------------------\n\nInput: o, Desired output: u\nOutupt logit: tensor([0.0057, 0.0015, 0.0031, 0.0021, 0.0072, 0.0162, 0.0187, 0.0148, 0.0947,\n        0.0055, 0.0046, 0.0672, 0.0440, 0.1352, 0.0375, 0.0798, 0.0056, 0.0371,\n        0.0080, 0.0259, 0.0011, 0.0069, 0.0210, 0.0149, 0.0295, 0.0171, 0.0183,\n        0.0268, 0.0354, 0.0580, 0.0093, 0.0052, 0.0638, 0.0191, 0.0592])\nThe probability of our desired next character u is : 0.058036573231220245\nNegative log likelihood: 2.846681833267212\n-------------------\n\nInput: u, Desired output: n\nOutupt logit: tensor([0.0350, 0.0166, 0.0693, 0.0026, 0.0262, 0.0485, 0.0123, 0.0051, 0.0323,\n        0.0475, 0.0576, 0.0103, 0.0776, 0.0321, 0.0190, 0.0069, 0.0105, 0.0115,\n        0.0946, 0.0065, 0.0271, 0.1473, 0.0070, 0.0124, 0.0149, 0.0142, 0.0119,\n        0.0053, 0.0151, 0.0165, 0.0072, 0.0423, 0.0157, 0.0268, 0.0143])\nThe probability of our desired next character n is : 0.007044113706797361\nNegative log likelihood: 4.955563068389893\n-------------------\n\nInput: n, Desired output: d\nOutupt logit: tensor([0.0325, 0.0060, 0.0296, 0.0481, 0.0198, 0.0358, 0.0458, 0.1321, 0.0036,\n        0.0079, 0.0146, 0.0137, 0.0368, 0.0177, 0.0128, 0.0089, 0.0257, 0.0118,\n        0.0185, 0.0059, 0.0110, 0.0098, 0.0369, 0.1478, 0.0342, 0.0024, 0.0905,\n        0.0326, 0.0121, 0.0147, 0.0364, 0.0065, 0.0154, 0.0211, 0.0009])\nThe probability of our desired next character d is : 0.036789409816265106\nNegative log likelihood: 3.3025453090667725\n-------------------\n\nInput: d, Desired output:  \nOutupt logit: tensor([0.0637, 0.0146, 0.0484, 0.0950, 0.0374, 0.0068, 0.0440, 0.0721, 0.0172,\n        0.0654, 0.0032, 0.0105, 0.0401, 0.0068, 0.0111, 0.0699, 0.0044, 0.0200,\n        0.0084, 0.0262, 0.0356, 0.0102, 0.0156, 0.0042, 0.0127, 0.0096, 0.0121,\n        0.0876, 0.0024, 0.0047, 0.0098, 0.0058, 0.0770, 0.0307, 0.0169])\nThe probability of our desired next character   is : 0.014557437039911747\nNegative log likelihood: 4.229653358459473\n-------------------\n\nInput:  , Desired output: o\nOutupt logit: tensor([0.0052, 0.0237, 0.0118, 0.0527, 0.0390, 0.0158, 0.0108, 0.0313, 0.0016,\n        0.0219, 0.0288, 0.0187, 0.0050, 0.0234, 0.0402, 0.1320, 0.0100, 0.0267,\n        0.0351, 0.0256, 0.0443, 0.0299, 0.0108, 0.0048, 0.0195, 0.0132, 0.0074,\n        0.0524, 0.1140, 0.0145, 0.0059, 0.0368, 0.0141, 0.0469, 0.0262])\nThe probability of our desired next character o is : 0.004781367257237434\nNegative log likelihood: 5.343028545379639\n-------------------\n\nInput: o, Desired output: f\nOutupt logit: tensor([0.0057, 0.0015, 0.0031, 0.0021, 0.0072, 0.0162, 0.0187, 0.0148, 0.0947,\n        0.0055, 0.0046, 0.0672, 0.0440, 0.1352, 0.0375, 0.0798, 0.0056, 0.0371,\n        0.0080, 0.0259, 0.0011, 0.0069, 0.0210, 0.0149, 0.0295, 0.0171, 0.0183,\n        0.0268, 0.0354, 0.0580, 0.0093, 0.0052, 0.0638, 0.0191, 0.0592])\nThe probability of our desired next character f is : 0.03752404451370239\nNegative log likelihood: 3.28277325630188\n-------------------\n\nInput: f, Desired output:  \nOutupt logit: tensor([0.0236, 0.0037, 0.0217, 0.0210, 0.0381, 0.0113, 0.0457, 0.0243, 0.0294,\n        0.0196, 0.0054, 0.0340, 0.0210, 0.0128, 0.1251, 0.0145, 0.0131, 0.0531,\n        0.0009, 0.0048, 0.0066, 0.0133, 0.0529, 0.0037, 0.0181, 0.0210, 0.0103,\n        0.0093, 0.0613, 0.0299, 0.0047, 0.1458, 0.0268, 0.0677, 0.0054])\nThe probability of our desired next character   is : 0.0036638884339481592\nNegative log likelihood: 5.6092305183410645\n-------------------\n\nInput:  , Desired output: v\nOutupt logit: tensor([0.0052, 0.0237, 0.0118, 0.0527, 0.0390, 0.0158, 0.0108, 0.0313, 0.0016,\n        0.0219, 0.0288, 0.0187, 0.0050, 0.0234, 0.0402, 0.1320, 0.0100, 0.0267,\n        0.0351, 0.0256, 0.0443, 0.0299, 0.0108, 0.0048, 0.0195, 0.0132, 0.0074,\n        0.0524, 0.1140, 0.0145, 0.0059, 0.0368, 0.0141, 0.0469, 0.0262])\nThe probability of our desired next character v is : 0.005868026986718178\nNegative log likelihood: 5.138236999511719\n-------------------\n\nInput: v, Desired output: e\nOutupt logit: tensor([0.0220, 0.1109, 0.0146, 0.0232, 0.0881, 0.0395, 0.0053, 0.0072, 0.0079,\n        0.0062, 0.0328, 0.0040, 0.0276, 0.0055, 0.0116, 0.0075, 0.0073, 0.0224,\n        0.0305, 0.0237, 0.0167, 0.0121, 0.0466, 0.0444, 0.0199, 0.0065, 0.0348,\n        0.0523, 0.0630, 0.0109, 0.0245, 0.0177, 0.0122, 0.0124, 0.1284])\nThe probability of our desired next character e is : 0.005462936125695705\nNegative log likelihood: 5.209768772125244\n-------------------\n\nInput: e, Desired output: n\nOutupt logit: tensor([0.0035, 0.0376, 0.0471, 0.0417, 0.0248, 0.0206, 0.0190, 0.0170, 0.0139,\n        0.0188, 0.0457, 0.0370, 0.0101, 0.0021, 0.0123, 0.0148, 0.0096, 0.0265,\n        0.0513, 0.0229, 0.0420, 0.0362, 0.0035, 0.0088, 0.0307, 0.0412, 0.1421,\n        0.0258, 0.0149, 0.0145, 0.0572, 0.0153, 0.0533, 0.0154, 0.0228])\nThe probability of our desired next character n is : 0.0035293258260935545\nNegative log likelihood: 5.646648406982422\n-------------------\n\nInput: n, Desired output: u\nOutupt logit: tensor([0.0325, 0.0060, 0.0296, 0.0481, 0.0198, 0.0358, 0.0458, 0.1321, 0.0036,\n        0.0079, 0.0146, 0.0137, 0.0368, 0.0177, 0.0128, 0.0089, 0.0257, 0.0118,\n        0.0185, 0.0059, 0.0110, 0.0098, 0.0369, 0.1478, 0.0342, 0.0024, 0.0905,\n        0.0326, 0.0121, 0.0147, 0.0364, 0.0065, 0.0154, 0.0211, 0.0009])\nThe probability of our desired next character u is : 0.014675113372504711\nNegative log likelihood: 4.221601963043213\n-------------------\n\nInput: u, Desired output: s\nOutupt logit: tensor([0.0350, 0.0166, 0.0693, 0.0026, 0.0262, 0.0485, 0.0123, 0.0051, 0.0323,\n        0.0475, 0.0576, 0.0103, 0.0776, 0.0321, 0.0190, 0.0069, 0.0105, 0.0115,\n        0.0946, 0.0065, 0.0271, 0.1473, 0.0070, 0.0124, 0.0149, 0.0142, 0.0119,\n        0.0053, 0.0151, 0.0165, 0.0072, 0.0423, 0.0157, 0.0268, 0.0143])\nThe probability of our desired next character s is : 0.005253707058727741\nNegative log likelihood: 5.248821258544922\n-------------------\n\nInput: s, Desired output: .\nOutupt logit: tensor([0.0039, 0.0229, 0.0031, 0.0030, 0.0134, 0.0136, 0.0369, 0.0320, 0.0341,\n        0.0192, 0.1605, 0.0210, 0.0177, 0.0023, 0.0141, 0.0214, 0.0819, 0.0222,\n        0.0148, 0.0554, 0.0057, 0.0209, 0.0038, 0.0273, 0.0075, 0.0180, 0.0056,\n        0.0254, 0.0910, 0.0065, 0.0236, 0.0095, 0.0703, 0.0212, 0.0704])\nThe probability of our desired next character . is : 0.0038798239547759295\nNegative log likelihood: 5.551965713500977\n-------------------\n\naverage log likelihood is 4.396584987640381\n</pre> In\u00a0[63]: Copied! <pre>## let's see full network \n\n# randomly initialize 35 neurons weights. Each neuron  receives 35 inputs \ng = torch.Generator().manual_seed(2179879798)\nW = torch.randn( (35, 35), generator=g, requires_grad=True)\n</pre> ## let's see full network   # randomly initialize 35 neurons weights. Each neuron  receives 35 inputs  g = torch.Generator().manual_seed(2179879798) W = torch.randn( (35, 35), generator=g, requires_grad=True)  In\u00a0[64]: Copied! <pre># forward pass \nxnec = F.one_hot(xs, num_classes=35).float()\nlogits = xenc @ W \ncounts = logits.exp() \nprobs = counts / counts.sum(dim=1, keepdims=True)  # outputs probability of all the characters.      \nloss = -probs[torch.arange(len(xnec)), ys].log().mean()  # negative log probability \nprint(loss)\n</pre>  # forward pass  xnec = F.one_hot(xs, num_classes=35).float() logits = xenc @ W  counts = logits.exp()  probs = counts / counts.sum(dim=1, keepdims=True)  # outputs probability of all the characters.       loss = -probs[torch.arange(len(xnec)), ys].log().mean()  # negative log probability  print(loss) <pre>tensor(3.7921, grad_fn=&lt;NegBackward0&gt;)\n</pre> In\u00a0[65]: Copied! <pre>## backward pass \nW.grad = None  ## set to zero gradient \nloss.backward()\n</pre> ## backward pass  W.grad = None  ## set to zero gradient  loss.backward()  In\u00a0[66]: Copied! <pre>## update the weights \nW.data += -00.1 * W.grad\n</pre> ## update the weights  W.data += -00.1 * W.grad  In\u00a0[71]: Copied! <pre>## 1. Let's put it all together \n\n# creating a dataset \nxs, ys = [], []\nfor w in data: \n    chs = [\".\"] + list(w) + [\".\"] \n\n    for ch1, ch2 in zip(chs, chs[1:]): \n        ix1, ix2 = stoi[ch1], stoi[ch2]\n        xs.append(ix1); ys.append(ix2) \n\n## let's convert to tensors \nxs = torch.tensor(xs) \nys = torch.tensor(ys)  \n\nnum = xs.nelement()\nprint(f\"Number of elements: {num}\")\n\n# randomly initialize 27 neurons weights. Each neuron  receives 27 inputs \ng = torch.Generator().manual_seed(2179879798)\nW = torch.randn( (35, 35), generator=g, requires_grad=True)\n</pre> ## 1. Let's put it all together   # creating a dataset  xs, ys = [], [] for w in data:      chs = [\".\"] + list(w) + [\".\"]       for ch1, ch2 in zip(chs, chs[1:]):          ix1, ix2 = stoi[ch1], stoi[ch2]         xs.append(ix1); ys.append(ix2)   ## let's convert to tensors  xs = torch.tensor(xs)  ys = torch.tensor(ys)    num = xs.nelement() print(f\"Number of elements: {num}\")  # randomly initialize 27 neurons weights. Each neuron  receives 27 inputs  g = torch.Generator().manual_seed(2179879798) W = torch.randn( (35, 35), generator=g, requires_grad=True)  <pre>Number of elements: 13649\n</pre> In\u00a0[76]: Copied! <pre>## 2. let's put it all together \n\n# Gradient descent \nn = 1000 \nfor k in range(n):\n     # forward pass \n    xnec = F.one_hot(xs, num_classes=35).float()\n    logits = xnec @ W \n    counts = logits.exp() \n    probs = counts / counts.sum(dim=1, keepdims=True)  # outputs probability of all the characters.      \n    loss = -probs[torch.arange(len(xnec)), ys].log().mean()  # negative log probability \n    loss += 0.01*(W**2).mean() # adding regularization because we are initializing random index, sometimes it may have lot of zeros. If we have lot of zeros our loss will get high. so we are doing it. \n    print(f\"Loss of iteration: {k}: {loss.item()}\") \n\n    ## backward pass \n    W.grad = None  ## set to zero gradient \n    loss.backward() \n\n    ## update the weights \n    W.data += -160* W.grad\n</pre> ## 2. let's put it all together   # Gradient descent  n = 1000  for k in range(n):      # forward pass      xnec = F.one_hot(xs, num_classes=35).float()     logits = xnec @ W      counts = logits.exp()      probs = counts / counts.sum(dim=1, keepdims=True)  # outputs probability of all the characters.           loss = -probs[torch.arange(len(xnec)), ys].log().mean()  # negative log probability      loss += 0.01*(W**2).mean() # adding regularization because we are initializing random index, sometimes it may have lot of zeros. If we have lot of zeros our loss will get high. so we are doing it.      print(f\"Loss of iteration: {k}: {loss.item()}\")       ## backward pass      W.grad = None  ## set to zero gradient      loss.backward()       ## update the weights      W.data += -160* W.grad  <pre>Loss of iteration: 0: 2.5691263675689697\nLoss of iteration: 1: 2.5691263675689697\nLoss of iteration: 2: 2.5691263675689697\nLoss of iteration: 3: 2.5691261291503906\nLoss of iteration: 4: 2.5691261291503906\nLoss of iteration: 5: 2.5691261291503906\nLoss of iteration: 6: 2.5691261291503906\nLoss of iteration: 7: 2.5691261291503906\nLoss of iteration: 8: 2.5691261291503906\nLoss of iteration: 9: 2.5691261291503906\nLoss of iteration: 10: 2.5691261291503906\nLoss of iteration: 11: 2.5691258907318115\nLoss of iteration: 12: 2.5691256523132324\nLoss of iteration: 13: 2.5691256523132324\nLoss of iteration: 14: 2.5691258907318115\nLoss of iteration: 15: 2.5691258907318115\nLoss of iteration: 16: 2.5691256523132324\nLoss of iteration: 17: 2.5691258907318115\nLoss of iteration: 18: 2.5691258907318115\nLoss of iteration: 19: 2.5691263675689697\nLoss of iteration: 20: 2.5691280364990234\nLoss of iteration: 21: 2.569134473800659\nLoss of iteration: 22: 2.5691568851470947\nLoss of iteration: 23: 2.569242000579834\nLoss of iteration: 24: 2.569544553756714\nLoss of iteration: 25: 2.5706703662872314\nLoss of iteration: 26: 2.574174165725708\nLoss of iteration: 27: 2.5854275226593018\nLoss of iteration: 28: 2.5949244499206543\nLoss of iteration: 29: 2.609837532043457\nLoss of iteration: 30: 2.580317974090576\nLoss of iteration: 31: 2.578026533126831\nLoss of iteration: 32: 2.58964204788208\nLoss of iteration: 33: 2.6089141368865967\nLoss of iteration: 34: 2.586026906967163\nLoss of iteration: 35: 2.580019950866699\nLoss of iteration: 36: 2.5867953300476074\nLoss of iteration: 37: 2.609297752380371\nLoss of iteration: 38: 2.58917498588562\nLoss of iteration: 39: 2.5802581310272217\nLoss of iteration: 40: 2.584146022796631\nLoss of iteration: 41: 2.6079297065734863\nLoss of iteration: 42: 2.5914227962493896\nLoss of iteration: 43: 2.5825295448303223\nLoss of iteration: 44: 2.582864284515381\nLoss of iteration: 45: 2.6064388751983643\nLoss of iteration: 46: 2.5922179222106934\nLoss of iteration: 47: 2.584603786468506\nLoss of iteration: 48: 2.5824201107025146\nLoss of iteration: 49: 2.605158805847168\nLoss of iteration: 50: 2.5924618244171143\nLoss of iteration: 51: 2.586273193359375\nLoss of iteration: 52: 2.5822501182556152\nLoss of iteration: 53: 2.60404109954834\nLoss of iteration: 54: 2.5925724506378174\nLoss of iteration: 55: 2.5877177715301514\nLoss of iteration: 56: 2.5821194648742676\nLoss of iteration: 57: 2.602921485900879\nLoss of iteration: 58: 2.592677354812622\nLoss of iteration: 59: 2.589179277420044\nLoss of iteration: 60: 2.5819475650787354\nLoss of iteration: 61: 2.601614236831665\nLoss of iteration: 62: 2.592832565307617\nLoss of iteration: 63: 2.5909106731414795\nLoss of iteration: 64: 2.581676721572876\nLoss of iteration: 65: 2.599860191345215\nLoss of iteration: 66: 2.593097448348999\nLoss of iteration: 67: 2.593259334564209\nLoss of iteration: 68: 2.581228256225586\nLoss of iteration: 69: 2.5972046852111816\nLoss of iteration: 70: 2.593567132949829\nLoss of iteration: 71: 2.5967884063720703\nLoss of iteration: 72: 2.580512285232544\nLoss of iteration: 73: 2.5928635597229004\nLoss of iteration: 74: 2.5943005084991455\nLoss of iteration: 75: 2.6022136211395264\nLoss of iteration: 76: 2.579725742340088\nLoss of iteration: 77: 2.586078405380249\nLoss of iteration: 78: 2.5944015979766846\nLoss of iteration: 79: 2.608640670776367\nLoss of iteration: 80: 2.580515146255493\nLoss of iteration: 81: 2.579108715057373\nLoss of iteration: 82: 2.5904576778411865\nLoss of iteration: 83: 2.609616279602051\nLoss of iteration: 84: 2.5852344036102295\nLoss of iteration: 85: 2.579023599624634\nLoss of iteration: 86: 2.5868325233459473\nLoss of iteration: 87: 2.609083414077759\nLoss of iteration: 88: 2.589090347290039\nLoss of iteration: 89: 2.5804708003997803\nLoss of iteration: 90: 2.5842857360839844\nLoss of iteration: 91: 2.607987880706787\nLoss of iteration: 92: 2.591306209564209\nLoss of iteration: 93: 2.582404375076294\nLoss of iteration: 94: 2.58292818069458\nLoss of iteration: 95: 2.6065196990966797\nLoss of iteration: 96: 2.592174530029297\nLoss of iteration: 97: 2.5844788551330566\nLoss of iteration: 98: 2.5824408531188965\nLoss of iteration: 99: 2.6052286624908447\nLoss of iteration: 100: 2.5924437046051025\nLoss of iteration: 101: 2.5861711502075195\nLoss of iteration: 102: 2.5822596549987793\nLoss of iteration: 103: 2.604105234146118\nLoss of iteration: 104: 2.5925607681274414\nLoss of iteration: 105: 2.58762526512146\nLoss of iteration: 106: 2.582127809524536\nLoss of iteration: 107: 2.6029891967773438\nLoss of iteration: 108: 2.592665910720825\nLoss of iteration: 109: 2.589082956314087\nLoss of iteration: 110: 2.5819597244262695\nLoss of iteration: 111: 2.601698637008667\nLoss of iteration: 112: 2.5928173065185547\nLoss of iteration: 113: 2.590792417526245\nLoss of iteration: 114: 2.5816969871520996\nLoss of iteration: 115: 2.599979877471924\nLoss of iteration: 116: 2.593074083328247\nLoss of iteration: 117: 2.593092441558838\nLoss of iteration: 118: 2.5812618732452393\nLoss of iteration: 119: 2.597395658493042\nLoss of iteration: 120: 2.5935285091400146\nLoss of iteration: 121: 2.596531867980957\nLoss of iteration: 122: 2.5805625915527344\nLoss of iteration: 123: 2.593181848526001\nLoss of iteration: 124: 2.5942513942718506\nLoss of iteration: 125: 2.6018338203430176\nLoss of iteration: 126: 2.5797572135925293\nLoss of iteration: 127: 2.586535692214966\nLoss of iteration: 128: 2.5944671630859375\nLoss of iteration: 129: 2.608323812484741\nLoss of iteration: 130: 2.580368995666504\nLoss of iteration: 131: 2.579399585723877\nLoss of iteration: 132: 2.5907933712005615\nLoss of iteration: 133: 2.6097662448883057\nLoss of iteration: 134: 2.5848867893218994\nLoss of iteration: 135: 2.5787923336029053\nLoss of iteration: 136: 2.586953639984131\nLoss of iteration: 137: 2.609035015106201\nLoss of iteration: 138: 2.5889554023742676\nLoss of iteration: 139: 2.580491065979004\nLoss of iteration: 140: 2.584408760070801\nLoss of iteration: 141: 2.608065605163574\nLoss of iteration: 142: 2.591212034225464\nLoss of iteration: 143: 2.582277297973633\nLoss of iteration: 144: 2.5829811096191406\nLoss of iteration: 145: 2.6066033840179443\nLoss of iteration: 146: 2.5921425819396973\nLoss of iteration: 147: 2.584362268447876\nLoss of iteration: 148: 2.582458019256592\nLoss of iteration: 149: 2.605299711227417\nLoss of iteration: 150: 2.5924317836761475\nLoss of iteration: 151: 2.586075782775879\nLoss of iteration: 152: 2.5822675228118896\nLoss of iteration: 153: 2.6041698455810547\nLoss of iteration: 154: 2.5925536155700684\nLoss of iteration: 155: 2.587538480758667\nLoss of iteration: 156: 2.5821356773376465\nLoss of iteration: 157: 2.603058099746704\nLoss of iteration: 158: 2.5926575660705566\nLoss of iteration: 159: 2.588989734649658\nLoss of iteration: 160: 2.5819709300994873\nLoss of iteration: 161: 2.601783514022827\nLoss of iteration: 162: 2.5928046703338623\nLoss of iteration: 163: 2.59067702293396\nLoss of iteration: 164: 2.5817155838012695\nLoss of iteration: 165: 2.600099563598633\nLoss of iteration: 166: 2.593053102493286\nLoss of iteration: 167: 2.5929291248321533\nLoss of iteration: 168: 2.5812935829162598\nLoss of iteration: 169: 2.5975844860076904\nLoss of iteration: 170: 2.5934925079345703\nLoss of iteration: 171: 2.596281051635742\nLoss of iteration: 172: 2.580611228942871\nLoss of iteration: 173: 2.5934948921203613\nLoss of iteration: 174: 2.594203472137451\nLoss of iteration: 175: 2.60145902633667\nLoss of iteration: 176: 2.57979154586792\nLoss of iteration: 177: 2.586991786956787\nLoss of iteration: 178: 2.5945208072662354\nLoss of iteration: 179: 2.6079885959625244\nLoss of iteration: 180: 2.5802371501922607\nLoss of iteration: 181: 2.5797171592712402\nLoss of iteration: 182: 2.591132640838623\nLoss of iteration: 183: 2.6099088191986084\nLoss of iteration: 184: 2.5845394134521484\nLoss of iteration: 185: 2.5785820484161377\nLoss of iteration: 186: 2.587085247039795\nLoss of iteration: 187: 2.6089894771575928\nLoss of iteration: 188: 2.588843822479248\nLoss of iteration: 189: 2.5805675983428955\nLoss of iteration: 190: 2.5846097469329834\nLoss of iteration: 191: 2.6082496643066406\nLoss of iteration: 192: 2.5912866592407227\nLoss of iteration: 193: 2.582395553588867\nLoss of iteration: 194: 2.583495855331421\nLoss of iteration: 195: 2.6072838306427\nLoss of iteration: 196: 2.593226194381714\nLoss of iteration: 197: 2.5854711532592773\nLoss of iteration: 198: 2.584918737411499\nLoss of iteration: 199: 2.6074635982513428\nLoss of iteration: 200: 2.596644639968872\nLoss of iteration: 201: 2.588649272918701\nLoss of iteration: 202: 2.587623119354248\nLoss of iteration: 203: 2.6070971488952637\nLoss of iteration: 204: 2.5981109142303467\nLoss of iteration: 205: 2.5903656482696533\nLoss of iteration: 206: 2.5877442359924316\nLoss of iteration: 207: 2.6060781478881836\nLoss of iteration: 208: 2.598263740539551\nLoss of iteration: 209: 2.5918495655059814\nLoss of iteration: 210: 2.5876030921936035\nLoss of iteration: 211: 2.604837417602539\nLoss of iteration: 212: 2.598414421081543\nLoss of iteration: 213: 2.5935206413269043\nLoss of iteration: 214: 2.5873584747314453\nLoss of iteration: 215: 2.6031925678253174\nLoss of iteration: 216: 2.598654270172119\nLoss of iteration: 217: 2.5957248210906982\nLoss of iteration: 218: 2.5869503021240234\nLoss of iteration: 219: 2.6007497310638428\nLoss of iteration: 220: 2.5990781784057617\nLoss of iteration: 221: 2.5989842414855957\nLoss of iteration: 222: 2.5862860679626465\nLoss of iteration: 223: 2.5967910289764404\nLoss of iteration: 224: 2.599774122238159\nLoss of iteration: 225: 2.6040287017822266\nLoss of iteration: 226: 2.5854551792144775\nLoss of iteration: 227: 2.5904464721679688\nLoss of iteration: 228: 2.600184917449951\nLoss of iteration: 229: 2.6105754375457764\nLoss of iteration: 230: 2.5857324600219727\nLoss of iteration: 231: 2.5830514430999756\nLoss of iteration: 232: 2.597113847732544\nLoss of iteration: 233: 2.6130049228668213\nLoss of iteration: 234: 2.58978009223938\nLoss of iteration: 235: 2.581333875656128\nLoss of iteration: 236: 2.592829704284668\nLoss of iteration: 237: 2.6118645668029785\nLoss of iteration: 238: 2.5942864418029785\nLoss of iteration: 239: 2.5835442543029785\nLoss of iteration: 240: 2.5903162956237793\nLoss of iteration: 241: 2.6112051010131836\nLoss of iteration: 242: 2.5966100692749023\nLoss of iteration: 243: 2.584961414337158\nLoss of iteration: 244: 2.588731050491333\nLoss of iteration: 245: 2.60976243019104\nLoss of iteration: 246: 2.5976901054382324\nLoss of iteration: 247: 2.5870659351348877\nLoss of iteration: 248: 2.5881218910217285\nLoss of iteration: 249: 2.608429193496704\nLoss of iteration: 250: 2.5980279445648193\nLoss of iteration: 251: 2.5888290405273438\nLoss of iteration: 252: 2.5879082679748535\nLoss of iteration: 253: 2.6072843074798584\nLoss of iteration: 254: 2.5981600284576416\nLoss of iteration: 255: 2.590311050415039\nLoss of iteration: 256: 2.587775230407715\nLoss of iteration: 257: 2.60617995262146\nLoss of iteration: 258: 2.5982627868652344\nLoss of iteration: 259: 2.5917506217956543\nLoss of iteration: 260: 2.587618350982666\nLoss of iteration: 261: 2.6049392223358154\nLoss of iteration: 262: 2.5984010696411133\nLoss of iteration: 263: 2.5933895111083984\nLoss of iteration: 264: 2.587379217147827\nLoss of iteration: 265: 2.60332989692688\nLoss of iteration: 266: 2.5986311435699463\nLoss of iteration: 267: 2.5955402851104736\nLoss of iteration: 268: 2.5869855880737305\nLoss of iteration: 269: 2.6009626388549805\nLoss of iteration: 270: 2.5990378856658936\nLoss of iteration: 271: 2.5987014770507812\nLoss of iteration: 272: 2.5863418579101562\nLoss of iteration: 273: 2.5971431732177734\nLoss of iteration: 274: 2.5997166633605957\nLoss of iteration: 275: 2.6035985946655273\nLoss of iteration: 276: 2.5855040550231934\nLoss of iteration: 277: 2.5909783840179443\nLoss of iteration: 278: 2.600214958190918\nLoss of iteration: 279: 2.610136032104492\nLoss of iteration: 280: 2.5856125354766846\nLoss of iteration: 281: 2.5834879875183105\nLoss of iteration: 282: 2.597501039505005\nLoss of iteration: 283: 2.6131205558776855\nLoss of iteration: 284: 2.5893678665161133\nLoss of iteration: 285: 2.58113956451416\nLoss of iteration: 286: 2.5929839611053467\nLoss of iteration: 287: 2.6117796897888184\nLoss of iteration: 288: 2.594106435775757\nLoss of iteration: 289: 2.5835988521575928\nLoss of iteration: 290: 2.5904946327209473\nLoss of iteration: 291: 2.611307382583618\nLoss of iteration: 292: 2.596466302871704\nLoss of iteration: 293: 2.584792375564575\nLoss of iteration: 294: 2.588813304901123\nLoss of iteration: 295: 2.609877347946167\nLoss of iteration: 296: 2.5976414680480957\nLoss of iteration: 297: 2.586907386779785\nLoss of iteration: 298: 2.588148832321167\nLoss of iteration: 299: 2.6085259914398193\nLoss of iteration: 300: 2.5980124473571777\nLoss of iteration: 301: 2.5887019634246826\nLoss of iteration: 302: 2.5879197120666504\nLoss of iteration: 303: 2.6073710918426514\nLoss of iteration: 304: 2.598151445388794\nLoss of iteration: 305: 2.590198040008545\nLoss of iteration: 306: 2.587785482406616\nLoss of iteration: 307: 2.606269359588623\nLoss of iteration: 308: 2.598253011703491\nLoss of iteration: 309: 2.59163236618042\nLoss of iteration: 310: 2.587632417678833\nLoss of iteration: 311: 2.6050467491149902\nLoss of iteration: 312: 2.5983872413635254\nLoss of iteration: 313: 2.593245506286621\nLoss of iteration: 314: 2.587402105331421\nLoss of iteration: 315: 2.60347843170166\nLoss of iteration: 316: 2.5986075401306152\nLoss of iteration: 317: 2.5953407287597656\nLoss of iteration: 318: 2.587023973464966\nLoss of iteration: 319: 2.601191282272339\nLoss of iteration: 320: 2.598996162414551\nLoss of iteration: 321: 2.5983974933624268\nLoss of iteration: 322: 2.5864031314849854\nLoss of iteration: 323: 2.5975208282470703\nLoss of iteration: 324: 2.599653720855713\nLoss of iteration: 325: 2.603132724761963\nLoss of iteration: 326: 2.585564136505127\nLoss of iteration: 327: 2.5915582180023193\nLoss of iteration: 328: 2.6002304553985596\nLoss of iteration: 329: 2.6096296310424805\nLoss of iteration: 330: 2.5855038166046143\nLoss of iteration: 331: 2.5840046405792236\nLoss of iteration: 332: 2.5979106426239014\nLoss of iteration: 333: 2.6132023334503174\nLoss of iteration: 334: 2.5889217853546143\nLoss of iteration: 335: 2.580977201461792\nLoss of iteration: 336: 2.5931737422943115\nLoss of iteration: 337: 2.611698627471924\nLoss of iteration: 338: 2.5938894748687744\nLoss of iteration: 339: 2.5836408138275146\nLoss of iteration: 340: 2.5907022953033447\nLoss of iteration: 341: 2.611422300338745\nLoss of iteration: 342: 2.5962960720062256\nLoss of iteration: 343: 2.584601640701294\nLoss of iteration: 344: 2.5889108180999756\nLoss of iteration: 345: 2.610006332397461\nLoss of iteration: 346: 2.597583532333374\nLoss of iteration: 347: 2.5867297649383545\nLoss of iteration: 348: 2.5881803035736084\nLoss of iteration: 349: 2.6086342334747314\nLoss of iteration: 350: 2.597993850708008\nLoss of iteration: 351: 2.5885612964630127\nLoss of iteration: 352: 2.5879323482513428\nLoss of iteration: 353: 2.607466459274292\nLoss of iteration: 354: 2.598142147064209\nLoss of iteration: 355: 2.5900745391845703\nLoss of iteration: 356: 2.587796449661255\nLoss of iteration: 357: 2.6063666343688965\nLoss of iteration: 358: 2.5982437133789062\nLoss of iteration: 359: 2.5915048122406006\nLoss of iteration: 360: 2.5876474380493164\nLoss of iteration: 361: 2.6051619052886963\nLoss of iteration: 362: 2.5983731746673584\nLoss of iteration: 363: 2.593092203140259\nLoss of iteration: 364: 2.5874264240264893\nLoss of iteration: 365: 2.6036345958709717\nLoss of iteration: 366: 2.598583221435547\nLoss of iteration: 367: 2.5951311588287354\nLoss of iteration: 368: 2.587064743041992\nLoss of iteration: 369: 2.6014301776885986\nLoss of iteration: 370: 2.5989530086517334\nLoss of iteration: 371: 2.598078966140747\nLoss of iteration: 372: 2.586467981338501\nLoss of iteration: 373: 2.5979151725769043\nLoss of iteration: 374: 2.5995867252349854\nLoss of iteration: 375: 2.6026418209075928\nLoss of iteration: 376: 2.5856330394744873\nLoss of iteration: 377: 2.5921733379364014\nLoss of iteration: 378: 2.6002283096313477\nLoss of iteration: 379: 2.609062671661377\nLoss of iteration: 380: 2.5854132175445557\nLoss of iteration: 381: 2.584599494934082\nLoss of iteration: 382: 2.5983242988586426\nLoss of iteration: 383: 2.6132285594940186\nLoss of iteration: 384: 2.5884578227996826\nLoss of iteration: 385: 2.5808701515197754\nLoss of iteration: 386: 2.5934102535247803\nLoss of iteration: 387: 2.6116440296173096\nLoss of iteration: 388: 2.5936241149902344\nLoss of iteration: 389: 2.5836400985717773\nLoss of iteration: 390: 2.5909364223480225\nLoss of iteration: 391: 2.6115503311157227\nLoss of iteration: 392: 2.596100330352783\nLoss of iteration: 393: 2.5843896865844727\nLoss of iteration: 394: 2.5890214443206787\nLoss of iteration: 395: 2.610145092010498\nLoss of iteration: 396: 2.5975167751312256\nLoss of iteration: 397: 2.586538553237915\nLoss of iteration: 398: 2.588216543197632\nLoss of iteration: 399: 2.6087496280670166\nLoss of iteration: 400: 2.5979738235473633\nLoss of iteration: 401: 2.5884101390838623\nLoss of iteration: 402: 2.5879461765289307\nLoss of iteration: 403: 2.6075661182403564\nLoss of iteration: 404: 2.598132371902466\nLoss of iteration: 405: 2.58994460105896\nLoss of iteration: 406: 2.5878076553344727\nLoss of iteration: 407: 2.606466770172119\nLoss of iteration: 408: 2.5982344150543213\nLoss of iteration: 409: 2.5913732051849365\nLoss of iteration: 410: 2.587663173675537\nLoss of iteration: 411: 2.605278968811035\nLoss of iteration: 412: 2.5983588695526123\nLoss of iteration: 413: 2.5929365158081055\nLoss of iteration: 414: 2.5874505043029785\nLoss of iteration: 415: 2.6037919521331787\nLoss of iteration: 416: 2.598559617996216\nLoss of iteration: 417: 2.594919443130493\nLoss of iteration: 418: 2.5871050357818604\nLoss of iteration: 419: 2.6016690731048584\nLoss of iteration: 420: 2.598910093307495\nLoss of iteration: 421: 2.5977611541748047\nLoss of iteration: 422: 2.5865328311920166\nLoss of iteration: 423: 2.598306894302368\nLoss of iteration: 424: 2.5995185375213623\nLoss of iteration: 425: 2.6021480560302734\nLoss of iteration: 426: 2.585707426071167\nLoss of iteration: 427: 2.5927939414978027\nLoss of iteration: 428: 2.6002085208892822\nLoss of iteration: 429: 2.608461856842041\nLoss of iteration: 430: 2.5853452682495117\nLoss of iteration: 431: 2.5852468013763428\nLoss of iteration: 432: 2.5987136363983154\nLoss of iteration: 433: 2.613179922103882\nLoss of iteration: 434: 2.588002920150757\nLoss of iteration: 435: 2.5808398723602295\nLoss of iteration: 436: 2.5936999320983887\nLoss of iteration: 437: 2.6116416454315186\nLoss of iteration: 438: 2.593308687210083\nLoss of iteration: 439: 2.5835683345794678\nLoss of iteration: 440: 2.591186761856079\nLoss of iteration: 441: 2.611685037612915\nLoss of iteration: 442: 2.5958876609802246\nLoss of iteration: 443: 2.584167242050171\nLoss of iteration: 444: 2.589139223098755\nLoss of iteration: 445: 2.6102843284606934\nLoss of iteration: 446: 2.597444772720337\nLoss of iteration: 447: 2.5863444805145264\nLoss of iteration: 448: 2.5882551670074463\nLoss of iteration: 449: 2.6088647842407227\nLoss of iteration: 450: 2.5979528427124023\nLoss of iteration: 451: 2.5882599353790283\nLoss of iteration: 452: 2.5879604816436768\nLoss of iteration: 453: 2.6076645851135254\nLoss of iteration: 454: 2.598123073577881\nLoss of iteration: 455: 2.5898170471191406\nLoss of iteration: 456: 2.5878186225891113\nLoss of iteration: 457: 2.6065635681152344\nLoss of iteration: 458: 2.5982253551483154\nLoss of iteration: 459: 2.5912466049194336\nLoss of iteration: 460: 2.587677001953125\nLoss of iteration: 461: 2.605391025543213\nLoss of iteration: 462: 2.598345994949341\nLoss of iteration: 463: 2.5927884578704834\nLoss of iteration: 464: 2.587473154067993\nLoss of iteration: 465: 2.603940725326538\nLoss of iteration: 466: 2.598536968231201\nLoss of iteration: 467: 2.5947203636169434\nLoss of iteration: 468: 2.587141990661621\nLoss of iteration: 469: 2.6018919944763184\nLoss of iteration: 470: 2.5988709926605225\nLoss of iteration: 471: 2.597463369369507\nLoss of iteration: 472: 2.5865941047668457\nLoss of iteration: 473: 2.5986709594726562\nLoss of iteration: 474: 2.599453926086426\nLoss of iteration: 475: 2.6016857624053955\nLoss of iteration: 476: 2.585782051086426\nLoss of iteration: 477: 2.5933780670166016\nLoss of iteration: 478: 2.600175142288208\nLoss of iteration: 479: 2.6078720092773438\nLoss of iteration: 480: 2.585303544998169\nLoss of iteration: 481: 2.5858981609344482\nLoss of iteration: 482: 2.5990471839904785\nLoss of iteration: 483: 2.6130571365356445\nLoss of iteration: 484: 2.5875914096832275\nLoss of iteration: 485: 2.5808897018432617\nLoss of iteration: 486: 2.5940282344818115\nLoss of iteration: 487: 2.6117002964019775\nLoss of iteration: 488: 2.5929596424102783\nLoss of iteration: 489: 2.58341908454895\nLoss of iteration: 490: 2.5914313793182373\nLoss of iteration: 491: 2.6118128299713135\nLoss of iteration: 492: 2.5956757068634033\nLoss of iteration: 493: 2.5839569568634033\nLoss of iteration: 494: 2.5892553329467773\nLoss of iteration: 495: 2.610412120819092\nLoss of iteration: 496: 2.597372055053711\nLoss of iteration: 497: 2.5861656665802\nLoss of iteration: 498: 2.58829402923584\nLoss of iteration: 499: 2.608971357345581\nLoss of iteration: 500: 2.5979316234588623\nLoss of iteration: 501: 2.588120937347412\nLoss of iteration: 502: 2.5879738330841064\nLoss of iteration: 503: 2.607754945755005\nLoss of iteration: 504: 2.598114013671875\nLoss of iteration: 505: 2.58970046043396\nLoss of iteration: 506: 2.587827682495117\nLoss of iteration: 507: 2.606651782989502\nLoss of iteration: 508: 2.598217725753784\nLoss of iteration: 509: 2.5911314487457275\nLoss of iteration: 510: 2.5876898765563965\nLoss of iteration: 511: 2.6054911613464355\nLoss of iteration: 512: 2.598334550857544\nLoss of iteration: 513: 2.592655658721924\nLoss of iteration: 514: 2.5874924659729004\nLoss of iteration: 515: 2.604072332382202\nLoss of iteration: 516: 2.598517894744873\nLoss of iteration: 517: 2.594543695449829\nLoss of iteration: 518: 2.587174892425537\nLoss of iteration: 519: 2.6020877361297607\nLoss of iteration: 520: 2.598836898803711\nLoss of iteration: 521: 2.5972020626068115\nLoss of iteration: 522: 2.5866477489471436\nLoss of iteration: 523: 2.598989248275757\nLoss of iteration: 524: 2.5993967056274414\nLoss of iteration: 525: 2.6012778282165527\nLoss of iteration: 526: 2.585850954055786\nLoss of iteration: 527: 2.593893527984619\nLoss of iteration: 528: 2.6001358032226562\nLoss of iteration: 529: 2.6073319911956787\nLoss of iteration: 530: 2.5852839946746826\nLoss of iteration: 531: 2.5865063667297363\nLoss of iteration: 532: 2.599311590194702\nLoss of iteration: 533: 2.6128814220428467\nLoss of iteration: 534: 2.587245464324951\nLoss of iteration: 535: 2.5810024738311768\nLoss of iteration: 536: 2.5943686962127686\nLoss of iteration: 537: 2.6118061542510986\nLoss of iteration: 538: 2.5926032066345215\nLoss of iteration: 539: 2.5832161903381348\nLoss of iteration: 540: 2.5916523933410645\nLoss of iteration: 541: 2.6119182109832764\nLoss of iteration: 542: 2.59548020362854\nLoss of iteration: 543: 2.5837790966033936\nLoss of iteration: 544: 2.5893638134002686\nLoss of iteration: 545: 2.6105220317840576\nLoss of iteration: 546: 2.597301959991455\nLoss of iteration: 547: 2.5860097408294678\nLoss of iteration: 548: 2.588331699371338\nLoss of iteration: 549: 2.609065532684326\nLoss of iteration: 550: 2.5979108810424805\nLoss of iteration: 551: 2.5879976749420166\nLoss of iteration: 552: 2.587986707687378\nLoss of iteration: 553: 2.607834577560425\nLoss of iteration: 554: 2.5981059074401855\nLoss of iteration: 555: 2.589597702026367\nLoss of iteration: 556: 2.587836503982544\nLoss of iteration: 557: 2.6067283153533936\nLoss of iteration: 558: 2.598210334777832\nLoss of iteration: 559: 2.591031312942505\nLoss of iteration: 560: 2.587700366973877\nLoss of iteration: 561: 2.6055779457092285\nLoss of iteration: 562: 2.59832501411438\nLoss of iteration: 563: 2.592541217803955\nLoss of iteration: 564: 2.5875091552734375\nLoss of iteration: 565: 2.604184865951538\nLoss of iteration: 566: 2.598501682281494\nLoss of iteration: 567: 2.594393253326416\nLoss of iteration: 568: 2.587202548980713\nLoss of iteration: 569: 2.6022536754608154\nLoss of iteration: 570: 2.5988078117370605\nLoss of iteration: 571: 2.5969796180725098\nLoss of iteration: 572: 2.586693048477173\nLoss of iteration: 573: 2.5992581844329834\nLoss of iteration: 574: 2.5993480682373047\nLoss of iteration: 575: 2.60093092918396\nLoss of iteration: 576: 2.585911989212036\nLoss of iteration: 577: 2.594331979751587\nLoss of iteration: 578: 2.6000945568084717\nLoss of iteration: 579: 2.6068601608276367\nLoss of iteration: 580: 2.585280179977417\nLoss of iteration: 581: 2.587047815322876\nLoss of iteration: 582: 2.599513053894043\nLoss of iteration: 583: 2.612678289413452\nLoss of iteration: 584: 2.5869648456573486\nLoss of iteration: 585: 2.5811526775360107\nLoss of iteration: 586: 2.594698429107666\nLoss of iteration: 587: 2.611936330795288\nLoss of iteration: 588: 2.5922622680664062\nLoss of iteration: 589: 2.5829920768737793\nLoss of iteration: 590: 2.5918407440185547\nLoss of iteration: 591: 2.6119937896728516\nLoss of iteration: 592: 2.5953097343444824\nLoss of iteration: 593: 2.583646059036255\nLoss of iteration: 594: 2.589463949203491\nLoss of iteration: 595: 2.61061429977417\nLoss of iteration: 596: 2.597235679626465\nLoss of iteration: 597: 2.585876941680908\nLoss of iteration: 598: 2.5883681774139404\nLoss of iteration: 599: 2.609147787094116\nLoss of iteration: 600: 2.597890853881836\nLoss of iteration: 601: 2.5878891944885254\nLoss of iteration: 602: 2.5879993438720703\nLoss of iteration: 603: 2.6079044342041016\nLoss of iteration: 604: 2.598098039627075\nLoss of iteration: 605: 2.5895073413848877\nLoss of iteration: 606: 2.5878443717956543\nLoss of iteration: 607: 2.606795072555542\nLoss of iteration: 608: 2.5982043743133545\nLoss of iteration: 609: 2.5909440517425537\nLoss of iteration: 610: 2.587709665298462\nLoss of iteration: 611: 2.605652332305908\nLoss of iteration: 612: 2.5983166694641113\nLoss of iteration: 613: 2.592442274093628\nLoss of iteration: 614: 2.5875234603881836\nLoss of iteration: 615: 2.604281425476074\nLoss of iteration: 616: 2.5984878540039062\nLoss of iteration: 617: 2.594264268875122\nLoss of iteration: 618: 2.587226152420044\nLoss of iteration: 619: 2.6023948192596436\nLoss of iteration: 620: 2.5987837314605713\nLoss of iteration: 621: 2.5967905521392822\nLoss of iteration: 622: 2.5867316722869873\nLoss of iteration: 623: 2.599485397338867\nLoss of iteration: 624: 2.59930682182312\nLoss of iteration: 625: 2.600637197494507\nLoss of iteration: 626: 2.585965394973755\nLoss of iteration: 627: 2.5947043895721436\nLoss of iteration: 628: 2.600055456161499\nLoss of iteration: 629: 2.6064510345458984\nLoss of iteration: 630: 2.585284948348999\nLoss of iteration: 631: 2.5875234603881836\nLoss of iteration: 632: 2.5996651649475098\nLoss of iteration: 633: 2.61246657371521\nLoss of iteration: 634: 2.5867390632629395\nLoss of iteration: 635: 2.5813214778900146\nLoss of iteration: 636: 2.595006227493286\nLoss of iteration: 637: 2.612072706222534\nLoss of iteration: 638: 2.5919463634490967\nLoss of iteration: 639: 2.5827696323394775\nLoss of iteration: 640: 2.5919978618621826\nLoss of iteration: 641: 2.6120409965515137\nLoss of iteration: 642: 2.595162868499756\nLoss of iteration: 643: 2.583552360534668\nLoss of iteration: 644: 2.5895562171936035\nLoss of iteration: 645: 2.610692262649536\nLoss of iteration: 646: 2.597172737121582\nLoss of iteration: 647: 2.585761547088623\nLoss of iteration: 648: 2.5884039402008057\nLoss of iteration: 649: 2.609220504760742\nLoss of iteration: 650: 2.597872018814087\nLoss of iteration: 651: 2.587792158126831\nLoss of iteration: 652: 2.5880112648010254\nLoss of iteration: 653: 2.607966184616089\nLoss of iteration: 654: 2.598090887069702\nLoss of iteration: 655: 2.5894265174865723\nLoss of iteration: 656: 2.5878517627716064\nLoss of iteration: 657: 2.6068546772003174\nLoss of iteration: 658: 2.598198890686035\nLoss of iteration: 659: 2.590866804122925\nLoss of iteration: 660: 2.5877180099487305\nLoss of iteration: 661: 2.6057186126708984\nLoss of iteration: 662: 2.598309278488159\nLoss of iteration: 663: 2.592355251312256\nLoss of iteration: 664: 2.587536096572876\nLoss of iteration: 665: 2.6043660640716553\nLoss of iteration: 666: 2.5984761714935303\nLoss of iteration: 667: 2.594151258468628\nLoss of iteration: 668: 2.5872464179992676\nLoss of iteration: 669: 2.602518081665039\nLoss of iteration: 670: 2.5987627506256104\nLoss of iteration: 671: 2.5966250896453857\nLoss of iteration: 672: 2.586765766143799\nLoss of iteration: 673: 2.5996837615966797\nLoss of iteration: 674: 2.599270820617676\nLoss of iteration: 675: 2.600379467010498\nLoss of iteration: 676: 2.586012840270996\nLoss of iteration: 677: 2.5950303077697754\nLoss of iteration: 678: 2.600017786026001\nLoss of iteration: 679: 2.606086492538452\nLoss of iteration: 680: 2.5852959156036377\nLoss of iteration: 681: 2.5879523754119873\nLoss of iteration: 682: 2.5997841358184814\nLoss of iteration: 683: 2.6122498512268066\nLoss of iteration: 684: 2.5865516662597656\nLoss of iteration: 685: 2.5815014839172363\nLoss of iteration: 686: 2.595296621322632\nLoss of iteration: 687: 2.612208604812622\nLoss of iteration: 688: 2.5916495323181152\nLoss of iteration: 689: 2.582554817199707\nLoss of iteration: 690: 2.592133045196533\nLoss of iteration: 691: 2.6120660305023193\nLoss of iteration: 692: 2.5950326919555664\nLoss of iteration: 693: 2.583491086959839\nLoss of iteration: 694: 2.5896451473236084\nLoss of iteration: 695: 2.61076283454895\nLoss of iteration: 696: 2.5971109867095947\nLoss of iteration: 697: 2.58565616607666\nLoss of iteration: 698: 2.5884387493133545\nLoss of iteration: 699: 2.6092886924743652\nLoss of iteration: 700: 2.5978524684906006\nLoss of iteration: 701: 2.587700843811035\nLoss of iteration: 702: 2.5880229473114014\nLoss of iteration: 703: 2.608024835586548\nLoss of iteration: 704: 2.59808349609375\nLoss of iteration: 705: 2.589350700378418\nLoss of iteration: 706: 2.5878586769104004\nLoss of iteration: 707: 2.606910467147827\nLoss of iteration: 708: 2.598193407058716\nLoss of iteration: 709: 2.590794324874878\nLoss of iteration: 710: 2.5877256393432617\nLoss of iteration: 711: 2.6057798862457275\nLoss of iteration: 712: 2.5983026027679443\nLoss of iteration: 713: 2.5922741889953613\nLoss of iteration: 714: 2.5875470638275146\nLoss of iteration: 715: 2.6044442653656006\nLoss of iteration: 716: 2.5984654426574707\nLoss of iteration: 717: 2.5940465927124023\nLoss of iteration: 718: 2.5872652530670166\nLoss of iteration: 719: 2.602630853652954\nLoss of iteration: 720: 2.5987439155578613\nLoss of iteration: 721: 2.5964744091033936\nLoss of iteration: 722: 2.586796283721924\nLoss of iteration: 723: 2.5998640060424805\nLoss of iteration: 724: 2.5992374420166016\nLoss of iteration: 725: 2.600144386291504\nLoss of iteration: 726: 2.58605694770813\nLoss of iteration: 727: 2.595327138900757\nLoss of iteration: 728: 2.5999813079833984\nLoss of iteration: 729: 2.6057496070861816\nLoss of iteration: 730: 2.585310935974121\nLoss of iteration: 731: 2.5883522033691406\nLoss of iteration: 732: 2.5998802185058594\nLoss of iteration: 733: 2.612027168273926\nLoss of iteration: 734: 2.5863895416259766\nLoss of iteration: 735: 2.581693410873413\nLoss of iteration: 736: 2.595576286315918\nLoss of iteration: 737: 2.612342596054077\nLoss of iteration: 738: 2.5913634300231934\nLoss of iteration: 739: 2.5823473930358887\nLoss of iteration: 740: 2.5922529697418213\nLoss of iteration: 741: 2.6120715141296387\nLoss of iteration: 742: 2.5949132442474365\nLoss of iteration: 743: 2.5834543704986572\nLoss of iteration: 744: 2.5897343158721924\nLoss of iteration: 745: 2.6108291149139404\nLoss of iteration: 746: 2.597048044204712\nLoss of iteration: 747: 2.5855553150177\nLoss of iteration: 748: 2.588474750518799\nLoss of iteration: 749: 2.6093554496765137\nLoss of iteration: 750: 2.597832441329956\nLoss of iteration: 751: 2.5876119136810303\nLoss of iteration: 752: 2.5880355834960938\nLoss of iteration: 753: 2.6080820560455322\nLoss of iteration: 754: 2.598076105117798\nLoss of iteration: 755: 2.5892765522003174\nLoss of iteration: 756: 2.587865114212036\nLoss of iteration: 757: 2.606964588165283\nLoss of iteration: 758: 2.5981884002685547\nLoss of iteration: 759: 2.590723991394043\nLoss of iteration: 760: 2.5877327919006348\nLoss of iteration: 761: 2.6058387756347656\nLoss of iteration: 762: 2.5982961654663086\nLoss of iteration: 763: 2.592196226119995\nLoss of iteration: 764: 2.5875580310821533\nLoss of iteration: 765: 2.6045188903808594\nLoss of iteration: 766: 2.598454713821411\nLoss of iteration: 767: 2.593946933746338\nLoss of iteration: 768: 2.58728289604187\nLoss of iteration: 769: 2.602739095687866\nLoss of iteration: 770: 2.5987255573272705\nLoss of iteration: 771: 2.596329689025879\nLoss of iteration: 772: 2.5868256092071533\nLoss of iteration: 773: 2.6000356674194336\nLoss of iteration: 774: 2.5992062091827393\nLoss of iteration: 775: 2.5999200344085693\nLoss of iteration: 776: 2.586099624633789\nLoss of iteration: 777: 2.5956106185913086\nLoss of iteration: 778: 2.5999443531036377\nLoss of iteration: 779: 2.6054234504699707\nLoss of iteration: 780: 2.58532977104187\nLoss of iteration: 781: 2.58874249458313\nLoss of iteration: 782: 2.599961280822754\nLoss of iteration: 783: 2.6117918491363525\nLoss of iteration: 784: 2.586242437362671\nLoss of iteration: 785: 2.581901788711548\nLoss of iteration: 786: 2.595855951309204\nLoss of iteration: 787: 2.612475872039795\nLoss of iteration: 788: 2.5910773277282715\nLoss of iteration: 789: 2.5821433067321777\nLoss of iteration: 790: 2.592365026473999\nLoss of iteration: 791: 2.612061023712158\nLoss of iteration: 792: 2.5947980880737305\nLoss of iteration: 793: 2.5834383964538574\nLoss of iteration: 794: 2.589827537536621\nLoss of iteration: 795: 2.6108949184417725\nLoss of iteration: 796: 2.596980571746826\nLoss of iteration: 797: 2.5854532718658447\nLoss of iteration: 798: 2.5885140895843506\nLoss of iteration: 799: 2.609424114227295\nLoss of iteration: 800: 2.597810745239258\nLoss of iteration: 801: 2.5875203609466553\nLoss of iteration: 802: 2.5880484580993652\nLoss of iteration: 803: 2.608140468597412\nLoss of iteration: 804: 2.5980682373046875\nLoss of iteration: 805: 2.589200735092163\nLoss of iteration: 806: 2.5878725051879883\nLoss of iteration: 807: 2.6070194244384766\nLoss of iteration: 808: 2.5981833934783936\nLoss of iteration: 809: 2.5906524658203125\nLoss of iteration: 810: 2.587739944458008\nLoss of iteration: 811: 2.605898141860962\nLoss of iteration: 812: 2.598289966583252\nLoss of iteration: 813: 2.5921177864074707\nLoss of iteration: 814: 2.587568759918213\nLoss of iteration: 815: 2.604593515396118\nLoss of iteration: 816: 2.598444700241089\nLoss of iteration: 817: 2.5938472747802734\nLoss of iteration: 818: 2.5872998237609863\nLoss of iteration: 819: 2.6028451919555664\nLoss of iteration: 820: 2.598708152770996\nLoss of iteration: 821: 2.596187114715576\nLoss of iteration: 822: 2.586854934692383\nLoss of iteration: 823: 2.6002042293548584\nLoss of iteration: 824: 2.599174976348877\nLoss of iteration: 825: 2.599699020385742\nLoss of iteration: 826: 2.5861425399780273\nLoss of iteration: 827: 2.5958893299102783\nLoss of iteration: 828: 2.5999057292938232\nLoss of iteration: 829: 2.6050984859466553\nLoss of iteration: 830: 2.585352897644043\nLoss of iteration: 831: 2.589134454727173\nLoss of iteration: 832: 2.6000304222106934\nLoss of iteration: 833: 2.6115379333496094\nLoss of iteration: 834: 2.586106777191162\nLoss of iteration: 835: 2.58213210105896\nLoss of iteration: 836: 2.596141815185547\nLoss of iteration: 837: 2.6126086711883545\nLoss of iteration: 838: 2.5907843112945557\nLoss of iteration: 839: 2.5819404125213623\nLoss of iteration: 840: 2.5924735069274902\nLoss of iteration: 841: 2.6120352745056152\nLoss of iteration: 842: 2.5946831703186035\nLoss of iteration: 843: 2.5834403038024902\nLoss of iteration: 844: 2.5899274349212646\nLoss of iteration: 845: 2.6109631061553955\nLoss of iteration: 846: 2.5969064235687256\nLoss of iteration: 847: 2.5853466987609863\nLoss of iteration: 848: 2.5885567665100098\nLoss of iteration: 849: 2.6094958782196045\nLoss of iteration: 850: 2.5977866649627686\nLoss of iteration: 851: 2.58742356300354\nLoss of iteration: 852: 2.5880627632141113\nLoss of iteration: 853: 2.6082022190093994\nLoss of iteration: 854: 2.598059892654419\nLoss of iteration: 855: 2.589120626449585\nLoss of iteration: 856: 2.5878798961639404\nLoss of iteration: 857: 2.607076644897461\nLoss of iteration: 858: 2.598177909851074\nLoss of iteration: 859: 2.5905778408050537\nLoss of iteration: 860: 2.587747812271118\nLoss of iteration: 861: 2.6059601306915283\nLoss of iteration: 862: 2.598283052444458\nLoss of iteration: 863: 2.592036247253418\nLoss of iteration: 864: 2.5875797271728516\nLoss of iteration: 865: 2.604671001434326\nLoss of iteration: 866: 2.5984344482421875\nLoss of iteration: 867: 2.5937442779541016\nLoss of iteration: 868: 2.587317705154419\nLoss of iteration: 869: 2.6029551029205322\nLoss of iteration: 870: 2.5986897945404053\nLoss of iteration: 871: 2.5960397720336914\nLoss of iteration: 872: 2.5868844985961914\nLoss of iteration: 873: 2.6003775596618652\nLoss of iteration: 874: 2.5991432666778564\nLoss of iteration: 875: 2.599471092224121\nLoss of iteration: 876: 2.5861871242523193\nLoss of iteration: 877: 2.5961761474609375\nLoss of iteration: 878: 2.599864959716797\nLoss of iteration: 879: 2.6047606468200684\nLoss of iteration: 880: 2.585380792617798\nLoss of iteration: 881: 2.5895442962646484\nLoss of iteration: 882: 2.600090742111206\nLoss of iteration: 883: 2.6112546920776367\nLoss of iteration: 884: 2.5859758853912354\nLoss of iteration: 885: 2.582395076751709\nLoss of iteration: 886: 2.596445083618164\nLoss of iteration: 887: 2.612744092941284\nLoss of iteration: 888: 2.590472459793091\nLoss of iteration: 889: 2.581735610961914\nLoss of iteration: 890: 2.5925841331481934\nLoss of iteration: 891: 2.611992835998535\nLoss of iteration: 892: 2.594561815261841\nLoss of iteration: 893: 2.5834600925445557\nLoss of iteration: 894: 2.5900399684906006\nLoss of iteration: 895: 2.6110363006591797\nLoss of iteration: 896: 2.5968217849731445\nLoss of iteration: 897: 2.585230588912964\nLoss of iteration: 898: 2.588606119155884\nLoss of iteration: 899: 2.609575033187866\nLoss of iteration: 900: 2.5977590084075928\nLoss of iteration: 901: 2.5873169898986816\nLoss of iteration: 902: 2.5880796909332275\nLoss of iteration: 903: 2.6082699298858643\nLoss of iteration: 904: 2.598050355911255\nLoss of iteration: 905: 2.5890331268310547\nLoss of iteration: 906: 2.587887763977051\nLoss of iteration: 907: 2.6071393489837646\nLoss of iteration: 908: 2.5981719493865967\nLoss of iteration: 909: 2.590496778488159\nLoss of iteration: 910: 2.5877559185028076\nLoss of iteration: 911: 2.6060268878936768\nLoss of iteration: 912: 2.5982768535614014\nLoss of iteration: 913: 2.5919482707977295\nLoss of iteration: 914: 2.5875916481018066\nLoss of iteration: 915: 2.6047534942626953\nLoss of iteration: 916: 2.598423957824707\nLoss of iteration: 917: 2.5936343669891357\nLoss of iteration: 918: 2.587336778640747\nLoss of iteration: 919: 2.6030712127685547\nLoss of iteration: 920: 2.5986711978912354\nLoss of iteration: 921: 2.595884323120117\nLoss of iteration: 922: 2.5869157314300537\nLoss of iteration: 923: 2.600560188293457\nLoss of iteration: 924: 2.5991098880767822\nLoss of iteration: 925: 2.5992307662963867\nLoss of iteration: 926: 2.5862345695495605\nLoss of iteration: 927: 2.59647798538208\nLoss of iteration: 928: 2.599820137023926\nLoss of iteration: 929: 2.6044018268585205\nLoss of iteration: 930: 2.585414171218872\nLoss of iteration: 931: 2.5899837017059326\nLoss of iteration: 932: 2.600142240524292\nLoss of iteration: 933: 2.6109321117401123\nLoss of iteration: 934: 2.5858495235443115\nLoss of iteration: 935: 2.5827012062072754\nLoss of iteration: 936: 2.596770763397217\nLoss of iteration: 937: 2.6128787994384766\nLoss of iteration: 938: 2.590134859085083\nLoss of iteration: 939: 2.581528902053833\nLoss of iteration: 940: 2.5927016735076904\nLoss of iteration: 941: 2.6119346618652344\nLoss of iteration: 942: 2.5944292545318604\nLoss of iteration: 943: 2.583495855331421\nLoss of iteration: 944: 2.5901691913604736\nLoss of iteration: 945: 2.611117124557495\nLoss of iteration: 946: 2.596722364425659\nLoss of iteration: 947: 2.5851011276245117\nLoss of iteration: 948: 2.588663339614868\nLoss of iteration: 949: 2.609663724899292\nLoss of iteration: 950: 2.597726345062256\nLoss of iteration: 951: 2.5871968269348145\nLoss of iteration: 952: 2.5880985260009766\nLoss of iteration: 953: 2.6083450317382812\nLoss of iteration: 954: 2.598039388656616\nLoss of iteration: 955: 2.588934898376465\nLoss of iteration: 956: 2.5878968238830566\nLoss of iteration: 957: 2.607208013534546\nLoss of iteration: 958: 2.598165512084961\nLoss of iteration: 959: 2.590406894683838\nLoss of iteration: 960: 2.5877649784088135\nLoss of iteration: 961: 2.606100082397461\nLoss of iteration: 962: 2.598269462585449\nLoss of iteration: 963: 2.5918524265289307\nLoss of iteration: 964: 2.58760404586792\nLoss of iteration: 965: 2.6048424243927\nLoss of iteration: 966: 2.598412036895752\nLoss of iteration: 967: 2.593515157699585\nLoss of iteration: 968: 2.5873568058013916\nLoss of iteration: 969: 2.603196620941162\nLoss of iteration: 970: 2.5986509323120117\nLoss of iteration: 971: 2.5957162380218506\nLoss of iteration: 972: 2.586948871612549\nLoss of iteration: 973: 2.6007559299468994\nLoss of iteration: 974: 2.599073886871338\nLoss of iteration: 975: 2.5989718437194824\nLoss of iteration: 976: 2.5862858295440674\nLoss of iteration: 977: 2.5968029499053955\nLoss of iteration: 978: 2.5997695922851562\nLoss of iteration: 979: 2.604010581970215\nLoss of iteration: 980: 2.5854551792144775\nLoss of iteration: 981: 2.5904648303985596\nLoss of iteration: 982: 2.600184440612793\nLoss of iteration: 983: 2.6105570793151855\nLoss of iteration: 984: 2.585726261138916\nLoss of iteration: 985: 2.583065986633301\nLoss of iteration: 986: 2.5971264839172363\nLoss of iteration: 987: 2.6130080223083496\nLoss of iteration: 988: 2.5897629261016846\nLoss of iteration: 989: 2.581324577331543\nLoss of iteration: 990: 2.5928332805633545\nLoss of iteration: 991: 2.6118602752685547\nLoss of iteration: 992: 2.594278573989868\nLoss of iteration: 993: 2.583545207977295\nLoss of iteration: 994: 2.5903210639953613\nLoss of iteration: 995: 2.6112074851989746\nLoss of iteration: 996: 2.5966033935546875\nLoss of iteration: 997: 2.584954261779785\nLoss of iteration: 998: 2.5887322425842285\nLoss of iteration: 999: 2.60976505279541\n</pre> In\u00a0[78]: Copied! <pre>## 3. let's put it all together \ng = torch.Generator().manual_seed(2147483647) \n## Inferencing\nfor i in range(10): \n    out = [] \n    ix = 0 \n\n    while True: \n        xnec_test = F.one_hot(torch.tensor([ix]), num_classes=35).float()\n        logits = xnec_test @ W \n        count = logits.exp() \n        p = count/count.sum(1, keepdims = True)\n        ix = torch.multinomial(p, num_samples = 1, replacement = True, generator= g).item() # doubt -&gt; after getting softmax why we are again sampling from the softmax distribution? \n        out.append(itos[ix]) \n\n        if ix == 0: break; \n\n    print(\"\".join(out))\n</pre> ## 3. let's put it all together  g = torch.Generator().manual_seed(2147483647)  ## Inferencing for i in range(10):      out = []      ix = 0       while True:          xnec_test = F.one_hot(torch.tensor([ix]), num_classes=35).float()         logits = xnec_test @ W          count = logits.exp()          p = count/count.sum(1, keepdims = True)         ix = torch.multinomial(p, num_samples = 1, replacement = True, generator= g).item() # doubt -&gt; after getting softmax why we are again sampling from the softmax distribution?          out.append(itos[ix])           if ix == 0: break;       print(\"\".join(out))  <pre>butack.\nbiching.\nh.\nbingseaves.\nbaledlerasweriorenketfucktatadenataconggaterass.\nuk.\nbiafur.\ncolerennemelos2g.\nleeber.\nfuk.\n</pre> In\u00a0[\u00a0]: Copied! <pre># If you notice neural network output and bigram output are same. \n# basically both are doing the same but we came the solution with different angle. \n# because of bigram model, instead of giving bigram representation, here we are giving neural network representation\n</pre> # If you notice neural network output and bigram output are same.  # basically both are doing the same but we came the solution with different angle.  # because of bigram model, instead of giving bigram representation, here we are giving neural network representation  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"01-MLP/","title":"02. Build MLP","text":"In\u00a0[1]: Copied! <pre>## Implementing the MLP character level model for words by following this paper  https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n## This is the first paper introduced for language modelling with MLP. Let's implement this \n\nprint(\n\"\"\"\nRoughly this paper states that finding the next character using the previous n characters. \n\nFor Example: n=3, xs = \"abc\", ys = \"d\" \n             n=5, xs = \"abcde\", ys = \"f\" \n\"\"\"\n)\n</pre> ## Implementing the MLP character level model for words by following this paper  https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf ## This is the first paper introduced for language modelling with MLP. Let's implement this   print( \"\"\" Roughly this paper states that finding the next character using the previous n characters.   For Example: n=3, xs = \"abc\", ys = \"d\"               n=5, xs = \"abcde\", ys = \"f\"  \"\"\" )  <pre>\nRoughly this paper states that finding the next character using the previous n characters. \n\nFor Example: n=3, xs = \"abc\", ys = \"d\" \n             n=5, xs = \"abcde\", ys = \"f\" \n\n</pre> In\u00a0[2]: Copied! <pre># architecture image \nfrom PIL import Image \nimport urllib\n\nImage.open(urllib.request.urlopen(\"https://pbs.twimg.com/media/Fhzl42hVUAI9U8V.jpg:large\")).resize((800, 600))\n\n\n# See bottom to top \n## We have 3 layer of input embedding (block size is 3)\n## one hidden layer + tanh \n## output layer corresponding to the output size + softamx \n\n### Note! We are going to implement he neural network based on this architecutre only ;)\n</pre> # architecture image  from PIL import Image  import urllib  Image.open(urllib.request.urlopen(\"https://pbs.twimg.com/media/Fhzl42hVUAI9U8V.jpg:large\")).resize((800, 600))   # See bottom to top  ## We have 3 layer of input embedding (block size is 3) ## one hidden layer + tanh  ## output layer corresponding to the output size + softamx   ### Note! We are going to implement he neural network based on this architecutre only ;) Out[2]: In\u00a0[115]: Copied! <pre>import torch \nimport torch.nn.functional as F \nimport matplotlib.pyplot as plt\n</pre> import torch  import torch.nn.functional as F  import matplotlib.pyplot as plt In\u00a0[116]: Copied! <pre>## exploring the dataset \nwords = open(\"names.txt\", \"r\").read().splitlines() \n\nprint(f\"Len of words: {len(words)}\") \nprint(f\"Words: {words[:10]}\")\n\n## Unique characters\nchars = sorted(list(set(\"\".join(words)))) \nprint(f\"Len of unique characters: {len(chars)}\") \n\n## building encoder, decoder like label2id and id2label \nstoi = { v:k+1 for k, v in enumerate(chars)}\nstoi['.'] = 0\nitos = { v:k for k, v in stoi.items()}\nprint(f'Itos:\\n{itos}')\n</pre> ## exploring the dataset  words = open(\"names.txt\", \"r\").read().splitlines()   print(f\"Len of words: {len(words)}\")  print(f\"Words: {words[:10]}\")  ## Unique characters chars = sorted(list(set(\"\".join(words))))  print(f\"Len of unique characters: {len(chars)}\")   ## building encoder, decoder like label2id and id2label  stoi = { v:k+1 for k, v in enumerate(chars)} stoi['.'] = 0 itos = { v:k for k, v in stoi.items()} print(f'Itos:\\n{itos}') <pre>Len of words: 32033\nWords: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\nLen of unique characters: 26\nItos:\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n</pre> In\u00a0[5]: Copied! <pre># Building the dataset \nblock_size = 3  # Total number of previous characters to predict the next character \n\nX, Y = [], []  # X is the input to the neural net, Y is the output of the corresponding X to the neural net \nfor w in words[:5]: \n\n    context = [0] * block_size  # if n = 3 [0, 0, 0]\n    print(w)\n    for ch in w + \".\": \n        ix = stoi[ch]\n\n        print(context, ix)\n        X.append(context)\n        Y.append(ix)\n\n        print(\"\".join(itos[i] for i in context), '----&gt;', itos[ix])\n        context = context[1:] + [ix]   # it's like rolling window \n\n    print(\"---\"*5)\n\n## This code tells us how the single word is converting to character level inputs \n\nX = torch.tensor(X)\nY = torch.tensor(Y)\nprint(\"XShape\", X.shape, \"YShape\", Y.shape)\n</pre> # Building the dataset  block_size = 3  # Total number of previous characters to predict the next character   X, Y = [], []  # X is the input to the neural net, Y is the output of the corresponding X to the neural net  for w in words[:5]:       context = [0] * block_size  # if n = 3 [0, 0, 0]     print(w)     for ch in w + \".\":          ix = stoi[ch]          print(context, ix)         X.append(context)         Y.append(ix)          print(\"\".join(itos[i] for i in context), '----&gt;', itos[ix])         context = context[1:] + [ix]   # it's like rolling window       print(\"---\"*5)  ## This code tells us how the single word is converting to character level inputs   X = torch.tensor(X) Y = torch.tensor(Y) print(\"XShape\", X.shape, \"YShape\", Y.shape) <pre>emma\n[0, 0, 0] 5\n... ----&gt; e\n[0, 0, 5] 13\n..e ----&gt; m\n[0, 5, 13] 13\n.em ----&gt; m\n[5, 13, 13] 1\nemm ----&gt; a\n[13, 13, 1] 0\nmma ----&gt; .\n---------------\nolivia\n[0, 0, 0] 15\n... ----&gt; o\n[0, 0, 15] 12\n..o ----&gt; l\n[0, 15, 12] 9\n.ol ----&gt; i\n[15, 12, 9] 22\noli ----&gt; v\n[12, 9, 22] 9\nliv ----&gt; i\n[9, 22, 9] 1\nivi ----&gt; a\n[22, 9, 1] 0\nvia ----&gt; .\n---------------\nava\n[0, 0, 0] 1\n... ----&gt; a\n[0, 0, 1] 22\n..a ----&gt; v\n[0, 1, 22] 1\n.av ----&gt; a\n[1, 22, 1] 0\nava ----&gt; .\n---------------\nisabella\n[0, 0, 0] 9\n... ----&gt; i\n[0, 0, 9] 19\n..i ----&gt; s\n[0, 9, 19] 1\n.is ----&gt; a\n[9, 19, 1] 2\nisa ----&gt; b\n[19, 1, 2] 5\nsab ----&gt; e\n[1, 2, 5] 12\nabe ----&gt; l\n[2, 5, 12] 12\nbel ----&gt; l\n[5, 12, 12] 1\nell ----&gt; a\n[12, 12, 1] 0\nlla ----&gt; .\n---------------\nsophia\n[0, 0, 0] 19\n... ----&gt; s\n[0, 0, 19] 15\n..s ----&gt; o\n[0, 19, 15] 16\n.so ----&gt; p\n[19, 15, 16] 8\nsop ----&gt; h\n[15, 16, 8] 9\noph ----&gt; i\n[16, 8, 9] 1\nphi ----&gt; a\n[8, 9, 1] 0\nhia ----&gt; .\n---------------\nXShape torch.Size([32, 3]) YShape torch.Size([32])\n</pre> In\u00a0[6]: Copied! <pre># Now we have created a input data, let's create an embedding to store this inupts and outputs: \n## Here we have 27 unique characters and we are going to embed in low dimensional space. \n\n### Let's create a two dimensional embeddings (we are initializing randomly)\nC = torch.randn( (27, 2) )  # 27 characters and 2 dimensional embeddings \n\n### Because it's a random number if you want to get the embedding for character \"e\", just get the index of \"e\" is 5 and do the indexing in C C[5] value is the\n### -- embedding. Later this embeddings will learned by back-propagation. Initially this is just a random numbers. \n\n# Embedding of character \"e\"\nprint(C[stoi[\"m\"]])\n</pre> # Now we have created a input data, let's create an embedding to store this inupts and outputs:  ## Here we have 27 unique characters and we are going to embed in low dimensional space.   ### Let's create a two dimensional embeddings (we are initializing randomly) C = torch.randn( (27, 2) )  # 27 characters and 2 dimensional embeddings   ### Because it's a random number if you want to get the embedding for character \"e\", just get the index of \"e\" is 5 and do the indexing in C C[5] value is the ### -- embedding. Later this embeddings will learned by back-propagation. Initially this is just a random numbers.   # Embedding of character \"e\" print(C[stoi[\"m\"]])  <pre>tensor([1.5654, 1.4854])\n</pre> In\u00a0[7]: Copied! <pre>## But the pytorch indexing is super powerfull, we can give two dimenional array to indexing, it can index. \n## It means we don't need a for loop to iterate over \"X\"(input), we can just give the \"X\" to the tensor and we get all the indexed elements.  \n\nemb = C[X]  # X is the input of the model \nprint(emb.shape)\n\n## 32 is row of X or len of unique char, 3 is the block size, 2 is the embedding size.\n</pre> ## But the pytorch indexing is super powerfull, we can give two dimenional array to indexing, it can index.  ## It means we don't need a for loop to iterate over \"X\"(input), we can just give the \"X\" to the tensor and we get all the indexed elements.    emb = C[X]  # X is the input of the model  print(emb.shape)  ## 32 is row of X or len of unique char, 3 is the block size, 2 is the embedding size.    <pre>torch.Size([32, 3, 2])\n</pre> In\u00a0[8]: Copied! <pre>## Here we are having 2 dimensional space of embedding and block size of 3 | Each character has 2 dimensional embeddings and we are taking 3 character to find the next character so input is 6 \nW1 = torch.randn( (emb[0].numel(), 100))  # emb[0].numel():6 = 3 character(blockSize) * 2 size embeddings, 100 is the next layer neuron \nb1 = torch.randn( 100 )\n</pre> ## Here we are having 2 dimensional space of embedding and block size of 3 | Each character has 2 dimensional embeddings and we are taking 3 character to find the next character so input is 6  W1 = torch.randn( (emb[0].numel(), 100))  # emb[0].numel():6 = 3 character(blockSize) * 2 size embeddings, 100 is the next layer neuron  b1 = torch.randn( 100 )  In\u00a0[9]: Copied! <pre># our aim is multiply emb@W + b but here the shape of emb and W shape is  different. So we can do matric multiplication between those. \n# emb shape: [32, 3, 2], W shape: [6, 100] both are differnet. \n# We need to change or reshape to do perform matrix mul. \n# In emb shape we try to concatenate last 2 shape [32, 3*2] = [32, 6]. Now we can do matrix multiplication \n\n## 1st way to do this reshaping \n# To do that we need to take all the embeddings separately and combine them. \n## To separate all the embedding dimension separtely we use \"torch.unbind(emb, &lt;dim-to-get-separately&gt;)\" \n## to combine all this embedding we use \"torch.cat((&lt;list-of-tensor-to-combine&gt;), dim = &lt;which-dim-need-to-concatenate&gt;)\"\n\n## 2nd way \n# To do that we can simply call the view function to reshape the embeddings \n\n## 1st way \nhard = torch.cat(torch.unbind(emb, 1), dim = 1)  ## this is in-efficient way to do that\n\n## 2nd way \neasy = emb.view(-1, emb[0].numel())\n\n# comparing the both\n# print(hard == easy)\n</pre> # our aim is multiply emb@W + b but here the shape of emb and W shape is  different. So we can do matric multiplication between those.  # emb shape: [32, 3, 2], W shape: [6, 100] both are differnet.  # We need to change or reshape to do perform matrix mul.  # In emb shape we try to concatenate last 2 shape [32, 3*2] = [32, 6]. Now we can do matrix multiplication   ## 1st way to do this reshaping  # To do that we need to take all the embeddings separately and combine them.  ## To separate all the embedding dimension separtely we use \"torch.unbind(emb, )\"  ## to combine all this embedding we use \"torch.cat((), dim = )\"  ## 2nd way  # To do that we can simply call the view function to reshape the embeddings   ## 1st way  hard = torch.cat(torch.unbind(emb, 1), dim = 1)  ## this is in-efficient way to do that  ## 2nd way  easy = emb.view(-1, emb[0].numel())  # comparing the both # print(hard == easy) In\u00a0[10]: Copied! <pre># Let's do the forward pass \n\nh = torch.tanh( emb.view(-1, emb[0].numel()) @ W1 + b1) # first hidden layer \n\n## Let's implement 2nd hidden layer (ouput + softmax)\n</pre> # Let's do the forward pass   h = torch.tanh( emb.view(-1, emb[0].numel()) @ W1 + b1) # first hidden layer   ## Let's implement 2nd hidden layer (ouput + softmax) In\u00a0[11]: Copied! <pre>## second hidden layer \nW2 = torch.randn((100, 27))  # 100 is previous layer size  \nb2 = torch.randn(27) \n\nlogits = h@W2 + b2  # log of counts \ncounts = logits.exp()\nprob = counts /counts.sum(dim=1, keepdim=True) \nloss = -prob[torch.arange(32), Y].log().mean()  # negative log likelihood \nprint(f\"Negative log likelihood: {loss}\")\n\n\n## we can also do this softmax by using the cross_entropy_function in the pytroch \nprint(f\"Cross-entropy using F.cross_entropy: {F.cross_entropy(logits, Y)}\")\n</pre> ## second hidden layer  W2 = torch.randn((100, 27))  # 100 is previous layer size   b2 = torch.randn(27)   logits = h@W2 + b2  # log of counts  counts = logits.exp() prob = counts /counts.sum(dim=1, keepdim=True)  loss = -prob[torch.arange(32), Y].log().mean()  # negative log likelihood  print(f\"Negative log likelihood: {loss}\")   ## we can also do this softmax by using the cross_entropy_function in the pytroch  print(f\"Cross-entropy using F.cross_entropy: {F.cross_entropy(logits, Y)}\") <pre>Negative log likelihood: 12.303730010986328\nCross-entropy using F.cross_entropy: 12.303729057312012\n</pre> In\u00a0[12]: Copied! <pre>\"\"\" \nWhy F.cross_entropy is good compare to our own implementation ? \n\n-&gt; First of all it don't create a itermediate tensors, Instead pytorch creates cluster of this operations. \n-&gt; Backword operations is much more easire than our own implementation. \n-&gt; It's more efficient compare to our own implementation. \n\n    For Example: If the tensor values are [200, 300, 400], applying the exponential function to them results in [-inf, -inf, -inf]\n    because as the input values increase, the output approaches higher infinity. This problem is solved in F.cross_entorpy \n\n    The approach used to solve the problem involves finding the maximum number in the given set and subtracting that maximum value from all\n    the elements.\n    By doing so, the maximum value becomes 0, and remain values get shifted negative or positive, the problem is then resolved. \n    It is worth noting that while applying the exponential function to numbers below 1 does not pose any issues, higher numbers \n    above 1 can result in certain challenges.\n\n\"\"\"\n</pre> \"\"\"  Why F.cross_entropy is good compare to our own implementation ?   -&gt; First of all it don't create a itermediate tensors, Instead pytorch creates cluster of this operations.  -&gt; Backword operations is much more easire than our own implementation.  -&gt; It's more efficient compare to our own implementation.       For Example: If the tensor values are [200, 300, 400], applying the exponential function to them results in [-inf, -inf, -inf]     because as the input values increase, the output approaches higher infinity. This problem is solved in F.cross_entorpy       The approach used to solve the problem involves finding the maximum number in the given set and subtracting that maximum value from all     the elements.     By doing so, the maximum value becomes 0, and remain values get shifted negative or positive, the problem is then resolved.      It is worth noting that while applying the exponential function to numbers below 1 does not pose any issues, higher numbers      above 1 can result in certain challenges.  \"\"\" Out[12]: <pre>\" \\nWhy F.cross_entropy is good compare to our own implementation ? \\n\\n-&gt; First of all it don't create a itermediate tensors, Instead pytorch creates cluster of this operations. \\n-&gt; Backword operations is much more easire than our own implementation. \\n-&gt; It's more efficient compare to our own implementation. \\n\\n    For Example: If the tensor values are [200, 300, 400], applying the exponential function to them results in [-inf, -inf, -inf]\\n    because as the input values increase, the output approaches higher infinity. This problem is solved in F.cross_entorpy \\n\\n    The approach used to solve the problem involves finding the maximum number in the given set and subtracting that maximum value from all\\n    the elements.\\n    By doing so, the maximum value becomes 0, and remain values get shifted negative or positive, the problem is then resolved. \\n    It is worth noting that while applying the exponential function to numbers below 1 does not pose any issues, higher numbers \\n    above 1 can result in certain challenges.\\n\\n\"</pre> In\u00a0[119]: Copied! <pre>## Let's set everything to the forward pass \n\n# Building the dataset \nblock_size = 3  # Total number of previous characters to predict the next character \n\nX, Y = [], [] \nfor w in words: \n    context = [0] * block_size  \n    for ch in w + \".\": \n        ix = stoi[ch]\n        X.append(context)\n        Y.append(ix)\n        context = context[1:] + [ix]   \n\nX = torch.tensor(X)\nY = torch.tensor(Y)\n\nprint(\"XShape\", X.shape, \"YShape\", Y.shape)\n\n# 1. Setting up the paramters \ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn( (27, 2) )  # Embedding tensor \nb1 = torch.randn(100, generator=g)\nW1 = torch.randn( (6, 100), generator=g)\nb2 = torch.randn(27, generator=g)\nW2 = torch.randn( (100, 27), generator=g) \n\nparameters = [C, b1, b2, W1, W2] \n\nprint(f\"Total of paramters: {sum(p.nelement() for p in parameters)}\")\n\n## making all require grads = True \nfor p in parameters:\n    p.requires_grad=True\n</pre> ## Let's set everything to the forward pass   # Building the dataset  block_size = 3  # Total number of previous characters to predict the next character   X, Y = [], []  for w in words:      context = [0] * block_size       for ch in w + \".\":          ix = stoi[ch]         X.append(context)         Y.append(ix)         context = context[1:] + [ix]     X = torch.tensor(X) Y = torch.tensor(Y)  print(\"XShape\", X.shape, \"YShape\", Y.shape)  # 1. Setting up the paramters  g = torch.Generator().manual_seed(2147483647) C = torch.randn( (27, 2) )  # Embedding tensor  b1 = torch.randn(100, generator=g) W1 = torch.randn( (6, 100), generator=g) b2 = torch.randn(27, generator=g) W2 = torch.randn( (100, 27), generator=g)   parameters = [C, b1, b2, W1, W2]   print(f\"Total of paramters: {sum(p.nelement() for p in parameters)}\")  ## making all require grads = True  for p in parameters:     p.requires_grad=True  <pre>XShape torch.Size([228146, 3]) YShape torch.Size([228146])\nTotal of paramters: 3481\n</pre> In\u00a0[14]: Copied! <pre># 2. Training loop \n\nfor _ in range(10000): \n\n    ## forward pass \n    ## Doing forward pass in all the data is very bad, so we take the batch and do the computation \n    ix = torch.randint(0, X.shape[0], (32,) )\n    emb = C[X[ix]]  # mini batch\n    h = torch.tanh( emb.view(-1, 6)@W1 + b1) \n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, Y[ix]) \n\n    ## backward pass \n    for p in parameters: \n        p.grad = None \n    loss.backward()\n\n    ## update \n    for p in parameters: \n        p.data += -0.001 * p.grad \n\n# all loss \nix = torch.randint(0, X.shape[0], (32,) )\nemb = C[X]  # mini batch\nh = torch.tanh( emb.view(-1, 6)@W1 + b1) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Y)\nprint(f\"Overall loss: {loss}\")\n</pre> # 2. Training loop   for _ in range(10000):       ## forward pass      ## Doing forward pass in all the data is very bad, so we take the batch and do the computation      ix = torch.randint(0, X.shape[0], (32,) )     emb = C[X[ix]]  # mini batch     h = torch.tanh( emb.view(-1, 6)@W1 + b1)      logits = h @ W2 + b2      loss = F.cross_entropy(logits, Y[ix])       ## backward pass      for p in parameters:          p.grad = None      loss.backward()      ## update      for p in parameters:          p.data += -0.001 * p.grad   # all loss  ix = torch.randint(0, X.shape[0], (32,) ) emb = C[X]  # mini batch h = torch.tanh( emb.view(-1, 6)@W1 + b1)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Y) print(f\"Overall loss: {loss}\")  <pre>Overall loss: 3.51939058303833\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[15]: Copied! <pre># How do we find the optimial learning rate? \n## One way we can calculate by trying with multiple learning rates and finding the optimal lr. \n## But this is very in-efficient way ;(  \n## So, we can create a linear learning array and find the optimal learning rate by looping through all those. \n\nlre = torch.linspace(-3, 0, 1000)  # 1000 points \nlre = 10**lre  ## search in google for this\n</pre> # How do we find the optimial learning rate?  ## One way we can calculate by trying with multiple learning rates and finding the optimal lr.  ## But this is very in-efficient way ;(   ## So, we can create a linear learning array and find the optimal learning rate by looping through all those.   lre = torch.linspace(-3, 0, 1000)  # 1000 points  lre = 10**lre  ## search in google for this    In\u00a0[16]: Copied! <pre># 2. Training loop with learning rate search \n\n\nlri = []\nlosses = []\nfor i in range(1000): \n\n    ## forward pass \n    ## Doing forward pass in all the data is very bad, so we take the batch and do the computation \n    ix = torch.randint(0, X.shape[0], (32,) )\n    emb = C[X[ix]]  # mini batch\n    h = torch.tanh( emb.view(-1, 6)@W1 + b1) \n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, Y[ix]) \n\n    ## backward pass \n    for p in parameters: \n        p.grad = None \n    loss.backward()\n\n    ## update \n    lr = lre[i]\n    for p in parameters: \n        p.data += -lr * p.grad \n\n    lri.append(lr)\n    losses.append(loss.item())\n</pre> # 2. Training loop with learning rate search    lri = [] losses = [] for i in range(1000):       ## forward pass      ## Doing forward pass in all the data is very bad, so we take the batch and do the computation      ix = torch.randint(0, X.shape[0], (32,) )     emb = C[X[ix]]  # mini batch     h = torch.tanh( emb.view(-1, 6)@W1 + b1)      logits = h @ W2 + b2      loss = F.cross_entropy(logits, Y[ix])       ## backward pass      for p in parameters:          p.grad = None      loss.backward()      ## update      lr = lre[i]     for p in parameters:          p.data += -lr * p.grad       lri.append(lr)     losses.append(loss.item()) In\u00a0[17]: Copied! <pre>plt.plot(lri, losses); \n# Here we found the learning rate near 0.01 is very good. So, will use this and train a model :)\n</pre> plt.plot(lri, losses);  # Here we found the learning rate near 0.01 is very good. So, will use this and train a model :)  In\u00a0[141]: Copied! <pre>## Currently we are overfitting the model because we are giving all the data to model for training. So, let's split the data to train, test and val \n# train is used to train the parameters\n# val/dev is used to train the hyper-parameters \n# test is used to evaluvate the performance of the model at the end ;) (only allowded to use this model after train/val otherwise model may learn from this data (data-leakage))\n\n# let's split our data : )\n\ndef build_dataset(words): \n    block_size = 3  \n    X, Y = [], [] \n    for w in words: \n        context = [0] * block_size  \n        for ch in w + \".\": \n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]   \n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n\n    print(\"XShape\", X.shape, \"YShape\", Y.shape)\n    return X, Y \n\nimport random \nrandom.seed(42)\n\nrandom.shuffle(words)\nn1 = int(len(words)*0.8) # calculate the index for 80% of the split \nn2 = int(len(words)*0.9) # calculate the index for 90% of the split \n\n\nXtr, Ytr = build_dataset(words[:n1]) \nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n</pre> ## Currently we are overfitting the model because we are giving all the data to model for training. So, let's split the data to train, test and val  # train is used to train the parameters # val/dev is used to train the hyper-parameters  # test is used to evaluvate the performance of the model at the end ;) (only allowded to use this model after train/val otherwise model may learn from this data (data-leakage))  # let's split our data : )  def build_dataset(words):      block_size = 3       X, Y = [], []      for w in words:          context = [0] * block_size           for ch in w + \".\":              ix = stoi[ch]             X.append(context)             Y.append(ix)             context = context[1:] + [ix]         X = torch.tensor(X)     Y = torch.tensor(Y)      print(\"XShape\", X.shape, \"YShape\", Y.shape)     return X, Y   import random  random.seed(42)  random.shuffle(words) n1 = int(len(words)*0.8) # calculate the index for 80% of the split  n2 = int(len(words)*0.9) # calculate the index for 90% of the split    Xtr, Ytr = build_dataset(words[:n1])  Xdev, Ydev = build_dataset(words[n1:n2]) Xte, Yte = build_dataset(words[n2:])  <pre>XShape torch.Size([182625, 3]) YShape torch.Size([182625])\nXShape torch.Size([22655, 3]) YShape torch.Size([22655])\nXShape torch.Size([22866, 3]) YShape torch.Size([22866])\n</pre> In\u00a0[142]: Copied! <pre># 1. Setting up the paramters \ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn( (27, 2) )  # Embedding tensor \nb1 = torch.randn(100, generator=g)\nW1 = torch.randn( (6, 100), generator=g)\nb2 = torch.randn(27, generator=g)\nW2 = torch.randn( (100, 27), generator=g) \n\nparameters = [C, b1, b2, W1, W2] \n\nprint(f\"Total of paramters: {sum(p.nelement() for p in parameters)}\")\n\n## making all require grads = True \nfor p in parameters:\n    p.requires_grad=True\n</pre> # 1. Setting up the paramters  g = torch.Generator().manual_seed(2147483647) C = torch.randn( (27, 2) )  # Embedding tensor  b1 = torch.randn(100, generator=g) W1 = torch.randn( (6, 100), generator=g) b2 = torch.randn(27, generator=g) W2 = torch.randn( (100, 27), generator=g)   parameters = [C, b1, b2, W1, W2]   print(f\"Total of paramters: {sum(p.nelement() for p in parameters)}\")  ## making all require grads = True  for p in parameters:     p.requires_grad=True  <pre>Total of paramters: 3481\n</pre> In\u00a0[5]: Copied! <pre># plotting the embeddings \n\nplt.figure(figsize=(8, 8))\n\n# Scatter plot\nplt.scatter(C[:, 0].data, C[:, 1].data, s=150, c='blue', edgecolors='black', alpha=0.7)\n\nfor i in range(C.shape[0]):\n    # Text annotations\n    plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n\n# Axis labels\nplt.xlabel(\"X-axis\", fontsize=12)\nplt.ylabel(\"Y-axis\", fontsize=12)\n\n# Title\nplt.title(\"Scatter Plot\", fontsize=14)\n\n# Gridlines\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# Background color\nplt.gca().set_facecolor('lightgray')\n\n# Adjust padding\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n</pre> # plotting the embeddings   plt.figure(figsize=(8, 8))  # Scatter plot plt.scatter(C[:, 0].data, C[:, 1].data, s=150, c='blue', edgecolors='black', alpha=0.7)  for i in range(C.shape[0]):     # Text annotations     plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")  # Axis labels plt.xlabel(\"X-axis\", fontsize=12) plt.ylabel(\"Y-axis\", fontsize=12)  # Title plt.title(\"Scatter Plot\", fontsize=14)  # Gridlines plt.grid(True, linestyle='--', alpha=0.5)  # Background color plt.gca().set_facecolor('lightgray')  # Adjust padding plt.tight_layout()  # Show the plot plt.show()  In\u00a0[150]: Copied! <pre>C.dtype\n</pre> C.dtype Out[150]: <pre>torch.float32</pre> In\u00a0[12]: Copied! <pre># 2. Training loop \n## Here changing the networks to only train with train split\n## And evaluvating with dev set \n\n\nfor _ in range(20000): \n\n    ## forward pass \n    ## Doing forward pass in all the data is very bad, so we take the batch and do the computation \n    ix = torch.randint(0, Xtr.shape[0], (32,) )\n    emb = C[Xtr[ix]]  # mini batch\n    h = torch.tanh( emb.view(-1, 6)@W1 + b1) \n    logits = h @ W2 + b2 \n    loss = F.cross_entropy(logits, Ytr[ix]) \n\n    ## backward pass \n    for p in parameters: \n        p.grad = None \n    loss.backward()\n\n    ## update \n    for p in parameters: \n        p.data += -0.001 * p.grad \n\n# all loss \nix = torch.randint(0, Xdev.shape[0], (32,) )\nemb = C[Xdev]  # evaluvating with dev set \nh = torch.tanh( emb.view(-1, 6)@W1 + b1) \nlogits = h @ W2 + b2 \nloss = F.cross_entropy(logits, Ydev)\nprint(f\"Overall loss: {loss}\")\n</pre> # 2. Training loop  ## Here changing the networks to only train with train split ## And evaluvating with dev set    for _ in range(20000):       ## forward pass      ## Doing forward pass in all the data is very bad, so we take the batch and do the computation      ix = torch.randint(0, Xtr.shape[0], (32,) )     emb = C[Xtr[ix]]  # mini batch     h = torch.tanh( emb.view(-1, 6)@W1 + b1)      logits = h @ W2 + b2      loss = F.cross_entropy(logits, Ytr[ix])       ## backward pass      for p in parameters:          p.grad = None      loss.backward()      ## update      for p in parameters:          p.data += -0.001 * p.grad   # all loss  ix = torch.randint(0, Xdev.shape[0], (32,) ) emb = C[Xdev]  # evaluvating with dev set  h = torch.tanh( emb.view(-1, 6)@W1 + b1)  logits = h @ W2 + b2  loss = F.cross_entropy(logits, Ydev) print(f\"Overall loss: {loss}\")  <pre>Overall loss: 2.318591833114624\n</pre> In\u00a0[13]: Copied! <pre># 3. inference \nblock_size = 3\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      emb = C[torch.tensor([context])] # (1,block_size,d)\n      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n      logits = h @ W2 + b2\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out))\n</pre> # 3. inference  block_size = 3 for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       emb = C[torch.tensor([context])] # (1,block_size,d)       h = torch.tanh(emb.view(1, -1) @ W1 + b1)       logits = h @ W2 + b2       probs = F.softmax(logits, dim=1)       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       context = context[1:] + [ix]       out.append(ix)       if ix == 0:         break          print(''.join(itos[i] for i in out)) <pre>meycan.\nmadhevcoriyo.\nsul.\njundy.\nivye.\ntailanye.\nbronkti.\nmolssezianah.\nshat.\nluy.\nlan.\nkeadaf.\ntcinynie.\nazeeklyn.\ndoly.\ndylin.\njimahtastadakel.\nbry.\nsiranynd.\nkaliw.\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Now we completed the whole architecture of the model and tuned the model according to the model :) \n\n### Now you need to explore the hyper parameters like changing the hidden layer, batch size, increasing the embedding dimension from 2 to n like 512\n</pre> ## Now we completed the whole architecture of the model and tuned the model according to the model :)   ### Now you need to explore the hyper parameters like changing the hidden layer, batch size, increasing the embedding dimension from 2 to n like 512      In\u00a0[142]: Copied! <pre>import torch\nimport random \nfrom tqdm.auto import tqdm \nimport torch.nn.functional as F \nimport matplotlib.pyplot as plt\n\n#A Neural Probabilistic Language Model\nclass NPLM: \n\n    def __init__(self, text_path, model_parameters): \n        self.text_path = text_path \n        self.model_parameters = model_parameters\n        self.update_params()\n        \n    \n    def update_params(self):\n        block_size = self.model_parameters['block_size']\n        train_size = self.model_parameters['train_size'] \n        epochs = self.model_parameters['epochs']\n        batch_size = self.model_parameters['batch_size']\n        hidden_layer = self.model_parameters['hidden_layer']\n        embedding_dimension = self.model_parameters['embedding_dimension']\n        learning_rate = self.model_parameters['learning_rate']\n        \n        self._load_prepare(block_size, train_size)\n        self.configure_model_dimensions(epochs, batch_size, hidden_layer, embedding_dimension, learning_rate)\n\n\n    def _load_prepare(self, block_size:int=3, train_size=0.8): \n        data = open(self.text_path, \"r\").read().splitlines()\n        self.chars = sorted(list(set(\"\".join(data))))\n\n        print(f\"Len of words: {len(data)}\") \n        print(f\"Len of unique characters in dataset: {len(self.chars)}\") \n        \n        stoi = { v:k+1 for k, v in enumerate(self.chars)}\n        stoi['.'] = 0\n        itos = { v:k for k, v in stoi.items()}\n        \n        self.stoi = stoi \n        self.itos = itos \n        self.block_size = block_size \n\n        def build_dataset(words): \n            block_size = self.block_size \n            X, Y = [], [] \n            for w in words: \n                context = [0] * block_size  \n                for ch in w + \".\": \n                    ix = self.stoi[ch]\n                    X.append(context)\n                    Y.append(ix)\n                    context = context[1:] + [ix]   \n\n            X = torch.tensor(X)\n            Y = torch.tensor(Y)\n            \n            return X, Y \n        \n        n2_slize=train_size + 0.1\n        n1 = int(len(data)*train_size)\n        n2 = int(len(data)*n2_slize)\n        random.shuffle(data)\n\n        self.train_X, self.train_Y = build_dataset(data[:n1]) \n        self.val_X, self.val_Y = build_dataset(data[n1:n2])\n        self.test_X, self.test_Y = build_dataset(data[n2:])\n    \n        print()\n        print(f\"Block size: {self.block_size}\")\n        print(f\"Len of TrainX: {len(self.train_X)}, Len of TrainY: {len(self.train_Y)}\")\n        print(f\"Len of ValX: {len(self.val_X)}, Len of ValY: {len(self.val_Y)}\")\n        print(f\"Len of TestX: {len(self.test_X)}, Len of TestY: {len(self.test_Y)}\")\n        \n        \n    def configure_model_dimensions(self, epochs=1000, batch_size=32, hidden_layer=100, embedding_dimension=2, learning_rate=0.1):\n\n        g = torch.Generator().manual_seed(2147483647)\n        self.input_layer_size = embedding_dimension * self.block_size\n        \n        self.b1 = torch.randn(hidden_layer, generator=g)\n        self.W1 = torch.randn( (self.input_layer_size, hidden_layer), generator=g)\n        self.b2 = torch.randn( len(self.chars)+1, generator=g)\n        self.W2 = torch.randn( (hidden_layer, 27 ), generator=g) \n        self.C = torch.randn( (len(self.chars)+1, embedding_dimension) )  \n\n        parameters = [self.C, self.b1, self.b2, self.W1, self.W2] \n\n        self.batch_size = batch_size \n        self.epochs = epochs \n        self.parameters = parameters\n        self.learning_rate = learning_rate\n        self.hidden_layer = hidden_layer\n        self.g = g\n\n        print(f\"Total of paramters: {sum(p.nelement() for p in parameters)}\")\n\n        ## making all require grads = True \n        for p in parameters:\n            p.requires_grad=True \n            \n            \n    def train_model(self): \n        \n        lossi = []\n        for br in tqdm(range(self.epochs)): \n            \n            ## forward pass\n            ix = torch.randint(0, self.train_X.shape[0], (self.batch_size,) )\n            emb = self.C[self.train_X[ix]]  # mini batch\n            h = torch.tanh( emb.view(-1, self.input_layer_size)@self.W1 + self.b1) \n            logits = h @ self.W2 + self.b2 \n            loss = F.cross_entropy(logits, self.train_Y[ix]) \n            \n            lossi.append(loss.item())\n\n            ## backward pass \n            for p in self.parameters: \n                p.grad = None \n            loss.backward()\n\n            ## update \n            lri =  self.learning_rate if br &lt; 8000 else 0.001\n            for p in self.parameters: \n                p.data += -lri * p.grad \n            \n            if br % 100 == 0: \n                print(f\"Training Loss ({br}) {loss.item()}\")\n                \n        # all loss \n        ix = torch.randint(0, self.val_X.shape[0], (self.batch_size,) )\n        emb = self.C[self.val_X[ix]]  # evaluvating with dev set \n        h = torch.tanh( emb.view(-1, self.input_layer_size)@self.W1 + self.b1) \n        logits = h @ self.W2 + self.b2 \n        loss = F.cross_entropy(logits, self.val_Y[ix])\n        print(f\"Validation loss: {loss}\") \n        \n        plt.plot(lossi, 'g')\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"LearningRate\")\n        plt.title(\"Learnign rate over epochs\")\n        min_index =  lossi.index(min(lossi))\n        plt.plot( min_index, lossi[min_index], 'ro' )\n        \n        return plt.show()\n    \n    \n    def sampling(self, words_needed=10): \n        \n        all_words=[]\n        for _ in range(words_needed):\n            \n            out = []\n            context = [0] * self.block_size \n            while True:\n                emb = self.C[torch.tensor([context])]\n                h = torch.tanh(emb.view(1, -1) @ self.W1 + self.b1)\n                logits = h @ self.W2 + self.b2\n                probs = F.softmax(logits, dim=1)\n                \n                ix = torch.multinomial(probs, num_samples=1, generator=self.g).item()\n                context = context[1:] + [ix]\n                out.append(ix)\n                \n                if ix == 0:\n                    break\n                \n            print(''.join(self.itos[i] for i in out))\n            all_words.append(\"\".join(self.itos[i] for i in out))\n</pre> import torch import random  from tqdm.auto import tqdm  import torch.nn.functional as F  import matplotlib.pyplot as plt  #A Neural Probabilistic Language Model class NPLM:       def __init__(self, text_path, model_parameters):          self.text_path = text_path          self.model_parameters = model_parameters         self.update_params()                   def update_params(self):         block_size = self.model_parameters['block_size']         train_size = self.model_parameters['train_size']          epochs = self.model_parameters['epochs']         batch_size = self.model_parameters['batch_size']         hidden_layer = self.model_parameters['hidden_layer']         embedding_dimension = self.model_parameters['embedding_dimension']         learning_rate = self.model_parameters['learning_rate']                  self._load_prepare(block_size, train_size)         self.configure_model_dimensions(epochs, batch_size, hidden_layer, embedding_dimension, learning_rate)       def _load_prepare(self, block_size:int=3, train_size=0.8):          data = open(self.text_path, \"r\").read().splitlines()         self.chars = sorted(list(set(\"\".join(data))))          print(f\"Len of words: {len(data)}\")          print(f\"Len of unique characters in dataset: {len(self.chars)}\")                   stoi = { v:k+1 for k, v in enumerate(self.chars)}         stoi['.'] = 0         itos = { v:k for k, v in stoi.items()}                  self.stoi = stoi          self.itos = itos          self.block_size = block_size           def build_dataset(words):              block_size = self.block_size              X, Y = [], []              for w in words:                  context = [0] * block_size                   for ch in w + \".\":                      ix = self.stoi[ch]                     X.append(context)                     Y.append(ix)                     context = context[1:] + [ix]                 X = torch.tensor(X)             Y = torch.tensor(Y)                          return X, Y                   n2_slize=train_size + 0.1         n1 = int(len(data)*train_size)         n2 = int(len(data)*n2_slize)         random.shuffle(data)          self.train_X, self.train_Y = build_dataset(data[:n1])          self.val_X, self.val_Y = build_dataset(data[n1:n2])         self.test_X, self.test_Y = build_dataset(data[n2:])              print()         print(f\"Block size: {self.block_size}\")         print(f\"Len of TrainX: {len(self.train_X)}, Len of TrainY: {len(self.train_Y)}\")         print(f\"Len of ValX: {len(self.val_X)}, Len of ValY: {len(self.val_Y)}\")         print(f\"Len of TestX: {len(self.test_X)}, Len of TestY: {len(self.test_Y)}\")                       def configure_model_dimensions(self, epochs=1000, batch_size=32, hidden_layer=100, embedding_dimension=2, learning_rate=0.1):          g = torch.Generator().manual_seed(2147483647)         self.input_layer_size = embedding_dimension * self.block_size                  self.b1 = torch.randn(hidden_layer, generator=g)         self.W1 = torch.randn( (self.input_layer_size, hidden_layer), generator=g)         self.b2 = torch.randn( len(self.chars)+1, generator=g)         self.W2 = torch.randn( (hidden_layer, 27 ), generator=g)          self.C = torch.randn( (len(self.chars)+1, embedding_dimension) )            parameters = [self.C, self.b1, self.b2, self.W1, self.W2]           self.batch_size = batch_size          self.epochs = epochs          self.parameters = parameters         self.learning_rate = learning_rate         self.hidden_layer = hidden_layer         self.g = g          print(f\"Total of paramters: {sum(p.nelement() for p in parameters)}\")          ## making all require grads = True          for p in parameters:             p.requires_grad=True                                def train_model(self):                   lossi = []         for br in tqdm(range(self.epochs)):                           ## forward pass             ix = torch.randint(0, self.train_X.shape[0], (self.batch_size,) )             emb = self.C[self.train_X[ix]]  # mini batch             h = torch.tanh( emb.view(-1, self.input_layer_size)@self.W1 + self.b1)              logits = h @ self.W2 + self.b2              loss = F.cross_entropy(logits, self.train_Y[ix])                           lossi.append(loss.item())              ## backward pass              for p in self.parameters:                  p.grad = None              loss.backward()              ## update              lri =  self.learning_rate if br &lt; 8000 else 0.001             for p in self.parameters:                  p.data += -lri * p.grad                           if br % 100 == 0:                  print(f\"Training Loss ({br}) {loss.item()}\")                          # all loss          ix = torch.randint(0, self.val_X.shape[0], (self.batch_size,) )         emb = self.C[self.val_X[ix]]  # evaluvating with dev set          h = torch.tanh( emb.view(-1, self.input_layer_size)@self.W1 + self.b1)          logits = h @ self.W2 + self.b2          loss = F.cross_entropy(logits, self.val_Y[ix])         print(f\"Validation loss: {loss}\")                   plt.plot(lossi, 'g')         plt.xlabel(\"Epochs\")         plt.ylabel(\"LearningRate\")         plt.title(\"Learnign rate over epochs\")         min_index =  lossi.index(min(lossi))         plt.plot( min_index, lossi[min_index], 'ro' )                  return plt.show()               def sampling(self, words_needed=10):                   all_words=[]         for _ in range(words_needed):                          out = []             context = [0] * self.block_size              while True:                 emb = self.C[torch.tensor([context])]                 h = torch.tanh(emb.view(1, -1) @ self.W1 + self.b1)                 logits = h @ self.W2 + self.b2                 probs = F.softmax(logits, dim=1)                                  ix = torch.multinomial(probs, num_samples=1, generator=self.g).item()                 context = context[1:] + [ix]                 out.append(ix)                                  if ix == 0:                     break                              print(''.join(self.itos[i] for i in out))             all_words.append(\"\".join(self.itos[i] for i in out)) In\u00a0[146]: Copied! <pre>text_path = \"names.txt\"\nmodel_parameters = {\n    \"block_size\" :3, \n    \"train_size\" :0.8, \n    'epochs' :10000, \n    'batch_size' :32, \n    'hidden_layer' :100, \n    'embedding_dimension' :50,\n    'learning_rate' :0.1 \n}\n\nobj = NPLM(text_path, model_parameters)\n</pre> text_path = \"names.txt\" model_parameters = {     \"block_size\" :3,      \"train_size\" :0.8,      'epochs' :10000,      'batch_size' :32,      'hidden_layer' :100,      'embedding_dimension' :50,     'learning_rate' :0.1  }  obj = NPLM(text_path, model_parameters)  <pre>Len of words: 32033\nLen of unique characters in dataset: 26\n\nBlock size: 3\nLen of TrainX: 182424, Len of TrainY: 182424\nLen of ValX: 22903, Len of ValY: 22903\nLen of TestX: 22819, Len of TestY: 22819\nTotal of paramters: 19177\n</pre> In\u00a0[147]: Copied! <pre>all_losses = obj.train_model()\n</pre> all_losses = obj.train_model() <pre>  0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Training Loss (0) 21.365528106689453\nTraining Loss (100) 7.525137901306152\nTraining Loss (200) 8.000117301940918\nTraining Loss (300) 5.434017658233643\nTraining Loss (400) 6.566066741943359\nTraining Loss (500) 5.396026134490967\nTraining Loss (600) 4.108829498291016\nTraining Loss (700) 4.28877592086792\nTraining Loss (800) 4.964651107788086\nTraining Loss (900) 4.141357898712158\nTraining Loss (1000) 3.156026840209961\nTraining Loss (1100) 4.4729413986206055\nTraining Loss (1200) 4.092074394226074\nTraining Loss (1300) 3.812209367752075\nTraining Loss (1400) 4.016378879547119\nTraining Loss (1500) 2.7465415000915527\nTraining Loss (1600) 2.3275375366210938\nTraining Loss (1700) 3.386552333831787\nTraining Loss (1800) 3.025444746017456\nTraining Loss (1900) 3.485633611679077\nTraining Loss (2000) 2.2200281620025635\nTraining Loss (2100) 2.706984519958496\nTraining Loss (2200) 3.5837714672088623\nTraining Loss (2300) 2.9370017051696777\nTraining Loss (2400) 3.2032883167266846\nTraining Loss (2500) 2.9951276779174805\nTraining Loss (2600) 2.3580543994903564\nTraining Loss (2700) 3.5030465126037598\nTraining Loss (2800) 4.340073108673096\nTraining Loss (2900) 3.0520877838134766\nTraining Loss (3000) 2.6629629135131836\nTraining Loss (3100) 3.189957857131958\nTraining Loss (3200) 3.202709674835205\nTraining Loss (3300) 2.784310817718506\nTraining Loss (3400) 2.526646137237549\nTraining Loss (3500) 2.5073351860046387\nTraining Loss (3600) 2.7305898666381836\nTraining Loss (3700) 3.096500873565674\nTraining Loss (3800) 2.7119500637054443\nTraining Loss (3900) 2.5519278049468994\nTraining Loss (4000) 2.1899893283843994\nTraining Loss (4100) 3.129143714904785\nTraining Loss (4200) 2.171104907989502\nTraining Loss (4300) 2.2405731678009033\nTraining Loss (4400) 2.0309736728668213\nTraining Loss (4500) 2.4675076007843018\nTraining Loss (4600) 2.4294281005859375\nTraining Loss (4700) 2.7461884021759033\nTraining Loss (4800) 2.019148349761963\nTraining Loss (4900) 2.3627750873565674\nTraining Loss (5000) 2.6719865798950195\nTraining Loss (5100) 2.290438175201416\nTraining Loss (5200) 2.761162757873535\nTraining Loss (5300) 2.4717183113098145\nTraining Loss (5400) 2.366727113723755\nTraining Loss (5500) 2.590480327606201\nTraining Loss (5600) 3.3551485538482666\nTraining Loss (5700) 2.6137754917144775\nTraining Loss (5800) 2.178959846496582\nTraining Loss (5900) 2.714170455932617\nTraining Loss (6000) 2.3206498622894287\nTraining Loss (6100) 2.1732938289642334\nTraining Loss (6200) 2.49531888961792\nTraining Loss (6300) 2.0259816646575928\nTraining Loss (6400) 2.205353021621704\nTraining Loss (6500) 2.4781100749969482\nTraining Loss (6600) 2.796325206756592\nTraining Loss (6700) 2.7649779319763184\nTraining Loss (6800) 1.8352924585342407\nTraining Loss (6900) 2.6141693592071533\nTraining Loss (7000) 2.5847437381744385\nTraining Loss (7100) 2.741269111633301\nTraining Loss (7200) 2.8293917179107666\nTraining Loss (7300) 2.983819007873535\nTraining Loss (7400) 2.1832122802734375\nTraining Loss (7500) 3.0581023693084717\nTraining Loss (7600) 2.326331853866577\nTraining Loss (7700) 1.9970585107803345\nTraining Loss (7800) 2.632941246032715\nTraining Loss (7900) 2.419503927230835\nTraining Loss (8000) 2.4589130878448486\nTraining Loss (8100) 2.841817617416382\nTraining Loss (8200) 2.2316696643829346\nTraining Loss (8300) 2.2907795906066895\nTraining Loss (8400) 2.3257570266723633\nTraining Loss (8500) 2.47363018989563\nTraining Loss (8600) 2.5161845684051514\nTraining Loss (8700) 2.1569809913635254\nTraining Loss (8800) 2.727275848388672\nTraining Loss (8900) 2.836402416229248\nTraining Loss (9000) 2.569993734359741\nTraining Loss (9100) 2.005176067352295\nTraining Loss (9200) 2.010479688644409\nTraining Loss (9300) 2.3766682147979736\nTraining Loss (9400) 2.8823678493499756\nTraining Loss (9500) 2.47355055809021\nTraining Loss (9600) 2.617953300476074\nTraining Loss (9700) 2.382654905319214\nTraining Loss (9800) 2.0608534812927246\nTraining Loss (9900) 2.6780548095703125\nValidation loss: 2.1477575302124023\n</pre> In\u00a0[124]: Copied! <pre>obj.sampling(words_needed = 10)\n</pre> obj.sampling(words_needed = 10) Out[124]: <pre>&lt;bound method NPLM.update_params of &lt;__main__.NPLM object at 0x0000023182F81D50&gt;&gt;</pre> In\u00a0[112]: Copied! <pre>model_parameters\n</pre> model_parameters Out[112]: <pre>{'block_size': 3, 'train_size': 0.8}</pre> In\u00a0[2]: Copied! <pre>!pip install charLLM\n</pre> !pip install charLLM <pre>'pip' is not recognized as an internal or external command,\noperable program or batch file.\n</pre> In\u00a0[175]: Copied! <pre>import\n</pre> import  <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nd:\\Andrew-karpathy-MakeMoreSeries\\2-Intro2MLP\\01-MLP.ipynb Cell 32 in &lt;cell line: 1&gt;()\n----&gt; &lt;a href='vscode-notebook-cell:/d%3A/Andrew-karpathy-MakeMoreSeries/2-Intro2MLP/01-MLP.ipynb#X61sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; import charLLM\n\nModuleNotFoundError: No module named 'charLLM'</pre> In\u00a0[4]: Copied! <pre>from charLLM import NPLM\n</pre> from charLLM import NPLM  <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nd:\\Andrew-karpathy-MakeMoreSeries\\2-Intro2MLP\\01-MLP.ipynb Cell 33 in &lt;cell line: 1&gt;()\n----&gt; &lt;a href='vscode-notebook-cell:/d%3A/Andrew-karpathy-MakeMoreSeries/2-Intro2MLP/01-MLP.ipynb#X62sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; from charLLM import NPLM\n\nModuleNotFoundError: No module named 'charLLM'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[180]: Copied! <pre>import charLLM\n</pre> import charLLM <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nd:\\Andrew-karpathy-MakeMoreSeries\\2-Intro2MLP\\01-MLP.ipynb Cell 35 in &lt;cell line: 1&gt;()\n----&gt; &lt;a href='vscode-notebook-cell:/d%3A/Andrew-karpathy-MakeMoreSeries/2-Intro2MLP/01-MLP.ipynb#X63sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; import charLLM\n\nModuleNotFoundError: No module named 'charLLM'</pre> In\u00a0[110]: Copied! <pre>obj.sampling(words_needed=5)\n</pre> obj.sampling(words_needed=5) <pre>nos.\nrimmeaydr.\nmalris.\nsfinliste.\neaamretla.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"01-Understanding-Derivaties/","title":"05. Understanding Derivaties","text":"In\u00a0[42]: Copied! <pre>!pip install graphviz\n</pre> !pip install graphviz <pre>Collecting graphviz\n  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\nInstalling collected packages: graphviz\nSuccessfully installed graphviz-0.20.1\n</pre> In\u00a0[2]: Copied! <pre>import math \nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import math  import numpy as np  import matplotlib.pyplot as plt %matplotlib inline  In\u00a0[3]: Copied! <pre>def f(x): \n    return 3*x**2 - 4*x + 5  # quadratic equation \n\nprint(f\"x = 3, output of function: {f(3)}\")\n\n## let's get some outputs \nxs = np.arange(-5, 5, 0.25) \nys = f(xs) \nplt.plot(xs, ys);\n</pre> def f(x):      return 3*x**2 - 4*x + 5  # quadratic equation   print(f\"x = 3, output of function: {f(3)}\")  ## let's get some outputs  xs = np.arange(-5, 5, 0.25)  ys = f(xs)  plt.plot(xs, ys);  <pre>x = 3, output of function: 20\n</pre> In\u00a0[4]: Copied! <pre>## let's see the derivative of the function by increasing a very small number \nh = 0.001 \nx = 3.0 \n\nprint(f\"f(x+h): {f(x+h)}\")\n\nderivative = (f(x + h) - f(x)) / h  # we are dividing the \"h\" because of normalization \nprint(f\"\\nDerivate with respect to the change in x: {derivative}\")\n\n\n## Try with x = -3, x = 2/3\n</pre> ## let's see the derivative of the function by increasing a very small number  h = 0.001  x = 3.0   print(f\"f(x+h): {f(x+h)}\")  derivative = (f(x + h) - f(x)) / h  # we are dividing the \"h\" because of normalization  print(f\"\\nDerivate with respect to the change in x: {derivative}\")   ## Try with x = -3, x = 2/3  <pre>f(x+h): 20.014003000000002\n\nDerivate with respect to the change in x: 14.00300000000243\n</pre> In\u00a0[5]: Copied! <pre>## let's see some  complex example \na = 2.0 \nb = -3.0 \nc = 10.0 \nh = 0.0001\n\nd1 = a*b + c \na += h\nd2 = a*b + c\n\nprint(\"d1\", d1)\nprint(\"d2\", d2) \nprint(\"derivative\", (d1-d2)/h)\n\n## Try change \"a+=h\" into \"b+=h\" and \"c+=h\" and check\n</pre> ## let's see some  complex example  a = 2.0  b = -3.0  c = 10.0  h = 0.0001  d1 = a*b + c  a += h d2 = a*b + c  print(\"d1\", d1) print(\"d2\", d2)  print(\"derivative\", (d1-d2)/h)  ## Try change \"a+=h\" into \"b+=h\" and \"c+=h\" and check  <pre>d1 4.0\nd2 3.999699999999999\nderivative 3.000000000010772\n</pre> In\u00a0[6]: Copied! <pre>## now let's start building micrograd for that we need to have a datastructure that holds the equation \n\n\nclass Value: \n    def __init__(self, value, _children = (), _op = \"\", label=\"\"): \n        self.data = value \n        self._prev = set(_children)  # protected variable (single underscore) \n        self._op = _op \n        self.label = label\n        self.grad = 0  # store all the grad \n    \n    def __repr__(self): \n        return (f\"Value=({self.data})\") \n    \n    def __add__(self, other):  # object.__add__(value)\n        return Value(self.data + other.data, (self, other), \"+\" )\n    \n    def __mul__(self, other): # object.__mul__(val) \n        return Value(self.data * other.data, (self, other), \"*\")\n    \n    def tanh(self): \n        x = self.data \n        t = (math.exp(2*x) - 1)/ (math.exp(2*x) + 1)\n        \n        return Value(t)\n</pre> ## now let's start building micrograd for that we need to have a datastructure that holds the equation    class Value:      def __init__(self, value, _children = (), _op = \"\", label=\"\"):          self.data = value          self._prev = set(_children)  # protected variable (single underscore)          self._op = _op          self.label = label         self.grad = 0  # store all the grad           def __repr__(self):          return (f\"Value=({self.data})\")           def __add__(self, other):  # object.__add__(value)         return Value(self.data + other.data, (self, other), \"+\" )          def __mul__(self, other): # object.__mul__(val)          return Value(self.data * other.data, (self, other), \"*\")          def tanh(self):          x = self.data          t = (math.exp(2*x) - 1)/ (math.exp(2*x) + 1)                  return Value(t)       In\u00a0[7]: Copied! <pre>a = Value(32, label = \"a\") \nb = Value(43, label = \"b\") \nc = Value(83, label = \"c\") \n\nd = a * b \ne = d*b + c\n</pre> a = Value(32, label = \"a\")  b = Value(43, label = \"b\")  c = Value(83, label = \"c\")   d = a * b  e = d*b + c    In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n  # builds a set of all nodes and edges in a graph\n  nodes, edges = set(), set()\n  def build(v):\n    if v not in nodes:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child, v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n  \n  nodes, edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name = uid + n._op, label = n._op)\n      # and connect this node to it\n      dot.edge(uid + n._op, uid)\n\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n  return dot\n</pre> from graphviz import Digraph  def trace(root):   # builds a set of all nodes and edges in a graph   nodes, edges = set(), set()   def build(v):     if v not in nodes:       nodes.add(v)       for child in v._prev:         edges.add((child, v))         build(child)   build(root)   return nodes, edges  def draw_dot(root):   dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right      nodes, edges = trace(root)   for n in nodes:     uid = str(id(n))     # for any value in the graph, create a rectangular ('record') node for it     dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')     if n._op:       # if this value is a result of some operation, create an op node for it       dot.node(name = uid + n._op, label = n._op)       # and connect this node to it       dot.edge(uid + n._op, uid)    for n1, n2 in edges:     # connect n1 to the op node of n2     dot.edge(str(id(n1)), str(id(n2)) + n2._op)    return dot In\u00a0[9]: Copied! <pre>draw_dot(e)\n</pre> draw_dot(e) <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\backend\\execute.py in run_check(cmd, input_lines, encoding, quiet, **kwargs)\n     78                 kwargs['stdout'] = kwargs['stderr'] = subprocess.PIPE\n---&gt; 79             proc = _run_input_lines(cmd, input_lines, kwargs=kwargs)\n     80         else:\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\backend\\execute.py in _run_input_lines(cmd, input_lines, kwargs)\n     98 def _run_input_lines(cmd, input_lines, *, kwargs):\n---&gt; 99     popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs)\n    100 \n\nD:\\Downloads\\Anaconda_God\\lib\\subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\n    950 \n--&gt; 951             self._execute_child(args, executable, preexec_fn, close_fds,\n    952                                 pass_fds, cwd, env,\n\nD:\\Downloads\\Anaconda_God\\lib\\subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\n   1419             try:\n-&gt; 1420                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n   1421                                          # no special security\n\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\nThe above exception was the direct cause of the following exception:\n\nExecutableNotFound                        Traceback (most recent call last)\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\IPython\\core\\formatters.py in __call__(self, obj, include, exclude)\n    968 \n    969             if method is not None:\n--&gt; 970                 return method(include=include, exclude=exclude)\n    971             return None\n    972         else:\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\jupyter_integration.py in _repr_mimebundle_(self, include, exclude, **_)\n     96         include = set(include) if include is not None else {self._jupyter_mimetype}\n     97         include -= set(exclude or [])\n---&gt; 98         return {mimetype: getattr(self, method_name)()\n     99                 for mimetype, method_name in MIME_TYPES.items()\n    100                 if mimetype in include}\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\jupyter_integration.py in &lt;dictcomp&gt;(.0)\n     96         include = set(include) if include is not None else {self._jupyter_mimetype}\n     97         include -= set(exclude or [])\n---&gt; 98         return {mimetype: getattr(self, method_name)()\n     99                 for mimetype, method_name in MIME_TYPES.items()\n    100                 if mimetype in include}\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\jupyter_integration.py in _repr_image_svg_xml(self)\n    110     def _repr_image_svg_xml(self) -&gt; str:\n    111         \"\"\"Return the rendered graph as SVG string.\"\"\"\n--&gt; 112         return self.pipe(format='svg', encoding=SVG_ENCODING)\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\piping.py in pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\n    102             '&lt;?xml version='\n    103         \"\"\"\n--&gt; 104         return self._pipe_legacy(format,\n    105                                  renderer=renderer,\n    106                                  formatter=formatter,\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\_tools.py in wrapper(*args, **kwargs)\n    169                               category=category)\n    170 \n--&gt; 171             return func(*args, **kwargs)\n    172 \n    173         return wrapper\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\piping.py in _pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\n    119                      engine: typing.Optional[str] = None,\n    120                      encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]:\n--&gt; 121         return self._pipe_future(format,\n    122                                  renderer=renderer,\n    123                                  formatter=formatter,\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\piping.py in _pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\n    147             if codecs.lookup(encoding) is codecs.lookup(self.encoding):\n    148                 # common case: both stdin and stdout need the same encoding\n--&gt; 149                 return self._pipe_lines_string(*args, encoding=encoding, **kwargs)\n    150             try:\n    151                 raw = self._pipe_lines(*args, input_encoding=self.encoding, **kwargs)\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\backend\\piping.py in pipe_lines_string(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet)\n    210     kwargs = {'input_lines': input_lines, 'encoding': encoding}\n    211 \n--&gt; 212     proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs)\n    213     return proc.stdout\n\nD:\\Downloads\\Anaconda_God\\lib\\site-packages\\graphviz\\backend\\execute.py in run_check(cmd, input_lines, encoding, quiet, **kwargs)\n     82     except OSError as e:\n     83         if e.errno == errno.ENOENT:\n---&gt; 84             raise ExecutableNotFound(cmd) from e\n     85         raise\n     86 \n\nExecutableNotFound: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH</pre> Out[9]: <pre>&lt;graphviz.graphs.Digraph at 0x2367e09fd60&gt;</pre> In\u00a0[70]: Copied! <pre># Now we computed the forward pass. \n## Next next is to compute the backward pass, here we are going to find the derivate for each element with respect to the last element. Here last element \"e\".\n</pre> # Now we computed the forward pass.  ## Next next is to compute the backward pass, here we are going to find the derivate for each element with respect to the last element. Here last element \"e\".  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[71]: Copied! <pre>def lol(): \n    h = 0.001 \n    \n    a = Value(2.0, label = \"a\") \n    b = Value(-3.0, label = \"b\") \n    c = Value(10.0, label = \"c\") \n    e = a * b \n    d = e + c \n    f = Value(-2.0) \n    L = d * f \n    L1 = L.data \n    \n    a = Value(2.0, label = \"a\") \n    # a.data += h\n    b = Value(-3.0, label = \"b\") \n    # b.data += h\n    c = Value(10.0, label = \"c\") \n    # c.data += h\n    e = a * b \n    d = e + c \n    f = Value(-2.0) \n    L = d * f \n    L2 = L.data \n    \n    print( (L2-L1)/ h)\n    \n    \nlol()\n</pre> def lol():      h = 0.001           a = Value(2.0, label = \"a\")      b = Value(-3.0, label = \"b\")      c = Value(10.0, label = \"c\")      e = a * b      d = e + c      f = Value(-2.0)      L = d * f      L1 = L.data           a = Value(2.0, label = \"a\")      # a.data += h     b = Value(-3.0, label = \"b\")      # b.data += h     c = Value(10.0, label = \"c\")      # c.data += h     e = a * b      d = e + c      f = Value(-2.0)      L = d * f      L2 = L.data           print( (L2-L1)/ h)           lol() <pre>0.0\n</pre> In\u00a0[72]: Copied! <pre>plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2)), color = 'brown'); plt.grid(); plt.title(\"Tanh\", color = \"blue\");\n</pre> plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2)), color = 'brown'); plt.grid(); plt.title(\"Tanh\", color = \"blue\"); In\u00a0[73]: Copied! <pre>## try with some complex example \n\n## inputs x1, x2 \nx1 = Value(2.0, label = 'x1') \nx2 = Value(0.0, label = \"x2\") \n\n## weight w1, w2\nw1=Value(-3.0, label = 'w1') \nw2= Value(1.0, label = \"w2\")\n\n## bias b1 \nb = Value(6.7, label = 'b')\n</pre> ## try with some complex example   ## inputs x1, x2  x1 = Value(2.0, label = 'x1')  x2 = Value(0.0, label = \"x2\")   ## weight w1, w2 w1=Value(-3.0, label = 'w1')  w2= Value(1.0, label = \"w2\")  ## bias b1  b = Value(6.7, label = 'b') In\u00a0[75]: Copied! <pre># x1*w1 + x2*w2 + b\nx1w1 = x1*w1\nx2w2 = x2*w2 \n\nx1w1x2w2 = x1w1 + x2w2\nn = x1w1x2w2 + b\no = n.tanh() \n\nprint(o)\n</pre> # x1*w1 + x2*w2 + b x1w1 = x1*w1 x2w2 = x2*w2   x1w1x2w2 = x1w1 + x2w2 n = x1w1x2w2 + b o = n.tanh()   print(o) <pre>Value=(0.6043677771171636)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"01-Understanding-Derivaties/#understanding-derivaties","title":"Understanding Derivaties\u00b6","text":"<ul> <li>We need to find the derivatives of every single point of x.</li> </ul> <p>Consider we have a number x = 3, and give it to the function f(x) and you got the output <code>20</code>. Now you are adding some number to x +<code>h=0.001</code>, and sending into function <code>f(x+h)</code>. We need to find how does the function responds with the added new variable or what sensitivity does it respond and what is the slope of the point does the function go up and down.</p>"},{"location":"02-NerualNetLibrary/","title":"06. Neural Net Library","text":"In\u00a0[3]: Copied! <pre>import math \nimport torch \nimport random \n# import Value\n</pre> import math  import torch  import random  # import Value  In\u00a0[4]: Copied! <pre>class Value:\n    \"\"\" stores a single scalar value and its gradient \"\"\"\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        # internal variables used for autograd graph construction\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data &lt; 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data &gt; 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def tanh(self): \n        x = self.data \n        t = (math.exp(2*x) - 1)/ (math.exp(2*x) + 1)\n        out = Value(t, (self, ), \"tanh\")\n\n        def _backward(): \n            self.grad += (1 - t**2) * out.grad \n        \n        out._backward = _backward \n\n        return out \n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __radd__(self, other): # other + self\n        return self + other\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def __rsub__(self, other): # other - self\n        return other + (-self)\n\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n\n    def __rtruediv__(self, other): # other / self\n        return other * self**-1\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:     \"\"\" stores a single scalar value and its gradient \"\"\"      def __init__(self, data, _children=(), _op=''):         self.data = data         self.grad = 0         # internal variables used for autograd graph construction         self._backward = lambda: None         self._prev = set(_children)         self._op = _op # the op that produced this node, for graphviz / debugging / etc      def __add__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data + other.data, (self, other), '+')          def _backward():             self.grad += out.grad             other.grad += out.grad         out._backward = _backward          return out      def __mul__(self, other):         other = other if isinstance(other, Value) else Value(other)         out = Value(self.data * other.data, (self, other), '*')          def _backward():             self.grad += other.data * out.grad             other.grad += self.data * out.grad         out._backward = _backward          return out      def __pow__(self, other):         assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"         out = Value(self.data**other, (self,), f'**{other}')          def _backward():             self.grad += (other * self.data**(other-1)) * out.grad         out._backward = _backward          return out      def relu(self):         out = Value(0 if self.data &lt; 0 else self.data, (self,), 'ReLU')          def _backward():             self.grad += (out.data &gt; 0) * out.grad         out._backward = _backward          return out      def backward(self):          # topological order all of the children in the graph         topo = []         visited = set()         def build_topo(v):             if v not in visited:                 visited.add(v)                 for child in v._prev:                     build_topo(child)                 topo.append(v)         build_topo(self)          # go one variable at a time and apply the chain rule to get its gradient         self.grad = 1         for v in reversed(topo):             v._backward()      def tanh(self):          x = self.data          t = (math.exp(2*x) - 1)/ (math.exp(2*x) + 1)         out = Value(t, (self, ), \"tanh\")          def _backward():              self.grad += (1 - t**2) * out.grad                   out._backward = _backward           return out       def __neg__(self): # -self         return self * -1      def __radd__(self, other): # other + self         return self + other      def __sub__(self, other): # self - other         return self + (-other)      def __rsub__(self, other): # other - self         return other + (-self)      def __rmul__(self, other): # other * self         return self * other      def __truediv__(self, other): # self / other         return self * other**-1      def __rtruediv__(self, other): # other / self         return other * self**-1      def __repr__(self):         return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[12]: Copied! <pre># let's implement neural network libraray from scratch \n\nclass Neuron: \n    \n    def __init__(self, nin:int): # nin -&gt; nr of inputs \n        self.w = [Value(random.uniform(-1, 1)) for x in range(nin)]\n        self.b = Value(random.uniform(-1, 1))\n        pass\n\n    def __call__(self, x): \n        # tanh( (w * x) + b ) \n        #\n        oct = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)  \n        out = oct.tanh() \n        return oct  \n\n    def parameters(self): \n        return self.w + [self.b]\n\nclass Layer:  # it's just a list of neuron s\n\n    def __init__(self, nin, nout): # nin -&gt; input dimension , nout -&gt; number of neurons in the layer \n        self.neurons = [ Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x): \n        out = [n(x) for n in self.neurons]\n        # print(self.neurons)\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self): \n        return [ p for layer in self.neurons for p in layer.paramters()] \n\nclass MLP: \n    \n    def __init__(self, nin, nouts): # nouts =&gt; sizes of all the layers in the mlp \n        sz = [nin] + nouts \n        self.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x): \n        for layer in self.layers: \n            x = layer(x) \n        \n        return x\n         \n    def parameters(self): \n        return [ p for layer in self.layes for p in layer.paramters()] \n\n\n\n\n\nx = [2.0, 3.0 ,-1.0]\nn = MLP(3, [4, 4, 1]) # two dimensional neuron, we need three of them \nn(x)\n</pre> # let's implement neural network libraray from scratch   class Neuron:           def __init__(self, nin:int): # nin -&gt; nr of inputs          self.w = [Value(random.uniform(-1, 1)) for x in range(nin)]         self.b = Value(random.uniform(-1, 1))         pass      def __call__(self, x):          # tanh( (w * x) + b )          #         oct = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)           out = oct.tanh()          return oct        def parameters(self):          return self.w + [self.b]  class Layer:  # it's just a list of neuron s      def __init__(self, nin, nout): # nin -&gt; input dimension , nout -&gt; number of neurons in the layer          self.neurons = [ Neuron(nin) for _ in range(nout)]      def __call__(self, x):          out = [n(x) for n in self.neurons]         # print(self.neurons)         return out[0] if len(out) == 1 else out      def parameters(self):          return [ p for layer in self.neurons for p in layer.paramters()]   class MLP:           def __init__(self, nin, nouts): # nouts =&gt; sizes of all the layers in the mlp          sz = [nin] + nouts          self.layers = [ Layer(sz[i], sz[i+1]) for i in range(len(nouts))]      def __call__(self, x):          for layer in self.layers:              x = layer(x)                   return x               def parameters(self):          return [ p for layer in self.layes for p in layer.paramters()]       x = [2.0, 3.0 ,-1.0] n = MLP(3, [4, 4, 1]) # two dimensional neuron, we need three of them  n(x) Out[12]: <pre>Value(data=-2.4116724020015443, grad=0)</pre> In\u00a0[11]: Copied! <pre>xs = [\n    [2.0, 3.0, -1.0], \n    [3.0, -1.0, 0.5], \n    [0.5, 1.0, 1.0], \n    [1.0, 1.0, -1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0] \n\nypred = [n(x) for x in xs]\n\n# calculate loss \nloss  = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n</pre> xs = [     [2.0, 3.0, -1.0],      [3.0, -1.0, 0.5],      [0.5, 1.0, 1.0],      [1.0, 1.0, -1.0] ] ys = [1.0, -1.0, -1.0, 1.0]   ypred = [n(x) for x in xs]  # calculate loss  loss  = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred)) loss Out[11]: <pre>Value(data=33.98456373541701, grad=0)</pre> In\u00a0[16]: Copied! <pre>\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 ypred.parameters()\n\nAttributeError: 'list' object has no attribute 'parameters'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"1-UnderstandingActivationsGradientMore/","title":"03. Understand activations and gradients","text":"In\u00a0[1]: Copied! <pre># In this notebook we will understand the activations and gradients during training and back-propagation (specially gradients when they flowing backwards, how they behave and\n# and how they look like). \n\n## And to understand the history why did they developed advanced architectures like RNN, LSTM and More. \n## This advanced models \"RNN\", \"LSTM\", and \"GRU\" are vey touch to optimize, to understand why this notebook will be very useful. \n\n## Let's create a dataset and model and complete the training before understanding this:)\n</pre> # In this notebook we will understand the activations and gradients during training and back-propagation (specially gradients when they flowing backwards, how they behave and # and how they look like).   ## And to understand the history why did they developed advanced architectures like RNN, LSTM and More.  ## This advanced models \"RNN\", \"LSTM\", and \"GRU\" are vey touch to optimize, to understand why this notebook will be very useful.   ## Let's create a dataset and model and complete the training before understanding this:)  In\u00a0[2]: Copied! <pre># imports \nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> # imports  import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[3]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines()  # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[4]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> <pre>C:\\Users\\aravi\\AppData\\Local\\Temp\\ipykernel_28424\\550266794.py:15: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n  X = torch.tensor(X)\n</pre> In\u00a0[5]: Copied! <pre># MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \nb1 = torch.randn(n_hidden,                        generator=g) \nW2 = torch.randn((n_hidden, vocab_size),          generator=g)\nb2 = torch.randn(vocab_size,                      generator=g)\n\nparameters = [C, W1, W2, b1, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # MLP revisited n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)  b1 = torch.randn(n_hidden,                        generator=g)  W2 = torch.randn((n_hidden, vocab_size),          generator=g) b2 = torch.randn(vocab_size,                      generator=g)  parameters = [C, W1, W2, b1, b2] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>11897\n</pre> In\u00a0[22]: Copied! <pre># same optimization as last time\nmax_steps = 40000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors\n  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n  # Linear layer\n  hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n  # Non-linearity\n  h = torch.tanh(hpreact) # hidden layer\n  logits = h @ W2 + b2 # output layer\n  loss = F.cross_entropy(logits, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n  \n  break\n</pre> # same optimization as last time max_steps = 40000 batch_size = 32 lossi = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   emb = C[Xb] # embed the characters into vectors   embcat = emb.view(emb.shape[0], -1) # concatenate the vectors   # Linear layer   hpreact = embcat @ W1 + b1 # hidden layer pre-activation   # Non-linearity   h = torch.tanh(hpreact) # hidden layer   logits = h @ W2 + b2 # output layer   loss = F.cross_entropy(logits, Yb) # loss function      # backward pass   for p in parameters:     p.grad = None   loss.backward()      # update   lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())      break    <pre>      0/  40000: 3.4957\n</pre> In\u00a0[13]: Copied! <pre>@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  hpreact = embcat @ W1 # + b1\n  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n  # hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n  h = torch.tanh(hpreact) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   hpreact = embcat @ W1 # + b1   #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias   # hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias   h = torch.tanh(hpreact) # (N, n_hidden)   logits = h @ W2 + b2 # (N, vocab_size)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 2.587782382965088\nval 2.6106646060943604\n</pre> In\u00a0[14]: Copied! <pre>## Now we completed this training and let's debug the issues:) \n\n## The first thing we are going to understand the weight initialization. \n## if you see the loss for first step it's very high, it's states that weight initialization is too high or very bad:) \n\n\"\"\" \nIn training of neural nets its almost always the case that you will have a rough idea for what loss to expect at initialization. It's depends on the problem setup and loss function. \nwe can calculate using the possible outcomes: \n\nFor Example: If you  have 27 possible outcomes, we can expect that that the possibility of next character is 27. \nat initialization, we have no reason to believe any characters to be much more likely than others. We would expect the probabilities distribution that comes out initially is \na uniform distribution assigning equal probability to all 27 characters.  \n\nSo the probability of any character is 1/27.0 and then calculate the loss. \n\n\"\"\"\ninLoss = - torch.tensor(1/27.0).log()\nprint(f\"Initial loss assumption: {inLoss}\")  # 3.29 \n\n\"\"\" \nIn our case we got more loss (25+). This means, In neural net while initialization the weights are messed up. Some characters are very confident and some characters are not confident. \n\nTo solve this: we want the logits to roughly zero when the network is  initialized. Let's see the logits for initial step. Please add break statement in the training loop and\nexecute this bellow code :) \nWe can also solve by making the weights uniform. Will see this :) \n\"\"\"\n\nprint(f\"Initial Logits: {logits[0]}\\nVery high we need roughly zero \ud83d\ude0d\")  # This are all very high values we need to be roughly zero\n</pre> ## Now we completed this training and let's debug the issues:)   ## The first thing we are going to understand the weight initialization.  ## if you see the loss for first step it's very high, it's states that weight initialization is too high or very bad:)   \"\"\"  In training of neural nets its almost always the case that you will have a rough idea for what loss to expect at initialization. It's depends on the problem setup and loss function.  we can calculate using the possible outcomes:   For Example: If you  have 27 possible outcomes, we can expect that that the possibility of next character is 27.  at initialization, we have no reason to believe any characters to be much more likely than others. We would expect the probabilities distribution that comes out initially is  a uniform distribution assigning equal probability to all 27 characters.    So the probability of any character is 1/27.0 and then calculate the loss.   \"\"\" inLoss = - torch.tensor(1/27.0).log() print(f\"Initial loss assumption: {inLoss}\")  # 3.29   \"\"\"  In our case we got more loss (25+). This means, In neural net while initialization the weights are messed up. Some characters are very confident and some characters are not confident.   To solve this: we want the logits to roughly zero when the network is  initialized. Let's see the logits for initial step. Please add break statement in the training loop and execute this bellow code :)  We can also solve by making the weights uniform. Will see this :)  \"\"\"  print(f\"Initial Logits: {logits[0]}\\nVery high we need roughly zero \ud83d\ude0d\")  # This are all very high values we need to be roughly zero   <pre>Initial loss assumption: 3.295836925506592\nInitial Logits: tensor([ 3.7627e+00,  6.3170e+00, -5.6324e-01, -6.4921e-01,  3.8004e-04,\n         4.8292e+00, -5.2193e+00, -7.8016e-01,  1.8332e+00,  3.0436e+00,\n        -1.9714e+00, -2.2416e+00,  2.5453e+00, -8.1987e-01, -1.3570e+00,\n         2.9829e+00, -2.5108e+00, -3.8701e+00,  1.4292e+00,  3.0940e-01,\n         3.0391e-01,  3.4396e-01, -2.8735e-01, -1.8788e+00, -3.9309e+00,\n         3.1855e+00,  9.7701e-01], grad_fn=&lt;SelectBackward0&gt;)\nVery high we need roughly zero \ud83d\ude0d\n</pre> In\u00a0[15]: Copied! <pre># Let's make some changes in the initialization process :) \n\nn_embd = 10 \nn_hidden = 200 \n\ng = torch.Generator().manual_seed(2147483647)\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \nb1 = torch.randn(n_hidden,                        generator=g) \n\n## Logits are calculated by h@W2+b2. We are initializing the b2 with random values(gaussian) of right size.Here we want roughly zeros we don't want random numbers. So let's add zero \n## it means b2 is zero while initialization.   \nb2 = torch.randn(vocab_size,                      generator=g) * 0  \n\n## Second we get logits by h@W2. So, we want logits to be very small, for this we would multiply the small value with W2 to get the small logits. \nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 \n## why we need small logits(logits near to zero) will discuss later!! \n\nparameters = [C, W1, W2, b1, b2]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n  p.requires_grad = True\n  \n# Now run the training loop and see the initial loss. Note! Don't forget to add break statement to get the first step logits. \n## If you see the loss we are getting roughly 3.5+ it's very great compare to previous initialization which is 27+ loss. \n## Now remove the break statement, run the full training and see the training loss. You will get amazing result because of this initialization. It got around 1.64 loss(cool \ud83d\ude0d)\n</pre> # Let's make some changes in the initialization process :)   n_embd = 10  n_hidden = 200   g = torch.Generator().manual_seed(2147483647) C  = torch.randn((vocab_size, n_embd),            generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)  b1 = torch.randn(n_hidden,                        generator=g)   ## Logits are calculated by h@W2+b2. We are initializing the b2 with random values(gaussian) of right size.Here we want roughly zeros we don't want random numbers. So let's add zero  ## it means b2 is zero while initialization.    b2 = torch.randn(vocab_size,                      generator=g) * 0    ## Second we get logits by h@W2. So, we want logits to be very small, for this we would multiply the small value with W2 to get the small logits.  W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1  ## why we need small logits(logits near to zero) will discuss later!!   parameters = [C, W1, W2, b1, b2] print(sum(p.nelement() for p in parameters)) for p in parameters:   p.requires_grad = True    # Now run the training loop and see the initial loss. Note! Don't forget to add break statement to get the first step logits.  ## If you see the loss we are getting roughly 3.5+ it's very great compare to previous initialization which is 27+ loss.  ## Now remove the break statement, run the full training and see the training loss. You will get amazing result because of this initialization. It got around 1.64 loss(cool \ud83d\ude0d) <pre>11897\n</pre> In\u00a0[16]: Copied! <pre>plt.plot(lossi); \n\n## why the plot is not like hockey stick (as usual) because most of the time the weight initialization at first is very bad so we usually get very high value. \n## And our model need to optimize the very big loss to low loss, this is the reason we usually get the hockey stick graph. \n## But now we got a thin straight graph because our initialization is perfect. This is fine :) \n\n## We got a very good results because instead of spending first 1000 iterations to reduce the loss or squashing down the weights. This is very useful technique :)\n</pre> plt.plot(lossi);   ## why the plot is not like hockey stick (as usual) because most of the time the weight initialization at first is very bad so we usually get very high value.  ## And our model need to optimize the very big loss to low loss, this is the reason we usually get the hockey stick graph.  ## But now we got a thin straight graph because our initialization is perfect. This is fine :)   ## We got a very good results because instead of spending first 1000 iterations to reduce the loss or squashing down the weights. This is very useful technique :)  In\u00a0[17]: Copied! <pre># Tanh converts all the values between -1 to +1 very smoothly. When back-propagating to the tanh, we will end with zero gradient error and it makes the neuron to zero. \n# To understand this we need to understand the back-propagation calculation for the tanh function. \n\ndef tanh_back_propagation(forward_output_of_tanh, previous_gradients): \n    \"\"\"Tanh back-propagation function\"\"\"\n    grad += (1 - forward_output_of_tanh**2) * previous_gradients\n    \n    return grad\n    \n## The problem !! \n# Consider forward_tanh_output = 1(feed forward), when back propagating we will insert this (1) in the formula, then gradients gets 0. because (1 - output**2) becomes 0. \n# Consider forward_tanh_output = -1(feed forward), when back propagating we will insert this (-1) in the formula, then gradients gets 0. because (1 - (-1)**2) becomes 0. \n# We know the tanh compress the inputs to -1 to +1 smoothly, but back propagating makes it zero. It means it destroy gradients to be zero. \n# This makes the neuron to be zero (dead neuron). This neuron will never learn anything after this.\n# This dead neuron sometimes happen in the initialization by chance or most of the time it happens while optimize the neural networks.\n      \n## The problem with some other activation functions !! \n# This is not only for tanh, other activation functions also has this issues like \"sigmoid\", \"relu\" and some more (research about this)\n \n## Will see how to solve this problem before this let's understand visually bellow :0\n</pre> # Tanh converts all the values between -1 to +1 very smoothly. When back-propagating to the tanh, we will end with zero gradient error and it makes the neuron to zero.  # To understand this we need to understand the back-propagation calculation for the tanh function.   def tanh_back_propagation(forward_output_of_tanh, previous_gradients):      \"\"\"Tanh back-propagation function\"\"\"     grad += (1 - forward_output_of_tanh**2) * previous_gradients          return grad      ## The problem !!  # Consider forward_tanh_output = 1(feed forward), when back propagating we will insert this (1) in the formula, then gradients gets 0. because (1 - output**2) becomes 0.  # Consider forward_tanh_output = -1(feed forward), when back propagating we will insert this (-1) in the formula, then gradients gets 0. because (1 - (-1)**2) becomes 0.  # We know the tanh compress the inputs to -1 to +1 smoothly, but back propagating makes it zero. It means it destroy gradients to be zero.  # This makes the neuron to be zero (dead neuron). This neuron will never learn anything after this. # This dead neuron sometimes happen in the initialization by chance or most of the time it happens while optimize the neural networks.        ## The problem with some other activation functions !!  # This is not only for tanh, other activation functions also has this issues like \"sigmoid\", \"relu\" and some more (research about this)   ## Will see how to solve this problem before this let's understand visually bellow :0   In\u00a0[52]: Copied! <pre>## Let's see the output of tanh in histogram \nplt.hist(h.view(-1).tolist(), 50); # view(-1) make to single dimension tensor \n\n## If you see the diagram, most of the value are in -1 and +1, it means most of outputs are near to -1 and +1 and this makes backward gradients makes destroy while back-propagating.\n</pre> ## Let's see the output of tanh in histogram  plt.hist(h.view(-1).tolist(), 50); # view(-1) make to single dimension tensor   ## If you see the diagram, most of the value are in -1 and +1, it means most of outputs are near to -1 and +1 and this makes backward gradients makes destroy while back-propagating.   In\u00a0[73]: Copied! <pre># How to solve this problem ? \n## If you see the network \"h\" is calculated by \"emb@W1+b1\" this is \"hpreact\". So let's optimize \"b1\" and \"W1\" to solve this issue. Why? \n## Because, This calculation \"emb@W1+b2\" makes the values to far from zero because of this our tanh is squashing between -1 to +1. We want \"hpreact\" closer to zero. \n## Let's solve this. So, let's add small values to W1 and b2 while initialization.\n</pre> # How to solve this problem ?  ## If you see the network \"h\" is calculated by \"emb@W1+b1\" this is \"hpreact\". So let's optimize \"b1\" and \"W1\" to solve this issue. Why?  ## Because, This calculation \"emb@W1+b2\" makes the values to far from zero because of this our tanh is squashing between -1 to +1. We want \"hpreact\" closer to zero.  ## Let's solve this. So, let's add small values to W1 and b2 while initialization.  In\u00a0[21]: Copied! <pre># Let's add some small value to W1 and b1 to solve this issue: \n\nn_embd = 10 \nn_hidden = 200 \n\ng = torch.Generator().manual_seed(2147483647)\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n\n# hpreact(emb@W1+b) Let's add some small values to W1 and b1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.1\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0  \nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 \n\n\nparameters = [C, W1, W2, b1, b2]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n  p.requires_grad = True\n  \n# Now put a break statement in the training loop and visualize the h(tanh) histogram to see if it's contains more -1 or +1.\n</pre> # Let's add some small value to W1 and b1 to solve this issue:   n_embd = 10  n_hidden = 200   g = torch.Generator().manual_seed(2147483647) C  = torch.randn((vocab_size, n_embd),            generator=g)  # hpreact(emb@W1+b) Let's add some small values to W1 and b1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.1 b1 = torch.randn(n_hidden,                        generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0   W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1    parameters = [C, W1, W2, b1, b2] print(sum(p.nelement() for p in parameters)) for p in parameters:   p.requires_grad = True    # Now put a break statement in the training loop and visualize the h(tanh) histogram to see if it's contains more -1 or +1.  <pre>11897\n</pre> In\u00a0[23]: Copied! <pre>plt.hist(h.view(-1).tolist(), 50);\n\n# Now we have very less -1 and +1. In-fact we don't have. It helps to avoid the dead neuron. \n# And it makes the distribution very cooler ;)\n</pre> plt.hist(h.view(-1).tolist(), 50);  # Now we have very less -1 and +1. In-fact we don't have. It helps to avoid the dead neuron.  # And it makes the distribution very cooler ;)  In\u00a0[28]: Copied! <pre># plt.imshow(h.abs() &gt; 0.99, cmap=\"gray\", interpolation=\"nearest\")  # If you see white color in the picture, it means it has value near 1. \n# If the plot has more white it means we have lot of values near to 1, which makes more dead neuron in our network.\n</pre> # plt.imshow(h.abs() &gt; 0.99, cmap=\"gray\", interpolation=\"nearest\")  # If you see white color in the picture, it means it has value near 1.  # If the plot has more white it means we have lot of values near to 1, which makes more dead neuron in our network.  In\u00a0[29]: Copied! <pre># Let's see all the result after optimization of our network ;) \n\n## Original network without optimization: \n# train 2.12\n# val 2.16 \n\n## fix tanh backward issue \n# train 2.07 \n# val 2.13 \n\n## fix initialization issue \n# train 2.03 \n# val 2.10\n</pre> # Let's see all the result after optimization of our network ;)   ## Original network without optimization:  # train 2.12 # val 2.16   ## fix tanh backward issue  # train 2.07  # val 2.13   ## fix initialization issue  # train 2.03  # val 2.10 In\u00a0[30]: Copied! <pre># Here we have very less network. So, we optimized the network very easily. But in practice we usually have very big networks in that case we will end up with lot of problems to optimize. \n# In practice, no one initializes the weight like what we did. What if we have 1000 of networks, and how do we initialize the weight?\n# To initialize the weight for larger networks, people usually follows some techniques like Kaiming Init (please look pytorch documentation to understand this)\n\n\"\"\" \nBefore this we need to lookup something: \n-&gt; Now-a-days you don't need to worry about the weight initialization because of modern architectures. \n-&gt; In modern architectures people uses \"batch_normalization\", \"layer_normalization\" and more techniques which significantly helps to reduce the loss and training time of the model.\n-&gt; First we will learn about batch normalization !! \n\n\"\"\"\n</pre> # Here we have very less network. So, we optimized the network very easily. But in practice we usually have very big networks in that case we will end up with lot of problems to optimize.  # In practice, no one initializes the weight like what we did. What if we have 1000 of networks, and how do we initialize the weight? # To initialize the weight for larger networks, people usually follows some techniques like Kaiming Init (please look pytorch documentation to understand this)  \"\"\"  Before this we need to lookup something:  -&gt; Now-a-days you don't need to worry about the weight initialization because of modern architectures.  -&gt; In modern architectures people uses \"batch_normalization\", \"layer_normalization\" and more techniques which significantly helps to reduce the loss and training time of the model. -&gt; First we will learn about batch normalization !!   \"\"\" In\u00a0[1]: Copied! <pre>\"\"\"\n# It came about 2015 by the google team! \n# It made it to train the very deep neural net quite reliably. \n# Let's understand how it's giving benefits. \n## We don't need a pre-activation should be very large so tanh makes to 1, or very small so tanh makes to -1. \n## In-fact we need pre-activation  roughly gaussian. (0mean and 1std-deviation) at-least at initialization. \n## This is what Batch normalization doing, it takes the \"pre-activation\" and normalizes it to \"gaussian distribution:\" \n## \n\"\"\"\n</pre> \"\"\" # It came about 2015 by the google team!  # It made it to train the very deep neural net quite reliably.  # Let's understand how it's giving benefits.  ## We don't need a pre-activation should be very large so tanh makes to 1, or very small so tanh makes to -1.  ## In-fact we need pre-activation  roughly gaussian. (0mean and 1std-deviation) at-least at initialization.  ## This is what Batch normalization doing, it takes the \"pre-activation\" and normalizes it to \"gaussian distribution:\"  ##  \"\"\" Out[1]: <pre>'\\n# It came about 2015 by the google team! \\n# It made it to train the very deep neural net quite reliably. \\n# Let\\'s understand how it\\'s giving benefits. \\n## We don\\'t need a pre-activation should be very large so tanh makes to 1, or very small so tanh makes to -1. \\n## In-fact we need pre-activation  roughly gaussian. (0mean and 1std-deviation) at-least at initialization. \\n## This is what Batch normalization doing, it takes the \"pre-activation\" and normalizes it to \"gaussian distribution:\" \\n## \\n'</pre> In\u00a0[1]: Copied! <pre># Let's see in action!!\n\nimport torch \n\n## Consider this is a pre-activation with the shape of 32, 200. \nhpreact = torch.randn( (4, 2, 2) ) # This is pre-activation (we need this roughly gaussian)\n\n## Let's take this and normalize to gaussian distribution.\n</pre> # Let's see in action!!  import torch   ## Consider this is a pre-activation with the shape of 32, 200.  hpreact = torch.randn( (4, 2, 2) ) # This is pre-activation (we need this roughly gaussian)  ## Let's take this and normalize to gaussian distribution.     In\u00a0[27]: Copied! <pre>fourth = torch.randn( (4, 3, 2) )\n# torch.\nfourth.shape\n</pre> fourth = torch.randn( (4, 3, 2) ) # torch. fourth.shape    Out[27]: <pre>torch.Size([4, 3, 2])</pre> In\u00a0[28]: Copied! <pre>import torchshow as ts\n</pre> import torchshow as ts In\u00a0[29]: Copied! <pre>len(fourth)\n</pre> len(fourth) Out[29]: <pre>4</pre> In\u00a0[30]: Copied! <pre>fourth[0]\n</pre> fourth[0] Out[30]: <pre>tensor([[ 0.9798, -0.3100],\n        [ 0.6040,  0.9283],\n        [ 0.3635,  1.5184]])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>ts.show(fourth)\n</pre> ts.show(fourth) In\u00a0[317]: Copied! <pre># hpreact.mean(dim = 0)\n</pre> # hpreact.mean(dim = 0) In\u00a0[318]: Copied! <pre>hpreact\n</pre> hpreact Out[318]: <pre>tensor([[[-0.0556,  0.3648],\n         [ 0.4200,  1.2959]],\n\n        [[-0.8864, -1.4869],\n         [ 0.0200,  0.0561]],\n\n        [[ 0.7532,  1.4177],\n         [ 1.9945,  0.3951]],\n\n        [[-0.8401, -2.0891],\n         [ 0.2592, -0.5302]]])</pre> In\u00a0[310]: Copied! <pre># dim = 0 \n\nhp_a = ( (0.0152 + 0.5926 + 0.6839 + 0.0231)/ 4)\nhp_b = ( (0.9670 + 0.5716 + -0.0575 + 1.0428)/ 4)\n\n\nhp_c = ( (0.7838 + 0.2630)/ 2)\nhp_d = ( (-0.0374 + 0.7228)/ 2 )\n\n(hp_a, hp_b)\n</pre> # dim = 0   hp_a = ( (0.0152 + 0.5926 + 0.6839 + 0.0231)/ 4) hp_b = ( (0.9670 + 0.5716 + -0.0575 + 1.0428)/ 4)   hp_c = ( (0.7838 + 0.2630)/ 2) hp_d = ( (-0.0374 + 0.7228)/ 2 )  (hp_a, hp_b)  Out[310]: <pre>(0.3287, 0.630975)</pre> In\u00a0[322]: Copied! <pre># dim = 1 \n\nhp_a_ = ( (-0.0556 + 0.4200)/ 2)\nhp_b_ = ( (0.3648 + 1.2959)/ 2)\n\nhp_c_ = ((-0.8864 + 0.0200)/ 2) \nhp_d_ = ((-1.4869 + 0.0561)/ 2) \n\n( hp_c_, hp_d_)\n</pre> # dim = 1   hp_a_ = ( (-0.0556 + 0.4200)/ 2) hp_b_ = ( (0.3648 + 1.2959)/ 2)  hp_c_ = ((-0.8864 + 0.0200)/ 2)  hp_d_ = ((-1.4869 + 0.0561)/ 2)   ( hp_c_, hp_d_) Out[322]: <pre>(-0.4332, -0.7154)</pre> In\u00a0[321]: Copied! <pre>hpreact.mean(dim = 1)\n</pre> hpreact.mean(dim = 1) Out[321]: <pre>tensor([[ 0.1822,  0.8303],\n        [-0.4332, -0.7154],\n        [ 1.3739,  0.9064],\n        [-0.2904, -1.3097]])</pre> In\u00a0[144]: Copied! <pre>for i in range(4): \n    # print(a[:,i].sum() / len(a[:,i]))\n    print(a[i, :].sum()/ len(a[i, :]))\n</pre> for i in range(4):      # print(a[:,i].sum() / len(a[:,i]))     print(a[i, :].sum()/ len(a[i, :])) <pre>tensor(-0.3894)\ntensor(-0.1993)\ntensor(-0.5139)\ntensor(-0.4111)\n</pre> In\u00a0[255]: Copied! <pre>a = [\n [[ 1.,  2.,  3.,  4.],\n  [ 5.,  6.,  7.,  8.],\n  [ 9., 10., 11., 12.]],\n [[13., 14., 15., 16.],\n  [17., 18., 19., 20.],\n  [21., 22., 23., 24.]], \n ]\n\na = torch.tensor(a)\n</pre> a = [  [[ 1.,  2.,  3.,  4.],   [ 5.,  6.,  7.,  8.],   [ 9., 10., 11., 12.]],  [[13., 14., 15., 16.],   [17., 18., 19., 20.],   [21., 22., 23., 24.]],   ]  a = torch.tensor(a)    In\u00a0[243]: Copied! <pre># dim = 0 for batches \na_ = ( (1 + 13 + 25) / 3)\nb = ( (2+14 + 26) / 3 )\nc= ( (3+15+27) / 3 ) \nd= ( (4+16+28) / 3 ) \n\nprint(a_, b, c, d)\n</pre> # dim = 0 for batches  a_ = ( (1 + 13 + 25) / 3) b = ( (2+14 + 26) / 3 ) c= ( (3+15+27) / 3 )  d= ( (4+16+28) / 3 )   print(a_, b, c, d) <pre>13.0 14.0 15.0 16.0\n</pre> In\u00a0[259]: Copied! <pre>b = torch.tensor([\n                   [1., 2., 3., 4.], \n                   [5., 6., 7., 8.], \n                   [9., 10., 11., 12.]\n                   ], \n                   )\n\nb.mean(dim=0)\n</pre> b = torch.tensor([                    [1., 2., 3., 4.],                     [5., 6., 7., 8.],                     [9., 10., 11., 12.]                    ],                     )  b.mean(dim=0) Out[259]: <pre>tensor([5., 6., 7., 8.])</pre> In\u00a0[272]: Copied! <pre>b.mean()\n</pre> b.mean() Out[272]: <pre>tensor(6.5000)</pre> In\u00a0[263]: Copied! <pre># dim = 1 for non-batch \nba__ = ( (1 + 2 + 3 + 4)/ 4) \nbb__ = ( (5 + 6 + 7 + 8)/ 4)\nbc__ = ( (9 + 10 + 11 + 12)/ 4)\n\n(ba__, bb__, bc__)\n</pre> # dim = 1 for non-batch  ba__ = ( (1 + 2 + 3 + 4)/ 4)  bb__ = ( (5 + 6 + 7 + 8)/ 4) bc__ = ( (9 + 10 + 11 + 12)/ 4)  (ba__, bb__, bc__)  Out[263]: <pre>(2.5, 6.5, 10.5)</pre> In\u00a0[207]: Copied! <pre># Since operations components are written in C++, they are not callable with operations such as ?? or \"__file__\" or \"getsourcefile\" type of operations.\n</pre> # Since operations components are written in C++, they are not callable with operations such as ?? or \"__file__\" or \"getsourcefile\" type of operations. Out[207]: <pre>tensor(5.)</pre> In\u00a0[208]: Copied! <pre>a[1][:, 0].sum() / 3\n</pre> a[1][:, 0].sum() / 3 Out[208]: <pre>tensor(17.)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"1-UnderstandingActivationsGradientMore/#fixing-initial-loss","title":"Fixing initial Loss\u00b6","text":""},{"location":"1-UnderstandingActivationsGradientMore/#fixing-tanh-backward-gradient-issue","title":"Fixing Tanh Backward gradient issue.\u00b6","text":""},{"location":"1-UnderstandingActivationsGradientMore/#kaiming-init","title":"Kaiming Init\u00b6","text":""},{"location":"1-UnderstandingActivationsGradientMore/#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"2-UnderstandingTorchMean/","title":"04. Understand torch mean","text":"In\u00a0[5]: Copied! <pre># Let's understand the torch mean function with different dimensions \n\nimport torch \nimport torchshow as ts  # This is for visualization process \nimport matplotlib.pyplot as plt \n\n## Let's create a normal distribution tensor \nnt = torch.randn( (4,4))  # 4 rows, 4 columns \n\nprint(f\"Tensor: {nt}\\nTensor shape: {nt.shape}\")\n\nplt.imshow(nt); \nplt.grid(\"off\")\n</pre> # Let's understand the torch mean function with different dimensions   import torch  import torchshow as ts  # This is for visualization process  import matplotlib.pyplot as plt   ## Let's create a normal distribution tensor  nt = torch.randn( (4,4))  # 4 rows, 4 columns   print(f\"Tensor: {nt}\\nTensor shape: {nt.shape}\")  plt.imshow(nt);  plt.grid(\"off\") <pre>Tensor: tensor([[ 1.7709,  1.1485,  0.0804, -0.0314],\n        [-0.0530,  0.8537,  1.1600,  0.3380],\n        [ 0.2127,  2.1935, -0.8011, -1.3338],\n        [ 0.6756, -0.4995, -0.9554, -0.5681]])\nTensor shape: torch.Size([4, 4])\n</pre> In\u00a0[15]: Copied! <pre># Let's calculate overall mean for 4x4 tensor \n\n## To find the overall mean, you can use the .mean() method \noverall_mean = nt.mean()  # overall mean is single number \n\n\"\"\"  \nCalculations: \n-&gt; Simply it adds all the elements and divide by len of the element. \n\noverall_mean = nt.view(-1).sum() / len(nt.view(-1))\n\nview(-1) -&gt; streches all the elements into one single tensor \n\"\"\"\n\nprint(f\"Overall mean: {overall_mean}\")\n</pre> # Let's calculate overall mean for 4x4 tensor   ## To find the overall mean, you can use the .mean() method  overall_mean = nt.mean()  # overall mean is single number   \"\"\"   Calculations:  -&gt; Simply it adds all the elements and divide by len of the element.   overall_mean = nt.view(-1).sum() / len(nt.view(-1))  view(-1) -&gt; streches all the elements into one single tensor  \"\"\"  print(f\"Overall mean: {overall_mean}\") <pre>Overall mean: 0.26193782687187195\n</pre> In\u00a0[47]: Copied! <pre># It looks very easy until we are calculating the mean based on the dimensions :(  \n# Let's calculate the mean for dim = 0 (rows) and dim = 1 (columns)\n\n## For better understanding let me transpose the matrix \nnt_t = nt.T  \n\nprint(f\"Transformed nt_t:\\n{nt_t}\") # output bellow \n\"\"\" \n\n\"\"\"\n\n## Let's calculate mean for dimension 0 (rows) and dimension 1 (cols) \ndim_0_mean = nt.mean(dim = 0) \ndim_1_mean = nt.mean(dim = 1)\n\n\"\"\" \nTransposed Tensor: \n[ 1.7709, -0.0530,  0.2127,  0.6756],\n[ 1.1485,  0.8537,  2.1935, -0.4995],\n[ 0.0804,  1.1600, -0.8011, -0.9554],\n[-0.0314,  0.3380, -1.3338, -0.5681] \n\nCalculations for dim = 0 (finding mean for all individual rows)\n[ 1.7709 + -0.0530 + 0.2127 + 0.6756] / 4 = (0.6515)\n[ 1.1485 +  0.8537 +  2.1935 + -0.4995] / 4 = (0.9240)\n[ 0.0804 +  1.1600 + -0.8011 + -0.9554] / 4 = (-0.1290)\n[-0.0314 + 0.3380 + -1.3338 + -0.5681] / 4 = (-0.3988)\n\noutput dim_0: [0.6515, 0.9240, -0.1290, -0.3988]\n\n\nCalculations for dim = 1 (finding mean for all individual columns) \n[1.7709 + 1.1485 + 0.0804 + -0.0313] / 4 = (0.7421)\n[-0.0530 + 0.8537 + 1.1600 + 0.3380 ] / 4 = (0.5747)\n[0.2127 + 2.1935 + -0.8011 + -1.3338 ] / 4 = (0.0678)\n[0.6756 + -0.4995 + -0.9554 + -0.5681] / 4 = (-0.3368)  \n\noutput dim_1 : [0.7421, 0.4554, 0.0678, -0.3368]\n\nNote: I am doing this calculations on the transformed tensor for better \n      understanding: \n\"\"\"\n\n\nprint(f\"dim0: {dim_0_mean},\\ndim1: {dim_1_mean}\")\n</pre> # It looks very easy until we are calculating the mean based on the dimensions :(   # Let's calculate the mean for dim = 0 (rows) and dim = 1 (columns)  ## For better understanding let me transpose the matrix  nt_t = nt.T    print(f\"Transformed nt_t:\\n{nt_t}\") # output bellow  \"\"\"   \"\"\"  ## Let's calculate mean for dimension 0 (rows) and dimension 1 (cols)  dim_0_mean = nt.mean(dim = 0)  dim_1_mean = nt.mean(dim = 1)  \"\"\"  Transposed Tensor:  [ 1.7709, -0.0530,  0.2127,  0.6756], [ 1.1485,  0.8537,  2.1935, -0.4995], [ 0.0804,  1.1600, -0.8011, -0.9554], [-0.0314,  0.3380, -1.3338, -0.5681]   Calculations for dim = 0 (finding mean for all individual rows) [ 1.7709 + -0.0530 + 0.2127 + 0.6756] / 4 = (0.6515) [ 1.1485 +  0.8537 +  2.1935 + -0.4995] / 4 = (0.9240) [ 0.0804 +  1.1600 + -0.8011 + -0.9554] / 4 = (-0.1290) [-0.0314 + 0.3380 + -1.3338 + -0.5681] / 4 = (-0.3988)  output dim_0: [0.6515, 0.9240, -0.1290, -0.3988]   Calculations for dim = 1 (finding mean for all individual columns)  [1.7709 + 1.1485 + 0.0804 + -0.0313] / 4 = (0.7421) [-0.0530 + 0.8537 + 1.1600 + 0.3380 ] / 4 = (0.5747) [0.2127 + 2.1935 + -0.8011 + -1.3338 ] / 4 = (0.0678) [0.6756 + -0.4995 + -0.9554 + -0.5681] / 4 = (-0.3368)    output dim_1 : [0.7421, 0.4554, 0.0678, -0.3368]  Note: I am doing this calculations on the transformed tensor for better        understanding:  \"\"\"   print(f\"dim0: {dim_0_mean},\\ndim1: {dim_1_mean}\") <pre>Transformed nt_t:\ntensor([[ 1.7709, -0.0530,  0.2127,  0.6756],\n        [ 1.1485,  0.8537,  2.1935, -0.4995],\n        [ 0.0804,  1.1600, -0.8011, -0.9554],\n        [-0.0314,  0.3380, -1.3338, -0.5681]])\ndim0: tensor([ 0.6515,  0.9240, -0.1290, -0.3988]),\ndim1: tensor([ 0.7421,  0.5747,  0.0678, -0.3369])\n</pre> In\u00a0[69]: Copied! <pre># It looks very easy until we are calculating the mean based on the dimensions with batches :) \n# Let's calculate the mean for dim = 0 (rows) and dim = 1 (columns) with batches \n\n\n## Let's create a 2 batch tensor with same shape \nnt_b = torch.randn( (2, 2, 2) )  # 2:batch, 2:rows, 2:cols \n\n## fixed \nnt_b = torch.tensor( [\n    [[-0.5085,  0.2839],\n    [ 1.1289,  3.3153]],\n\n    [[ 1.2075,  0.4010],\n    [ 2.6585, -0.3439]]\n    \n]) \n\nprint(f\"Tensor with 2 batch:\\n{nt_b}\")\n\n## Let's find the mean for dim=0 and dim=1 in the both the batches \nnt_b_dim0Mean = nt_b.mean(dim=0) \nnt_b_dim1Mean = nt_b.mean(dim=1)\n\"\"\" \n2Batch Tensor: \n[   # Batch 1 \n    [[-0.5085,  0.2839],\n    [ 1.1289,  3.3153]],\n\n    # Batch 2 \n    [[ 1.2075,  0.4010],\n    [ 2.6585, -0.3439]]\n]\n\nCalculations for dim=0 \na_dim0: [ -0.5085 + 1.2075 ] / 2 = (0.3495)\nb_dim0: [ 0.2839 + 0.4010 ] / 2 = (0.3424)\nc_dim0: [ 1.1289 + 2.6585 ] / 2 = (1.8937)\nd_dim0: [ 3.3153 + -0.3439 ] / 2 = (1.4857)\n\noutput dim_0: [0.3495, 0.3424, 1.8937, 1.4857] \n\nCalculations for dim=1 \na_dim1: [-0.5085 + 1.1289] / 2 = (0.3102)\nb_dim1: [0.2839 + 3.3153] / 2 = (1.7996)\nc_dim1: [1.2075 + 2.6585] / 2 = (1.9330)\nd_dim1: [0.4010 + -0.3439] / 2 = (0.0286)\n\noutput dim_1: [0.3102, 1.7996, 1.9330, 0.0286]\n\"\"\"\n\nprint(f\"Dim0: {nt_b_dim0Mean}\\nDim1: {nt_b_dim1Mean}\")\n</pre> # It looks very easy until we are calculating the mean based on the dimensions with batches :)  # Let's calculate the mean for dim = 0 (rows) and dim = 1 (columns) with batches    ## Let's create a 2 batch tensor with same shape  nt_b = torch.randn( (2, 2, 2) )  # 2:batch, 2:rows, 2:cols   ## fixed  nt_b = torch.tensor( [     [[-0.5085,  0.2839],     [ 1.1289,  3.3153]],      [[ 1.2075,  0.4010],     [ 2.6585, -0.3439]]      ])   print(f\"Tensor with 2 batch:\\n{nt_b}\")  ## Let's find the mean for dim=0 and dim=1 in the both the batches  nt_b_dim0Mean = nt_b.mean(dim=0)  nt_b_dim1Mean = nt_b.mean(dim=1) \"\"\"  2Batch Tensor:  [   # Batch 1      [[-0.5085,  0.2839],     [ 1.1289,  3.3153]],      # Batch 2      [[ 1.2075,  0.4010],     [ 2.6585, -0.3439]] ]  Calculations for dim=0  a_dim0: [ -0.5085 + 1.2075 ] / 2 = (0.3495) b_dim0: [ 0.2839 + 0.4010 ] / 2 = (0.3424) c_dim0: [ 1.1289 + 2.6585 ] / 2 = (1.8937) d_dim0: [ 3.3153 + -0.3439 ] / 2 = (1.4857)  output dim_0: [0.3495, 0.3424, 1.8937, 1.4857]   Calculations for dim=1  a_dim1: [-0.5085 + 1.1289] / 2 = (0.3102) b_dim1: [0.2839 + 3.3153] / 2 = (1.7996) c_dim1: [1.2075 + 2.6585] / 2 = (1.9330) d_dim1: [0.4010 + -0.3439] / 2 = (0.0286)  output dim_1: [0.3102, 1.7996, 1.9330, 0.0286] \"\"\"  print(f\"Dim0: {nt_b_dim0Mean}\\nDim1: {nt_b_dim1Mean}\") <pre>Tensor with 2 batch:\ntensor([[[-0.5085,  0.2839],\n         [ 1.1289,  3.3153]],\n\n        [[ 1.2075,  0.4010],\n         [ 2.6585, -0.3439]]])\nDim0: tensor([[0.3495, 0.3424],\n        [1.8937, 1.4857]])\nDim1: tensor([[0.3102, 1.7996],\n        [1.9330, 0.0285]])\n</pre> In\u00a0[\u00a0]: Copied! <pre># This batch calculations will apply for n number of batches !! \n\n## You can also specify the keepdims=True, all the calculations are same but it outputs in same dimension of input except the calculating dimension. \n## nt_b.mean(dim=1, keepdims=True)\n</pre> # This batch calculations will apply for n number of batches !!   ## You can also specify the keepdims=True, all the calculations are same but it outputs in same dimension of input except the calculating dimension.  ## nt_b.mean(dim=1, keepdims=True)"}]}