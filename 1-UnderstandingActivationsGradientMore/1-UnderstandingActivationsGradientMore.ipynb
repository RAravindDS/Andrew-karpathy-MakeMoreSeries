{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we will understand the activations and gradients during training and back-propagation (specially gradients when they flowing backwards, how they behave and\n",
    "# and how they look like). \n",
    "\n",
    "## And to understand the history why did they developed advanced architectures like RNN, LSTM and More. \n",
    "## This advanced models \"RNN\", \"LSTM\", and \"GRU\" are vey touch to optimize, to understand why this notebook will be very useful. \n",
    "\n",
    "## Let's create a dataset and model and complete the training before understanding this:) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\AppData\\Local\\Temp\\ipykernel_28424\\550266794.py:15: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  X = torch.tensor(X)\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \n",
    "b1 = torch.randn(n_hidden,                        generator=g) \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g)\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  40000: 3.4957\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 40000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  \n",
    "  break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.587782382965088\n",
      "val 2.6106646060943604\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  # hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing initial Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss assumption: 3.295836925506592\n",
      "Initial Logits: tensor([ 3.7627e+00,  6.3170e+00, -5.6324e-01, -6.4921e-01,  3.8004e-04,\n",
      "         4.8292e+00, -5.2193e+00, -7.8016e-01,  1.8332e+00,  3.0436e+00,\n",
      "        -1.9714e+00, -2.2416e+00,  2.5453e+00, -8.1987e-01, -1.3570e+00,\n",
      "         2.9829e+00, -2.5108e+00, -3.8701e+00,  1.4292e+00,  3.0940e-01,\n",
      "         3.0391e-01,  3.4396e-01, -2.8735e-01, -1.8788e+00, -3.9309e+00,\n",
      "         3.1855e+00,  9.7701e-01], grad_fn=<SelectBackward0>)\n",
      "Very high we need roughly zero üòç\n"
     ]
    }
   ],
   "source": [
    "## Now we completed this training and let's debug the issues:) \n",
    "\n",
    "## The first thing we are going to understand the weight initialization. \n",
    "## if you see the loss for first step it's very high, it's states that weight initialization is too high or very bad:) \n",
    "\n",
    "\"\"\" \n",
    "In training of neural nets its almost always the case that you will have a rough idea for what loss to expect at initialization. It's depends on the problem setup and loss function. \n",
    "we can calculate using the possible outcomes: \n",
    "\n",
    "For Example: If you  have 27 possible outcomes, we can expect that that the possibility of next character is 27. \n",
    "at initialization, we have no reason to believe any characters to be much more likely than others. We would expect the probabilities distribution that comes out initially is \n",
    "a uniform distribution assigning equal probability to all 27 characters.  \n",
    "\n",
    "So the probability of any character is 1/27.0 and then calculate the loss. \n",
    "\n",
    "\"\"\"\n",
    "inLoss = - torch.tensor(1/27.0).log()\n",
    "print(f\"Initial loss assumption: {inLoss}\")  # 3.29 \n",
    "\n",
    "\"\"\" \n",
    "In our case we got more loss (25+). This means, In neural net while initialization the weights are messed up. Some characters are very confident and some characters are not confident. \n",
    "\n",
    "To solve this: we want the logits to roughly zero when the network is  initialized. Let's see the logits for initial step. Please add break statement in the training loop and\n",
    "execute this bellow code :) \n",
    "We can also solve by making the weights uniform. Will see this :) \n",
    "\"\"\"\n",
    "\n",
    "print(f\"Initial Logits: {logits[0]}\\nVery high we need roughly zero üòç\")  # This are all very high values we need to be roughly zero \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# Let's make some changes in the initialization process :) \n",
    "\n",
    "n_embd = 10 \n",
    "n_hidden = 200 \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \n",
    "b1 = torch.randn(n_hidden,                        generator=g) \n",
    "\n",
    "## Logits are calculated by h@W2+b2. We are initializing the b2 with random values(gaussian) of right size.Here we want roughly zeros we don't want random numbers. So let's add zero \n",
    "## it means b2 is zero while initialization.   \n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  \n",
    "\n",
    "## Second we get logits by h@W2. So, we want logits to be very small, for this we would multiply the small value with W2 to get the small logits. \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 \n",
    "## why we need small logits(logits near to zero) will discuss later!! \n",
    "\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# Now run the training loop and see the initial loss. Note! Don't forget to add break statement to get the first step logits. \n",
    "## If you see the loss we are getting roughly 3.5+ it's very great compare to previous initialization which is 27+ loss. \n",
    "## Now remove the break statement, run the full training and see the training loss. You will get amazing result because of this initialization. It got around 1.64 loss(cool üòç)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi); \n",
    "\n",
    "## why the plot is not like hockey stick (as usual) because most of the time the weight initialization at first is very bad so we usually get very high value. \n",
    "## And our model need to optimize the very big loss to low loss, this is the reason we usually get the hockey stick graph. \n",
    "## But now we got a thin straight graph because our initialization is perfect. This is fine :) \n",
    "\n",
    "## We got a very good results because instead of spending first 1000 iterations to reduce the loss or squashing down the weights. This is very useful technique :) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Tanh Backward gradient issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh converts all the values between -1 to +1 very smoothly. When back-propagating to the tanh, we will end with zero gradient error and it makes the neuron to zero. \n",
    "# To understand this we need to understand the back-propagation calculation for the tanh function. \n",
    "\n",
    "def tanh_back_propagation(forward_output_of_tanh, previous_gradients): \n",
    "    \"\"\"Tanh back-propagation function\"\"\"\n",
    "    grad += (1 - forward_output_of_tanh**2) * previous_gradients\n",
    "    \n",
    "    return grad\n",
    "    \n",
    "## The problem !! \n",
    "# Consider forward_tanh_output = 1(feed forward), when back propagating we will insert this (1) in the formula, then gradients gets 0. because (1 - output**2) becomes 0. \n",
    "# Consider forward_tanh_output = -1(feed forward), when back propagating we will insert this (-1) in the formula, then gradients gets 0. because (1 - (-1)**2) becomes 0. \n",
    "# We know the tanh compress the inputs to -1 to +1 smoothly, but back propagating makes it zero. It means it destroy gradients to be zero. \n",
    "# This makes the neuron to be zero (dead neuron). This neuron will never learn anything after this.\n",
    "# This dead neuron sometimes happen in the initialization by chance or most of the time it happens while optimize the neural networks.\n",
    "      \n",
    "## The problem with some other activation functions !! \n",
    "# This is not only for tanh, other activation functions also has this issues like \"sigmoid\", \"relu\" and some more (research about this)\n",
    " \n",
    "## Will see how to solve this problem before this let's understand visually bellow :0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's see the output of tanh in histogram \n",
    "plt.hist(h.view(-1).tolist(), 50); # view(-1) make to single dimension tensor \n",
    "\n",
    "## If you see the diagram, most of the value are in -1 and +1, it means most of outputs are near to -1 and +1 and this makes backward gradients makes destroy while back-propagating.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to solve this problem ? \n",
    "## If you see the network \"h\" is calculated by \"emb@W1+b1\" this is \"hpreact\". So let's optimize \"b1\" and \"W1\" to solve this issue. Why? \n",
    "## Because, This calculation \"emb@W1+b2\" makes the values to far from zero because of this our tanh is squashing between -1 to +1. We want \"hpreact\" closer to zero. \n",
    "## Let's solve this. So, let's add small values to W1 and b2 while initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# Let's add some small value to W1 and b1 to solve this issue: \n",
    "\n",
    "n_embd = 10 \n",
    "n_hidden = 200 \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "# hpreact(emb@W1+b) Let's add some small values to W1 and b1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.1\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 \n",
    "\n",
    "\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# Now put a break statement in the training loop and visualize the h(tanh) histogram to see if it's contains more -1 or +1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);\n",
    "\n",
    "# Now we have very less -1 and +1. In-fact we don't have. It helps to avoid the dead neuron. \n",
    "# And it makes the distribution very cooler ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\")  # If you see white color in the picture, it means it has value near 1. \n",
    "# If the plot has more white it means we have lot of values near to 1, which makes more dead neuron in our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see all the result after optimization of our network ;) \n",
    "\n",
    "## Original network without optimization: \n",
    "# train 2.12\n",
    "# val 2.16 \n",
    "\n",
    "## fix tanh backward issue \n",
    "# train 2.07 \n",
    "# val 2.13 \n",
    "\n",
    "## fix initialization issue \n",
    "# train 2.03 \n",
    "# val 2.10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have very less network. So, we optimized the network very easily. But in practice we usually have very big networks in that case we will end up with lot of problems to optimize. \n",
    "# In practice, no one initializes the weight like what we did. What if we have 1000 of networks, and how do we initialize the weight?\n",
    "# To initialize the weight for larger networks, people usually follows some techniques like Kaiming Init (please look pytorch documentation to understand this)\n",
    "\n",
    "\"\"\" \n",
    "Before this we need to lookup something: \n",
    "-> Now-a-days you don't need to worry about the weight initialization because of modern architectures. \n",
    "-> In modern architectures people uses \"batch_normalization\", \"layer_normalization\" and more techniques which significantly helps to reduce the loss and training time of the model.\n",
    "-> First we will learn about batch normalization !! \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# It came about 2015 by the google team! \\n# It made it to train the very deep neural net quite reliably. \\n# Let\\'s understand how it\\'s giving benefits. \\n## We don\\'t need a pre-activation should be very large so tanh makes to 1, or very small so tanh makes to -1. \\n## In-fact we need pre-activation  roughly gaussian. (0mean and 1std-deviation) at-least at initialization. \\n## This is what Batch normalization doing, it takes the \"pre-activation\" and normalizes it to \"gaussian distribution:\" \\n## \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# It came about 2015 by the google team! \n",
    "# It made it to train the very deep neural net quite reliably. \n",
    "# Let's understand how it's giving benefits. \n",
    "## We don't need a pre-activation should be very large so tanh makes to 1, or very small so tanh makes to -1. \n",
    "## In-fact we need pre-activation  roughly gaussian. (0mean and 1std-deviation) at-least at initialization. \n",
    "## This is what Batch normalization doing, it takes the \"pre-activation\" and normalizes it to \"gaussian distribution:\" \n",
    "## \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see in action!!\n",
    "\n",
    "import torch \n",
    "\n",
    "## Consider this is a pre-activation with the shape of 32, 200. \n",
    "hpreact = torch.randn( (4, 2, 2) ) # This is pre-activation (we need this roughly gaussian)\n",
    "\n",
    "## Let's take this and normalize to gaussian distribution.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth = torch.randn( (4, 3, 2) )\n",
    "# torch.\n",
    "fourth.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchshow as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fourth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9798, -0.3100],\n",
       "        [ 0.6040,  0.9283],\n",
       "        [ 0.3635,  1.5184]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(fourth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpreact.mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0556,  0.3648],\n",
       "         [ 0.4200,  1.2959]],\n",
       "\n",
       "        [[-0.8864, -1.4869],\n",
       "         [ 0.0200,  0.0561]],\n",
       "\n",
       "        [[ 0.7532,  1.4177],\n",
       "         [ 1.9945,  0.3951]],\n",
       "\n",
       "        [[-0.8401, -2.0891],\n",
       "         [ 0.2592, -0.5302]]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3287, 0.630975)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim = 0 \n",
    "\n",
    "hp_a = ( (0.0152 + 0.5926 + 0.6839 + 0.0231)/ 4)\n",
    "hp_b = ( (0.9670 + 0.5716 + -0.0575 + 1.0428)/ 4)\n",
    "\n",
    "\n",
    "hp_c = ( (0.7838 + 0.2630)/ 2)\n",
    "hp_d = ( (-0.0374 + 0.7228)/ 2 )\n",
    "\n",
    "(hp_a, hp_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.4332, -0.7154)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim = 1 \n",
    "\n",
    "hp_a_ = ( (-0.0556 + 0.4200)/ 2)\n",
    "hp_b_ = ( (0.3648 + 1.2959)/ 2)\n",
    "\n",
    "hp_c_ = ((-0.8864 + 0.0200)/ 2) \n",
    "hp_d_ = ((-1.4869 + 0.0561)/ 2) \n",
    "\n",
    "( hp_c_, hp_d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1822,  0.8303],\n",
       "        [-0.4332, -0.7154],\n",
       "        [ 1.3739,  0.9064],\n",
       "        [-0.2904, -1.3097]])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.mean(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3894)\n",
      "tensor(-0.1993)\n",
      "tensor(-0.5139)\n",
      "tensor(-0.4111)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    # print(a[:,i].sum() / len(a[:,i]))\n",
    "    print(a[i, :].sum()/ len(a[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    " [[ 1.,  2.,  3.,  4.],\n",
    "  [ 5.,  6.,  7.,  8.],\n",
    "  [ 9., 10., 11., 12.]],\n",
    " [[13., 14., 15., 16.],\n",
    "  [17., 18., 19., 20.],\n",
    "  [21., 22., 23., 24.]], \n",
    " ]\n",
    "\n",
    "a = torch.tensor(a) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0 14.0 15.0 16.0\n"
     ]
    }
   ],
   "source": [
    "# dim = 0 for batches \n",
    "a_ = ( (1 + 13 + 25) / 3)\n",
    "b = ( (2+14 + 26) / 3 )\n",
    "c= ( (3+15+27) / 3 ) \n",
    "d= ( (4+16+28) / 3 ) \n",
    "\n",
    "print(a_, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 6., 7., 8.])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([\n",
    "                   [1., 2., 3., 4.], \n",
    "                   [5., 6., 7., 8.], \n",
    "                   [9., 10., 11., 12.]\n",
    "                   ], \n",
    "                   )\n",
    "\n",
    "b.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.5000)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5, 6.5, 10.5)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim = 1 for non-batch \n",
    "ba__ = ( (1 + 2 + 3 + 4)/ 4) \n",
    "bb__ = ( (5 + 6 + 7 + 8)/ 4)\n",
    "bc__ = ( (9 + 10 + 11 + 12)/ 4)\n",
    "\n",
    "(ba__, bb__, bc__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since operations components are written in C++, they are not callable with operations such as ?? or \"__file__\" or \"getsourcefile\" type of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][:, 0].sum() / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
